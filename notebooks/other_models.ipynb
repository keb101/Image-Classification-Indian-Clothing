{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3l5o8OIkfYs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Rescaling, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JG0nKaPy2IkC",
    "outputId": "d86271ef-b2b9-427e-f98b-a3710dce35a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91166 files belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# Making data sets from images that are in folders named as their class_label\n",
    "column_labels = ['blouse', 'dhoti_pants', 'dupattas', 'gowns', 'kurta_men', 'leggings_and_salwars', 'lehenga', 'mojaris_men', 'mojaris_women', 'nehru_jackets', 'palazzos', 'petticoats', 'sherwanis', 'saree', 'women_kurta']\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    'data/interim/train_w_labels',\n",
    "    labels='inferred',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=256,\n",
    "    image_size=(70,70),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    crop_to_aspect_ratio=False,\n",
    "    class_names=column_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUgoYX4W2IkJ",
    "outputId": "6dbf2bbd-4a50-4ef7-b193-66bd6681d59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7500 files belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "val_dataset = image_dataset_from_directory(\n",
    "    'data/interim/val_w_labels',\n",
    "    labels='inferred',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=256,\n",
    "    image_size=(70,70),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    crop_to_aspect_ratio=False,\n",
    "    class_names=column_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE9jKr1pbFAV",
    "outputId": "6e4184bf-a713-49e3-bf52-c740ac706cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7500 files belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = image_dataset_from_directory(\n",
    "    'data/interim/test_w_labels',\n",
    "    labels='inferred',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=256,\n",
    "    image_size=(70,70),\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    crop_to_aspect_ratio=False,\n",
    "    class_names=column_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpIb_1JkbFAW"
   },
   "outputs": [],
   "source": [
    "# Normalizing images\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "train_ds_norm = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds_norm = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds_norm = test_dataset.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJjyzAD1R_w8"
   },
   "outputs": [],
   "source": [
    "# Setting up model\n",
    "img_width, img_height = 70, 70\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "n_classes = 15\n",
    "input_shape= (img_width, img_height, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_P4CJrNbRYsG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.45))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DStG_NLPRYsJ"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/a.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKFX7q_dRYsK",
    "outputId": "4c488aa4-7b0a-4590-fbc2-89298d041751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "357/357 [==============================] - 37s 65ms/step - loss: 1.7320 - accuracy: 0.4611 - val_loss: 2.2054 - val_accuracy: 0.2724\n",
      "Epoch 2/200\n",
      "357/357 [==============================] - 24s 65ms/step - loss: 1.1569 - accuracy: 0.6273 - val_loss: 0.9640 - val_accuracy: 0.6789\n",
      "Epoch 3/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.9983 - accuracy: 0.6748 - val_loss: 0.8978 - val_accuracy: 0.7003\n",
      "Epoch 4/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.9134 - accuracy: 0.7043 - val_loss: 1.0133 - val_accuracy: 0.6631\n",
      "Epoch 5/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.8460 - accuracy: 0.7269 - val_loss: 0.7374 - val_accuracy: 0.7589\n",
      "Epoch 6/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.8004 - accuracy: 0.7420 - val_loss: 0.6980 - val_accuracy: 0.7669\n",
      "Epoch 7/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.7677 - accuracy: 0.7521 - val_loss: 0.8913 - val_accuracy: 0.7109\n",
      "Epoch 8/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.7349 - accuracy: 0.7624 - val_loss: 0.7456 - val_accuracy: 0.7492\n",
      "Epoch 9/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.7171 - accuracy: 0.7680 - val_loss: 0.6430 - val_accuracy: 0.7900\n",
      "Epoch 10/200\n",
      "357/357 [==============================] - 24s 67ms/step - loss: 0.6941 - accuracy: 0.7750 - val_loss: 0.6335 - val_accuracy: 0.7933\n",
      "Epoch 11/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.6794 - accuracy: 0.7794 - val_loss: 0.7400 - val_accuracy: 0.7569\n",
      "Epoch 12/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.6666 - accuracy: 0.7841 - val_loss: 0.6170 - val_accuracy: 0.7940\n",
      "Epoch 13/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.6482 - accuracy: 0.7894 - val_loss: 0.6111 - val_accuracy: 0.7983\n",
      "Epoch 14/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.6386 - accuracy: 0.7932 - val_loss: 0.5925 - val_accuracy: 0.8008\n",
      "Epoch 15/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.6268 - accuracy: 0.7980 - val_loss: 0.5811 - val_accuracy: 0.8072\n",
      "Epoch 16/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.6174 - accuracy: 0.7997 - val_loss: 0.5663 - val_accuracy: 0.8171\n",
      "Epoch 17/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.6087 - accuracy: 0.8022 - val_loss: 0.6147 - val_accuracy: 0.7976\n",
      "Epoch 18/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.6014 - accuracy: 0.8045 - val_loss: 0.5711 - val_accuracy: 0.8140\n",
      "Epoch 19/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5919 - accuracy: 0.8095 - val_loss: 0.6002 - val_accuracy: 0.8012\n",
      "Epoch 20/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5868 - accuracy: 0.8098 - val_loss: 0.5770 - val_accuracy: 0.8051\n",
      "Epoch 21/200\n",
      "357/357 [==============================] - 24s 65ms/step - loss: 0.5837 - accuracy: 0.8110 - val_loss: 0.5661 - val_accuracy: 0.8109\n",
      "Epoch 22/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5736 - accuracy: 0.8143 - val_loss: 0.6379 - val_accuracy: 0.7987\n",
      "Epoch 23/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.5656 - accuracy: 0.8161 - val_loss: 0.5616 - val_accuracy: 0.8189\n",
      "Epoch 24/200\n",
      "357/357 [==============================] - 24s 67ms/step - loss: 0.5610 - accuracy: 0.8173 - val_loss: 0.5607 - val_accuracy: 0.8152\n",
      "Epoch 25/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5599 - accuracy: 0.8178 - val_loss: 0.5823 - val_accuracy: 0.8069\n",
      "Epoch 26/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.5525 - accuracy: 0.8210 - val_loss: 0.5481 - val_accuracy: 0.8196\n",
      "Epoch 27/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5503 - accuracy: 0.8213 - val_loss: 0.5515 - val_accuracy: 0.8203\n",
      "Epoch 28/200\n",
      "357/357 [==============================] - 24s 67ms/step - loss: 0.5384 - accuracy: 0.8235 - val_loss: 0.5131 - val_accuracy: 0.8307\n",
      "Epoch 29/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5392 - accuracy: 0.8249 - val_loss: 0.5311 - val_accuracy: 0.8301\n",
      "Epoch 30/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5387 - accuracy: 0.8245 - val_loss: 0.5303 - val_accuracy: 0.8277\n",
      "Epoch 31/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5370 - accuracy: 0.8261 - val_loss: 0.5257 - val_accuracy: 0.8284\n",
      "Epoch 32/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5329 - accuracy: 0.8272 - val_loss: 0.5308 - val_accuracy: 0.8236\n",
      "Epoch 33/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5224 - accuracy: 0.8298 - val_loss: 0.5461 - val_accuracy: 0.8205\n",
      "Epoch 34/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.5253 - accuracy: 0.8288 - val_loss: 0.4991 - val_accuracy: 0.8367\n",
      "Epoch 35/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5223 - accuracy: 0.8295 - val_loss: 0.5322 - val_accuracy: 0.8236\n",
      "Epoch 36/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5197 - accuracy: 0.8298 - val_loss: 0.5066 - val_accuracy: 0.8324\n",
      "Epoch 37/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.5164 - accuracy: 0.8303 - val_loss: 0.6063 - val_accuracy: 0.8025\n",
      "Epoch 38/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5052 - accuracy: 0.8351 - val_loss: 0.5129 - val_accuracy: 0.8291\n",
      "Epoch 39/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5057 - accuracy: 0.8348 - val_loss: 0.5220 - val_accuracy: 0.8319\n",
      "Epoch 40/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.5072 - accuracy: 0.8349 - val_loss: 0.5314 - val_accuracy: 0.8288\n",
      "Epoch 41/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5037 - accuracy: 0.8353 - val_loss: 0.5313 - val_accuracy: 0.8265\n",
      "Epoch 42/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5031 - accuracy: 0.8359 - val_loss: 0.5790 - val_accuracy: 0.8116\n",
      "Epoch 43/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.5016 - accuracy: 0.8363 - val_loss: 0.5284 - val_accuracy: 0.8296\n",
      "Epoch 44/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4961 - accuracy: 0.8380 - val_loss: 0.5143 - val_accuracy: 0.8357\n",
      "Epoch 45/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4958 - accuracy: 0.8388 - val_loss: 0.5257 - val_accuracy: 0.8291\n",
      "Epoch 46/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4942 - accuracy: 0.8384 - val_loss: 0.4858 - val_accuracy: 0.8427\n",
      "Epoch 47/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4904 - accuracy: 0.8391 - val_loss: 0.4958 - val_accuracy: 0.8393\n",
      "Epoch 48/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4904 - accuracy: 0.8395 - val_loss: 0.5750 - val_accuracy: 0.8068\n",
      "Epoch 49/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4872 - accuracy: 0.8402 - val_loss: 0.5427 - val_accuracy: 0.8257\n",
      "Epoch 50/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4849 - accuracy: 0.8405 - val_loss: 0.4883 - val_accuracy: 0.8373\n",
      "Epoch 51/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4821 - accuracy: 0.8419 - val_loss: 0.5550 - val_accuracy: 0.8157\n",
      "Epoch 52/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4762 - accuracy: 0.8435 - val_loss: 0.4880 - val_accuracy: 0.8392\n",
      "Epoch 53/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4807 - accuracy: 0.8421 - val_loss: 0.4829 - val_accuracy: 0.8432\n",
      "Epoch 54/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4768 - accuracy: 0.8428 - val_loss: 0.4925 - val_accuracy: 0.8361\n",
      "Epoch 55/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4736 - accuracy: 0.8450 - val_loss: 0.5172 - val_accuracy: 0.8309\n",
      "Epoch 56/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4764 - accuracy: 0.8432 - val_loss: 0.4857 - val_accuracy: 0.8364\n",
      "Epoch 57/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4729 - accuracy: 0.8433 - val_loss: 0.5384 - val_accuracy: 0.8275\n",
      "Epoch 58/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4745 - accuracy: 0.8431 - val_loss: 0.4810 - val_accuracy: 0.8411\n",
      "Epoch 59/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4733 - accuracy: 0.8441 - val_loss: 0.4914 - val_accuracy: 0.8392\n",
      "Epoch 60/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4717 - accuracy: 0.8437 - val_loss: 0.5088 - val_accuracy: 0.8352\n",
      "Epoch 61/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4655 - accuracy: 0.8466 - val_loss: 0.4766 - val_accuracy: 0.8449\n",
      "Epoch 62/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4666 - accuracy: 0.8463 - val_loss: 0.4945 - val_accuracy: 0.8384\n",
      "Epoch 63/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4708 - accuracy: 0.8468 - val_loss: 0.5178 - val_accuracy: 0.8256\n",
      "Epoch 64/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4633 - accuracy: 0.8483 - val_loss: 0.4808 - val_accuracy: 0.8436\n",
      "Epoch 65/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4618 - accuracy: 0.8479 - val_loss: 0.5047 - val_accuracy: 0.8392\n",
      "Epoch 66/200\n",
      "357/357 [==============================] - 23s 65ms/step - loss: 0.4593 - accuracy: 0.8478 - val_loss: 0.4906 - val_accuracy: 0.8404\n",
      "Epoch 67/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4607 - accuracy: 0.8478 - val_loss: 0.5106 - val_accuracy: 0.8307\n",
      "Epoch 68/200\n",
      "357/357 [==============================] - 25s 68ms/step - loss: 0.4583 - accuracy: 0.8480 - val_loss: 0.4743 - val_accuracy: 0.8443\n",
      "Epoch 69/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4580 - accuracy: 0.8484 - val_loss: 0.4684 - val_accuracy: 0.8437\n",
      "Epoch 70/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4533 - accuracy: 0.8494 - val_loss: 0.5455 - val_accuracy: 0.8149\n",
      "Epoch 71/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4542 - accuracy: 0.8504 - val_loss: 0.5162 - val_accuracy: 0.8299\n",
      "Epoch 72/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4561 - accuracy: 0.8500 - val_loss: 0.4730 - val_accuracy: 0.8412\n",
      "Epoch 73/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4519 - accuracy: 0.8502 - val_loss: 0.4771 - val_accuracy: 0.8452\n",
      "Epoch 74/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4512 - accuracy: 0.8500 - val_loss: 0.4645 - val_accuracy: 0.8468\n",
      "Epoch 75/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4517 - accuracy: 0.8505 - val_loss: 0.4641 - val_accuracy: 0.8491\n",
      "Epoch 76/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4505 - accuracy: 0.8508 - val_loss: 0.4673 - val_accuracy: 0.8487\n",
      "Epoch 77/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4482 - accuracy: 0.8519 - val_loss: 0.4903 - val_accuracy: 0.8389\n",
      "Epoch 78/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4489 - accuracy: 0.8521 - val_loss: 0.4844 - val_accuracy: 0.8396\n",
      "Epoch 79/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4470 - accuracy: 0.8504 - val_loss: 0.4846 - val_accuracy: 0.8404\n",
      "Epoch 80/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4476 - accuracy: 0.8518 - val_loss: 0.4822 - val_accuracy: 0.8476\n",
      "Epoch 81/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4473 - accuracy: 0.8518 - val_loss: 0.5347 - val_accuracy: 0.8263\n",
      "Epoch 82/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4433 - accuracy: 0.8536 - val_loss: 0.4731 - val_accuracy: 0.8453\n",
      "Epoch 83/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4386 - accuracy: 0.8549 - val_loss: 0.5159 - val_accuracy: 0.8289\n",
      "Epoch 84/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4426 - accuracy: 0.8534 - val_loss: 0.4950 - val_accuracy: 0.8368\n",
      "Epoch 85/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4430 - accuracy: 0.8542 - val_loss: 0.4673 - val_accuracy: 0.8492\n",
      "Epoch 86/200\n",
      "357/357 [==============================] - 24s 65ms/step - loss: 0.4406 - accuracy: 0.8550 - val_loss: 0.4636 - val_accuracy: 0.8492\n",
      "Epoch 87/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4396 - accuracy: 0.8543 - val_loss: 0.4945 - val_accuracy: 0.8344\n",
      "Epoch 88/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4367 - accuracy: 0.8548 - val_loss: 0.4737 - val_accuracy: 0.8448\n",
      "Epoch 89/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4381 - accuracy: 0.8530 - val_loss: 0.4736 - val_accuracy: 0.8452\n",
      "Epoch 90/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4383 - accuracy: 0.8550 - val_loss: 0.4725 - val_accuracy: 0.8408\n",
      "Epoch 91/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4380 - accuracy: 0.8556 - val_loss: 0.4907 - val_accuracy: 0.8439\n",
      "Epoch 92/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4326 - accuracy: 0.8567 - val_loss: 0.4857 - val_accuracy: 0.8447\n",
      "Epoch 93/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4360 - accuracy: 0.8558 - val_loss: 0.4909 - val_accuracy: 0.8404\n",
      "Epoch 94/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4346 - accuracy: 0.8550 - val_loss: 0.4746 - val_accuracy: 0.8449\n",
      "Epoch 95/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4349 - accuracy: 0.8550 - val_loss: 0.4924 - val_accuracy: 0.8383\n",
      "Epoch 96/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4335 - accuracy: 0.8557 - val_loss: 0.4723 - val_accuracy: 0.8459\n",
      "Epoch 97/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4336 - accuracy: 0.8557 - val_loss: 0.5639 - val_accuracy: 0.8147\n",
      "Epoch 98/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4308 - accuracy: 0.8569 - val_loss: 0.4647 - val_accuracy: 0.8496\n",
      "Epoch 99/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4332 - accuracy: 0.8561 - val_loss: 0.4642 - val_accuracy: 0.8483\n",
      "Epoch 100/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4300 - accuracy: 0.8570 - val_loss: 0.4627 - val_accuracy: 0.8471\n",
      "Epoch 101/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4310 - accuracy: 0.8566 - val_loss: 0.4781 - val_accuracy: 0.8497\n",
      "Epoch 102/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4261 - accuracy: 0.8581 - val_loss: 0.4875 - val_accuracy: 0.8412\n",
      "Epoch 103/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4283 - accuracy: 0.8574 - val_loss: 0.4838 - val_accuracy: 0.8449\n",
      "Epoch 104/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4266 - accuracy: 0.8576 - val_loss: 0.4980 - val_accuracy: 0.8353\n",
      "Epoch 105/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4300 - accuracy: 0.8580 - val_loss: 0.4741 - val_accuracy: 0.8476\n",
      "Epoch 106/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4269 - accuracy: 0.8579 - val_loss: 0.5112 - val_accuracy: 0.8327\n",
      "Epoch 107/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4255 - accuracy: 0.8576 - val_loss: 0.4651 - val_accuracy: 0.8481\n",
      "Epoch 108/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4258 - accuracy: 0.8590 - val_loss: 0.4731 - val_accuracy: 0.8461\n",
      "Epoch 109/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4283 - accuracy: 0.8580 - val_loss: 0.4593 - val_accuracy: 0.8457\n",
      "Epoch 110/200\n",
      "357/357 [==============================] - 24s 65ms/step - loss: 0.4225 - accuracy: 0.8587 - val_loss: 0.4574 - val_accuracy: 0.8533\n",
      "Epoch 111/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4258 - accuracy: 0.8577 - val_loss: 0.4672 - val_accuracy: 0.8484\n",
      "Epoch 112/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4196 - accuracy: 0.8597 - val_loss: 0.4917 - val_accuracy: 0.8431\n",
      "Epoch 113/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4209 - accuracy: 0.8598 - val_loss: 0.4818 - val_accuracy: 0.8415\n",
      "Epoch 114/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4214 - accuracy: 0.8591 - val_loss: 0.4632 - val_accuracy: 0.8472\n",
      "Epoch 115/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4237 - accuracy: 0.8586 - val_loss: 0.4718 - val_accuracy: 0.8461\n",
      "Epoch 116/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4238 - accuracy: 0.8593 - val_loss: 0.4940 - val_accuracy: 0.8379\n",
      "Epoch 117/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4195 - accuracy: 0.8606 - val_loss: 0.4750 - val_accuracy: 0.8461\n",
      "Epoch 118/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4209 - accuracy: 0.8605 - val_loss: 0.4623 - val_accuracy: 0.8507\n",
      "Epoch 119/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4172 - accuracy: 0.8605 - val_loss: 0.4944 - val_accuracy: 0.8396\n",
      "Epoch 120/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4193 - accuracy: 0.8612 - val_loss: 0.4692 - val_accuracy: 0.8481\n",
      "Epoch 121/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4171 - accuracy: 0.8600 - val_loss: 0.4597 - val_accuracy: 0.8492\n",
      "Epoch 122/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4188 - accuracy: 0.8610 - val_loss: 0.4659 - val_accuracy: 0.8475\n",
      "Epoch 123/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4184 - accuracy: 0.8608 - val_loss: 0.4551 - val_accuracy: 0.8543\n",
      "Epoch 124/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4162 - accuracy: 0.8610 - val_loss: 0.4574 - val_accuracy: 0.8516\n",
      "Epoch 125/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4155 - accuracy: 0.8624 - val_loss: 0.4659 - val_accuracy: 0.8488\n",
      "Epoch 126/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4159 - accuracy: 0.8603 - val_loss: 0.4811 - val_accuracy: 0.8452\n",
      "Epoch 127/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4121 - accuracy: 0.8622 - val_loss: 0.4528 - val_accuracy: 0.8544\n",
      "Epoch 128/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4134 - accuracy: 0.8617 - val_loss: 0.4608 - val_accuracy: 0.8533\n",
      "Epoch 129/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4131 - accuracy: 0.8620 - val_loss: 0.4583 - val_accuracy: 0.8528\n",
      "Epoch 130/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4179 - accuracy: 0.8600 - val_loss: 0.4573 - val_accuracy: 0.8471\n",
      "Epoch 131/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4106 - accuracy: 0.8635 - val_loss: 0.4674 - val_accuracy: 0.8457\n",
      "Epoch 132/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4133 - accuracy: 0.8625 - val_loss: 0.4686 - val_accuracy: 0.8485\n",
      "Epoch 133/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4155 - accuracy: 0.8614 - val_loss: 0.4621 - val_accuracy: 0.8485\n",
      "Epoch 134/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4116 - accuracy: 0.8634 - val_loss: 0.4687 - val_accuracy: 0.8492\n",
      "Epoch 135/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4133 - accuracy: 0.8629 - val_loss: 0.4588 - val_accuracy: 0.8493\n",
      "Epoch 136/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4121 - accuracy: 0.8617 - val_loss: 0.4531 - val_accuracy: 0.8488\n",
      "Epoch 137/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4092 - accuracy: 0.8646 - val_loss: 0.4864 - val_accuracy: 0.8400\n",
      "Epoch 138/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4100 - accuracy: 0.8627 - val_loss: 0.5148 - val_accuracy: 0.8328\n",
      "Epoch 139/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4094 - accuracy: 0.8636 - val_loss: 0.4634 - val_accuracy: 0.8463\n",
      "Epoch 140/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4087 - accuracy: 0.8646 - val_loss: 0.4573 - val_accuracy: 0.8513\n",
      "Epoch 141/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4069 - accuracy: 0.8635 - val_loss: 0.4567 - val_accuracy: 0.8528\n",
      "Epoch 142/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4089 - accuracy: 0.8632 - val_loss: 0.4646 - val_accuracy: 0.8529\n",
      "Epoch 143/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4068 - accuracy: 0.8632 - val_loss: 0.4668 - val_accuracy: 0.8467\n",
      "Epoch 144/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4081 - accuracy: 0.8635 - val_loss: 0.4663 - val_accuracy: 0.8461\n",
      "Epoch 145/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4068 - accuracy: 0.8645 - val_loss: 0.4541 - val_accuracy: 0.8515\n",
      "Epoch 146/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4069 - accuracy: 0.8634 - val_loss: 0.4613 - val_accuracy: 0.8533\n",
      "Epoch 147/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4088 - accuracy: 0.8644 - val_loss: 0.4563 - val_accuracy: 0.8528\n",
      "Epoch 148/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4041 - accuracy: 0.8644 - val_loss: 0.4614 - val_accuracy: 0.8509\n",
      "Epoch 149/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4049 - accuracy: 0.8647 - val_loss: 0.4625 - val_accuracy: 0.8503\n",
      "Epoch 150/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4040 - accuracy: 0.8650 - val_loss: 0.4571 - val_accuracy: 0.8509\n",
      "Epoch 151/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4055 - accuracy: 0.8659 - val_loss: 0.4573 - val_accuracy: 0.8503\n",
      "Epoch 152/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4046 - accuracy: 0.8636 - val_loss: 0.4638 - val_accuracy: 0.8512\n",
      "Epoch 153/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4065 - accuracy: 0.8639 - val_loss: 0.4652 - val_accuracy: 0.8461\n",
      "Epoch 154/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.4033 - accuracy: 0.8658 - val_loss: 0.4582 - val_accuracy: 0.8507\n",
      "Epoch 155/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4035 - accuracy: 0.8664 - val_loss: 0.4566 - val_accuracy: 0.8501\n",
      "Epoch 156/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4049 - accuracy: 0.8650 - val_loss: 0.4554 - val_accuracy: 0.8529\n",
      "Epoch 157/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4043 - accuracy: 0.8650 - val_loss: 0.4755 - val_accuracy: 0.8445\n",
      "Epoch 158/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4028 - accuracy: 0.8663 - val_loss: 0.4659 - val_accuracy: 0.8491\n",
      "Epoch 159/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4032 - accuracy: 0.8653 - val_loss: 0.4502 - val_accuracy: 0.8553\n",
      "Epoch 160/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3994 - accuracy: 0.8664 - val_loss: 0.4565 - val_accuracy: 0.8571\n",
      "Epoch 161/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.3993 - accuracy: 0.8661 - val_loss: 0.4629 - val_accuracy: 0.8513\n",
      "Epoch 162/200\n",
      "357/357 [==============================] - 24s 66ms/step - loss: 0.4012 - accuracy: 0.8663 - val_loss: 0.4472 - val_accuracy: 0.8551\n",
      "Epoch 163/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3984 - accuracy: 0.8655 - val_loss: 0.4499 - val_accuracy: 0.8512\n",
      "Epoch 164/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4007 - accuracy: 0.8653 - val_loss: 0.4646 - val_accuracy: 0.8499\n",
      "Epoch 165/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4004 - accuracy: 0.8655 - val_loss: 0.4587 - val_accuracy: 0.8484\n",
      "Epoch 166/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4008 - accuracy: 0.8652 - val_loss: 0.4745 - val_accuracy: 0.8495\n",
      "Epoch 167/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4007 - accuracy: 0.8663 - val_loss: 0.4828 - val_accuracy: 0.8439\n",
      "Epoch 168/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3986 - accuracy: 0.8660 - val_loss: 0.4503 - val_accuracy: 0.8516\n",
      "Epoch 169/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.3965 - accuracy: 0.8665 - val_loss: 0.4641 - val_accuracy: 0.8528\n",
      "Epoch 170/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4001 - accuracy: 0.8657 - val_loss: 0.4727 - val_accuracy: 0.8501\n",
      "Epoch 171/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3964 - accuracy: 0.8666 - val_loss: 0.4590 - val_accuracy: 0.8548\n",
      "Epoch 172/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4000 - accuracy: 0.8667 - val_loss: 0.4606 - val_accuracy: 0.8481\n",
      "Epoch 173/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3990 - accuracy: 0.8666 - val_loss: 0.4722 - val_accuracy: 0.8520\n",
      "Epoch 174/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3975 - accuracy: 0.8665 - val_loss: 0.4729 - val_accuracy: 0.8540\n",
      "Epoch 175/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3964 - accuracy: 0.8668 - val_loss: 0.4650 - val_accuracy: 0.8483\n",
      "Epoch 176/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3980 - accuracy: 0.8668 - val_loss: 0.4529 - val_accuracy: 0.8527\n",
      "Epoch 177/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3994 - accuracy: 0.8656 - val_loss: 0.4578 - val_accuracy: 0.8539\n",
      "Epoch 178/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3955 - accuracy: 0.8679 - val_loss: 0.4606 - val_accuracy: 0.8529\n",
      "Epoch 179/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3970 - accuracy: 0.8675 - val_loss: 0.4927 - val_accuracy: 0.8444\n",
      "Epoch 180/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3978 - accuracy: 0.8669 - val_loss: 0.4967 - val_accuracy: 0.8431\n",
      "Epoch 181/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3964 - accuracy: 0.8674 - val_loss: 0.4726 - val_accuracy: 0.8489\n",
      "Epoch 182/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3885 - accuracy: 0.8690 - val_loss: 0.4620 - val_accuracy: 0.8491\n",
      "Epoch 183/200\n",
      "357/357 [==============================] - 24s 65ms/step - loss: 0.3929 - accuracy: 0.8682 - val_loss: 0.4741 - val_accuracy: 0.8456\n",
      "Epoch 184/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3964 - accuracy: 0.8676 - val_loss: 0.4791 - val_accuracy: 0.8440\n",
      "Epoch 185/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3934 - accuracy: 0.8682 - val_loss: 0.4507 - val_accuracy: 0.8529\n",
      "Epoch 186/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3926 - accuracy: 0.8672 - val_loss: 0.4656 - val_accuracy: 0.8504\n",
      "Epoch 187/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3964 - accuracy: 0.8664 - val_loss: 0.4547 - val_accuracy: 0.8540\n",
      "Epoch 188/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3946 - accuracy: 0.8669 - val_loss: 0.4663 - val_accuracy: 0.8492\n",
      "Epoch 189/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3967 - accuracy: 0.8681 - val_loss: 0.4599 - val_accuracy: 0.8507\n",
      "Epoch 190/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3949 - accuracy: 0.8670 - val_loss: 0.4563 - val_accuracy: 0.8515\n",
      "Epoch 191/200\n",
      "357/357 [==============================] - 23s 65ms/step - loss: 0.3980 - accuracy: 0.8665 - val_loss: 0.4573 - val_accuracy: 0.8539\n",
      "Epoch 192/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.3937 - accuracy: 0.8667 - val_loss: 0.4682 - val_accuracy: 0.8499\n",
      "Epoch 193/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3926 - accuracy: 0.8680 - val_loss: 0.4903 - val_accuracy: 0.8445\n",
      "Epoch 194/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3943 - accuracy: 0.8685 - val_loss: 0.4801 - val_accuracy: 0.8457\n",
      "Epoch 195/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.3945 - accuracy: 0.8674 - val_loss: 0.4800 - val_accuracy: 0.8477\n",
      "Epoch 196/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3925 - accuracy: 0.8681 - val_loss: 0.4478 - val_accuracy: 0.8513\n",
      "Epoch 197/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3904 - accuracy: 0.8693 - val_loss: 0.4528 - val_accuracy: 0.8515\n",
      "Epoch 198/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.3947 - accuracy: 0.8660 - val_loss: 0.4667 - val_accuracy: 0.8480\n",
      "Epoch 199/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3898 - accuracy: 0.8687 - val_loss: 0.4562 - val_accuracy: 0.8519\n",
      "Epoch 200/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3893 - accuracy: 0.8687 - val_loss: 0.4872 - val_accuracy: 0.8404\n"
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJv_AEUY7gJ4"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/a.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ajocLgX7gJ4",
    "outputId": "3ea705ee-cbc4-42e8-a1b3-2d57e6883bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 1s - loss: 0.4472 - accuracy: 0.8551 - 1s/epoch - 44ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "0-pgO6Ackkrc",
    "outputId": "1be36a60-d619-46c5-8f28-d3e88a2f857c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy and loss values for both the training and validation sets\n",
    "fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "ax1.plot(epochs, accuracy, 'b--', label='Training accuracy')\n",
    "ax1.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "ax2.plot(epochs, loss, 'b--', label='Training loss')\n",
    "ax2.plot(epochs, val_loss, 'b',label='Validation loss')\n",
    "ax1.set_title('Accuracy')\n",
    "ax2.set_title('Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "\n",
    "ax2.set_xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFysm4xdcC0s"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XTxj5KiO8Ql"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/b.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "40JtkJdnO8Ql",
    "outputId": "ec4bb4ef-02a3-4db3-ae7c-3f14198c7906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "713/713 [==============================] - 28s 34ms/step - loss: 1.4580 - accuracy: 0.5394 - val_loss: 0.9895 - val_accuracy: 0.6664\n",
      "Epoch 2/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 1.0254 - accuracy: 0.6646 - val_loss: 1.0011 - val_accuracy: 0.6731\n",
      "Epoch 3/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.8968 - accuracy: 0.7085 - val_loss: 0.8194 - val_accuracy: 0.7281\n",
      "Epoch 4/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.8166 - accuracy: 0.7355 - val_loss: 0.7454 - val_accuracy: 0.7588\n",
      "Epoch 5/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.7602 - accuracy: 0.7551 - val_loss: 0.7551 - val_accuracy: 0.7615\n",
      "Epoch 6/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.7179 - accuracy: 0.7704 - val_loss: 1.0164 - val_accuracy: 0.6868\n",
      "Epoch 7/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.6878 - accuracy: 0.7788 - val_loss: 0.6720 - val_accuracy: 0.7823\n",
      "Epoch 8/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.6639 - accuracy: 0.7868 - val_loss: 0.6288 - val_accuracy: 0.7952\n",
      "Epoch 9/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6383 - accuracy: 0.7957 - val_loss: 0.6416 - val_accuracy: 0.7893\n",
      "Epoch 10/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.6195 - accuracy: 0.8012 - val_loss: 0.6162 - val_accuracy: 0.7947\n",
      "Epoch 11/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.6033 - accuracy: 0.8059 - val_loss: 0.5566 - val_accuracy: 0.8117\n",
      "Epoch 12/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5883 - accuracy: 0.8087 - val_loss: 0.6238 - val_accuracy: 0.8065\n",
      "Epoch 13/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.5736 - accuracy: 0.8152 - val_loss: 0.5291 - val_accuracy: 0.8287\n",
      "Epoch 14/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5641 - accuracy: 0.8173 - val_loss: 0.5800 - val_accuracy: 0.8131\n",
      "Epoch 15/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5519 - accuracy: 0.8218 - val_loss: 0.5731 - val_accuracy: 0.8129\n",
      "Epoch 16/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5408 - accuracy: 0.8252 - val_loss: 0.5672 - val_accuracy: 0.8160\n",
      "Epoch 17/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5317 - accuracy: 0.8274 - val_loss: 0.5631 - val_accuracy: 0.8175\n",
      "Epoch 18/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5201 - accuracy: 0.8319 - val_loss: 0.5629 - val_accuracy: 0.8172\n",
      "Epoch 19/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5165 - accuracy: 0.8328 - val_loss: 0.5754 - val_accuracy: 0.8119\n",
      "Epoch 20/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5094 - accuracy: 0.8349 - val_loss: 0.5326 - val_accuracy: 0.8311\n",
      "Epoch 21/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4995 - accuracy: 0.8380 - val_loss: 0.5215 - val_accuracy: 0.8339\n",
      "Epoch 22/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4912 - accuracy: 0.8390 - val_loss: 0.5442 - val_accuracy: 0.8152\n",
      "Epoch 23/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4857 - accuracy: 0.8402 - val_loss: 0.5441 - val_accuracy: 0.8245\n",
      "Epoch 24/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.4854 - accuracy: 0.8416 - val_loss: 0.5020 - val_accuracy: 0.8405\n",
      "Epoch 25/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4747 - accuracy: 0.8456 - val_loss: 0.5160 - val_accuracy: 0.8352\n",
      "Epoch 26/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4704 - accuracy: 0.8465 - val_loss: 0.5115 - val_accuracy: 0.8383\n",
      "Epoch 27/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4670 - accuracy: 0.8477 - val_loss: 0.5023 - val_accuracy: 0.8429\n",
      "Epoch 28/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.4622 - accuracy: 0.8490 - val_loss: 0.4889 - val_accuracy: 0.8456\n",
      "Epoch 29/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4587 - accuracy: 0.8491 - val_loss: 0.5098 - val_accuracy: 0.8367\n",
      "Epoch 30/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4552 - accuracy: 0.8509 - val_loss: 0.5801 - val_accuracy: 0.8160\n",
      "Epoch 31/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4502 - accuracy: 0.8517 - val_loss: 0.5140 - val_accuracy: 0.8409\n",
      "Epoch 32/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4493 - accuracy: 0.8523 - val_loss: 0.5251 - val_accuracy: 0.8361\n",
      "Epoch 33/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4396 - accuracy: 0.8550 - val_loss: 0.5388 - val_accuracy: 0.8351\n",
      "Epoch 34/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4390 - accuracy: 0.8569 - val_loss: 0.5667 - val_accuracy: 0.8199\n",
      "Epoch 35/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.4381 - accuracy: 0.8551 - val_loss: 0.4876 - val_accuracy: 0.8453\n",
      "Epoch 36/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4346 - accuracy: 0.8571 - val_loss: 0.5015 - val_accuracy: 0.8412\n",
      "Epoch 37/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4294 - accuracy: 0.8585 - val_loss: 0.4924 - val_accuracy: 0.8452\n",
      "Epoch 38/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4285 - accuracy: 0.8581 - val_loss: 0.4881 - val_accuracy: 0.8437\n",
      "Epoch 39/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4267 - accuracy: 0.8594 - val_loss: 0.5279 - val_accuracy: 0.8319\n",
      "Epoch 40/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4213 - accuracy: 0.8610 - val_loss: 0.4856 - val_accuracy: 0.8469\n",
      "Epoch 41/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4198 - accuracy: 0.8611 - val_loss: 0.4865 - val_accuracy: 0.8423\n",
      "Epoch 42/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4164 - accuracy: 0.8621 - val_loss: 0.5016 - val_accuracy: 0.8423\n",
      "Epoch 43/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4143 - accuracy: 0.8631 - val_loss: 0.4893 - val_accuracy: 0.8451\n",
      "Epoch 44/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.4107 - accuracy: 0.8637 - val_loss: 0.4791 - val_accuracy: 0.8488\n",
      "Epoch 45/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4085 - accuracy: 0.8650 - val_loss: 0.4833 - val_accuracy: 0.8487\n",
      "Epoch 46/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4065 - accuracy: 0.8651 - val_loss: 0.4961 - val_accuracy: 0.8457\n",
      "Epoch 47/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4078 - accuracy: 0.8650 - val_loss: 0.4997 - val_accuracy: 0.8485\n",
      "Epoch 48/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4004 - accuracy: 0.8667 - val_loss: 0.5241 - val_accuracy: 0.8380\n",
      "Epoch 49/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3958 - accuracy: 0.8689 - val_loss: 0.5050 - val_accuracy: 0.8416\n",
      "Epoch 50/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4008 - accuracy: 0.8678 - val_loss: 0.5157 - val_accuracy: 0.8417\n",
      "Epoch 51/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.3978 - accuracy: 0.8678 - val_loss: 0.4663 - val_accuracy: 0.8501\n",
      "Epoch 52/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3928 - accuracy: 0.8688 - val_loss: 0.5109 - val_accuracy: 0.8391\n",
      "Epoch 53/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3953 - accuracy: 0.8683 - val_loss: 0.4860 - val_accuracy: 0.8440\n",
      "Epoch 54/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3902 - accuracy: 0.8692 - val_loss: 0.4674 - val_accuracy: 0.8519\n",
      "Epoch 55/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3919 - accuracy: 0.8695 - val_loss: 0.4820 - val_accuracy: 0.8400\n",
      "Epoch 56/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3881 - accuracy: 0.8696 - val_loss: 0.5042 - val_accuracy: 0.8376\n",
      "Epoch 57/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3873 - accuracy: 0.8706 - val_loss: 0.5002 - val_accuracy: 0.8437\n",
      "Epoch 58/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3820 - accuracy: 0.8719 - val_loss: 0.4934 - val_accuracy: 0.8487\n",
      "Epoch 59/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3852 - accuracy: 0.8709 - val_loss: 0.4801 - val_accuracy: 0.8455\n",
      "Epoch 60/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.3832 - accuracy: 0.8720 - val_loss: 0.4662 - val_accuracy: 0.8521\n",
      "Epoch 61/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3823 - accuracy: 0.8718 - val_loss: 0.4753 - val_accuracy: 0.8513\n",
      "Epoch 62/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3797 - accuracy: 0.8724 - val_loss: 0.4970 - val_accuracy: 0.8416\n",
      "Epoch 63/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3797 - accuracy: 0.8731 - val_loss: 0.5468 - val_accuracy: 0.8351\n",
      "Epoch 64/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3749 - accuracy: 0.8729 - val_loss: 0.4698 - val_accuracy: 0.8543\n",
      "Epoch 65/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3754 - accuracy: 0.8734 - val_loss: 0.4921 - val_accuracy: 0.8472\n",
      "Epoch 66/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3726 - accuracy: 0.8742 - val_loss: 0.5126 - val_accuracy: 0.8432\n",
      "Epoch 67/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3743 - accuracy: 0.8733 - val_loss: 0.4885 - val_accuracy: 0.8507\n",
      "Epoch 68/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3713 - accuracy: 0.8753 - val_loss: 0.5384 - val_accuracy: 0.8253\n",
      "Epoch 69/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3705 - accuracy: 0.8753 - val_loss: 0.5120 - val_accuracy: 0.8373\n",
      "Epoch 70/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3662 - accuracy: 0.8765 - val_loss: 0.4914 - val_accuracy: 0.8397\n",
      "Epoch 71/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3675 - accuracy: 0.8745 - val_loss: 0.4851 - val_accuracy: 0.8435\n",
      "Epoch 72/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3675 - accuracy: 0.8756 - val_loss: 0.4722 - val_accuracy: 0.8533\n",
      "Epoch 73/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3649 - accuracy: 0.8757 - val_loss: 0.4993 - val_accuracy: 0.8436\n",
      "Epoch 74/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3638 - accuracy: 0.8764 - val_loss: 0.5069 - val_accuracy: 0.8455\n",
      "Epoch 75/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3624 - accuracy: 0.8776 - val_loss: 0.4772 - val_accuracy: 0.8493\n",
      "Epoch 76/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3594 - accuracy: 0.8779 - val_loss: 0.4689 - val_accuracy: 0.8492\n",
      "Epoch 77/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3632 - accuracy: 0.8766 - val_loss: 0.4967 - val_accuracy: 0.8409\n",
      "Epoch 78/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3619 - accuracy: 0.8777 - val_loss: 0.4948 - val_accuracy: 0.8500\n",
      "Epoch 79/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3571 - accuracy: 0.8796 - val_loss: 0.4747 - val_accuracy: 0.8473\n",
      "Epoch 80/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3589 - accuracy: 0.8788 - val_loss: 0.5016 - val_accuracy: 0.8449\n",
      "Epoch 81/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3608 - accuracy: 0.8781 - val_loss: 0.4904 - val_accuracy: 0.8472\n",
      "Epoch 82/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3574 - accuracy: 0.8793 - val_loss: 0.4796 - val_accuracy: 0.8541\n",
      "Epoch 83/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3555 - accuracy: 0.8798 - val_loss: 0.4902 - val_accuracy: 0.8544\n",
      "Epoch 84/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3540 - accuracy: 0.8801 - val_loss: 0.4768 - val_accuracy: 0.8540\n",
      "Epoch 85/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3546 - accuracy: 0.8796 - val_loss: 0.4732 - val_accuracy: 0.8507\n",
      "Epoch 86/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3544 - accuracy: 0.8793 - val_loss: 0.5190 - val_accuracy: 0.8435\n",
      "Epoch 87/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3548 - accuracy: 0.8787 - val_loss: 0.4789 - val_accuracy: 0.8523\n",
      "Epoch 88/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3509 - accuracy: 0.8804 - val_loss: 0.5150 - val_accuracy: 0.8461\n",
      "Epoch 89/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3508 - accuracy: 0.8807 - val_loss: 0.5099 - val_accuracy: 0.8435\n",
      "Epoch 90/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3518 - accuracy: 0.8803 - val_loss: 0.5275 - val_accuracy: 0.8443\n",
      "Epoch 91/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3483 - accuracy: 0.8824 - val_loss: 0.4883 - val_accuracy: 0.8512\n",
      "Epoch 92/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3496 - accuracy: 0.8805 - val_loss: 0.4879 - val_accuracy: 0.8532\n",
      "Epoch 93/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3474 - accuracy: 0.8817 - val_loss: 0.4968 - val_accuracy: 0.8464\n",
      "Epoch 94/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3474 - accuracy: 0.8812 - val_loss: 0.4966 - val_accuracy: 0.8400\n",
      "Epoch 95/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3463 - accuracy: 0.8807 - val_loss: 0.4996 - val_accuracy: 0.8509\n",
      "Epoch 96/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3458 - accuracy: 0.8817 - val_loss: 0.4734 - val_accuracy: 0.8545\n",
      "Epoch 97/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3425 - accuracy: 0.8840 - val_loss: 0.4778 - val_accuracy: 0.8504\n",
      "Epoch 98/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3468 - accuracy: 0.8812 - val_loss: 0.4877 - val_accuracy: 0.8535\n",
      "Epoch 99/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3427 - accuracy: 0.8831 - val_loss: 0.5307 - val_accuracy: 0.8459\n",
      "Epoch 100/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3447 - accuracy: 0.8815 - val_loss: 0.5000 - val_accuracy: 0.8523\n",
      "Epoch 101/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3440 - accuracy: 0.8818 - val_loss: 0.4718 - val_accuracy: 0.8584\n",
      "Epoch 102/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3418 - accuracy: 0.8828 - val_loss: 0.4802 - val_accuracy: 0.8576\n",
      "Epoch 103/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3377 - accuracy: 0.8837 - val_loss: 0.4887 - val_accuracy: 0.8569\n",
      "Epoch 104/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3398 - accuracy: 0.8832 - val_loss: 0.4924 - val_accuracy: 0.8537\n",
      "Epoch 105/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3402 - accuracy: 0.8843 - val_loss: 0.4756 - val_accuracy: 0.8559\n",
      "Epoch 106/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3417 - accuracy: 0.8829 - val_loss: 0.4952 - val_accuracy: 0.8549\n",
      "Epoch 107/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3404 - accuracy: 0.8843 - val_loss: 0.5034 - val_accuracy: 0.8529\n",
      "Epoch 108/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3388 - accuracy: 0.8838 - val_loss: 0.5081 - val_accuracy: 0.8516\n",
      "Epoch 109/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3367 - accuracy: 0.8835 - val_loss: 0.5216 - val_accuracy: 0.8469\n",
      "Epoch 110/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3388 - accuracy: 0.8840 - val_loss: 0.5567 - val_accuracy: 0.8427\n",
      "Epoch 111/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3351 - accuracy: 0.8857 - val_loss: 0.5049 - val_accuracy: 0.8524\n",
      "Epoch 112/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3366 - accuracy: 0.8848 - val_loss: 0.4861 - val_accuracy: 0.8549\n",
      "Epoch 113/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3353 - accuracy: 0.8855 - val_loss: 0.5084 - val_accuracy: 0.8405\n",
      "Epoch 114/200\n",
      "711/713 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.8830"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-20a1d8010408>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                         )\n\u001b[0;32m-> 1729\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2070\u001b[0m                         ):\n\u001b[1;32m   2071\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CW4ugASguD5A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXmr8pKsuUbz"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/b.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhhX_T3uuUb1",
    "outputId": "d87851a5-e256-439c-f3a9-8098db04db30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 - 1s - loss: 0.4662 - accuracy: 0.8521 - 1s/epoch - 20ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPdwDoCRuEFW"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ON7czqxuEFY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-T3F0XeFuEFZ"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/c.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ASEkgO8FuEFZ",
    "outputId": "ba1ad93a-6957-49f0-e076-b918d199babb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "713/713 [==============================] - 27s 32ms/step - loss: 1.6105 - accuracy: 0.4905 - val_loss: 1.1931 - val_accuracy: 0.6179\n",
      "Epoch 2/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 1.1124 - accuracy: 0.6371 - val_loss: 1.0208 - val_accuracy: 0.6747\n",
      "Epoch 3/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.9742 - accuracy: 0.6813 - val_loss: 1.2035 - val_accuracy: 0.6431\n",
      "Epoch 4/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.8948 - accuracy: 0.7073 - val_loss: 0.9834 - val_accuracy: 0.6873\n",
      "Epoch 5/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.8413 - accuracy: 0.7270 - val_loss: 0.7096 - val_accuracy: 0.7715\n",
      "Epoch 6/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.7910 - accuracy: 0.7450 - val_loss: 0.7397 - val_accuracy: 0.7579\n",
      "Epoch 7/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.7568 - accuracy: 0.7560 - val_loss: 0.9143 - val_accuracy: 0.7209\n",
      "Epoch 8/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.7306 - accuracy: 0.7622 - val_loss: 0.8503 - val_accuracy: 0.7299\n",
      "Epoch 9/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.7061 - accuracy: 0.7724 - val_loss: 0.6520 - val_accuracy: 0.7939\n",
      "Epoch 10/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6880 - accuracy: 0.7789 - val_loss: 0.6600 - val_accuracy: 0.7872\n",
      "Epoch 11/200\n",
      "713/713 [==============================] - 24s 34ms/step - loss: 0.6692 - accuracy: 0.7851 - val_loss: 0.6139 - val_accuracy: 0.7943\n",
      "Epoch 12/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.6542 - accuracy: 0.7883 - val_loss: 0.6083 - val_accuracy: 0.8039\n",
      "Epoch 13/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6389 - accuracy: 0.7942 - val_loss: 0.6135 - val_accuracy: 0.8009\n",
      "Epoch 14/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.6308 - accuracy: 0.7958 - val_loss: 0.5835 - val_accuracy: 0.8076\n",
      "Epoch 15/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6205 - accuracy: 0.7995 - val_loss: 0.6142 - val_accuracy: 0.7964\n",
      "Epoch 16/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6095 - accuracy: 0.8033 - val_loss: 0.6147 - val_accuracy: 0.8013\n",
      "Epoch 17/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6036 - accuracy: 0.8055 - val_loss: 0.6228 - val_accuracy: 0.7973\n",
      "Epoch 18/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.5942 - accuracy: 0.8092 - val_loss: 0.5590 - val_accuracy: 0.8192\n",
      "Epoch 19/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5865 - accuracy: 0.8101 - val_loss: 0.5623 - val_accuracy: 0.8132\n",
      "Epoch 20/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.5809 - accuracy: 0.8132 - val_loss: 0.5578 - val_accuracy: 0.8189\n",
      "Epoch 21/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5746 - accuracy: 0.8152 - val_loss: 0.5959 - val_accuracy: 0.8001\n",
      "Epoch 22/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5649 - accuracy: 0.8179 - val_loss: 0.5599 - val_accuracy: 0.8131\n",
      "Epoch 23/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.5587 - accuracy: 0.8191 - val_loss: 0.5575 - val_accuracy: 0.8161\n",
      "Epoch 24/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5536 - accuracy: 0.8205 - val_loss: 0.5838 - val_accuracy: 0.8107\n",
      "Epoch 25/200\n",
      "713/713 [==============================] - 24s 33ms/step - loss: 0.5469 - accuracy: 0.8231 - val_loss: 0.5468 - val_accuracy: 0.8189\n",
      "Epoch 26/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5441 - accuracy: 0.8242 - val_loss: 0.5652 - val_accuracy: 0.8115\n",
      "Epoch 27/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5412 - accuracy: 0.8252 - val_loss: 0.5724 - val_accuracy: 0.8121\n",
      "Epoch 28/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.5366 - accuracy: 0.8251 - val_loss: 0.5283 - val_accuracy: 0.8296\n",
      "Epoch 29/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5312 - accuracy: 0.8302 - val_loss: 0.6274 - val_accuracy: 0.7975\n",
      "Epoch 30/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.5266 - accuracy: 0.8296 - val_loss: 0.5095 - val_accuracy: 0.8335\n",
      "Epoch 31/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.5244 - accuracy: 0.8289 - val_loss: 0.5063 - val_accuracy: 0.8336\n",
      "Epoch 32/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5212 - accuracy: 0.8305 - val_loss: 0.5461 - val_accuracy: 0.8244\n",
      "Epoch 33/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5186 - accuracy: 0.8322 - val_loss: 0.5396 - val_accuracy: 0.8223\n",
      "Epoch 34/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5125 - accuracy: 0.8323 - val_loss: 0.5272 - val_accuracy: 0.8280\n",
      "Epoch 35/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5079 - accuracy: 0.8339 - val_loss: 0.5668 - val_accuracy: 0.8172\n",
      "Epoch 36/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.5117 - accuracy: 0.8344 - val_loss: 0.4992 - val_accuracy: 0.8348\n",
      "Epoch 37/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5036 - accuracy: 0.8359 - val_loss: 0.5962 - val_accuracy: 0.8037\n",
      "Epoch 38/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.4999 - accuracy: 0.8373 - val_loss: 0.4874 - val_accuracy: 0.8385\n",
      "Epoch 39/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4989 - accuracy: 0.8381 - val_loss: 0.5090 - val_accuracy: 0.8359\n",
      "Epoch 40/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4971 - accuracy: 0.8385 - val_loss: 0.5402 - val_accuracy: 0.8252\n",
      "Epoch 41/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4959 - accuracy: 0.8386 - val_loss: 0.5514 - val_accuracy: 0.8216\n",
      "Epoch 42/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4923 - accuracy: 0.8402 - val_loss: 0.5394 - val_accuracy: 0.8235\n",
      "Epoch 43/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4905 - accuracy: 0.8397 - val_loss: 0.4913 - val_accuracy: 0.8400\n",
      "Epoch 44/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4898 - accuracy: 0.8399 - val_loss: 0.4931 - val_accuracy: 0.8416\n",
      "Epoch 45/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4874 - accuracy: 0.8405 - val_loss: 0.5089 - val_accuracy: 0.8301\n",
      "Epoch 46/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4828 - accuracy: 0.8417 - val_loss: 0.4934 - val_accuracy: 0.8371\n",
      "Epoch 47/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4832 - accuracy: 0.8424 - val_loss: 0.5205 - val_accuracy: 0.8272\n",
      "Epoch 48/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.4790 - accuracy: 0.8446 - val_loss: 0.4828 - val_accuracy: 0.8420\n",
      "Epoch 49/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4758 - accuracy: 0.8461 - val_loss: 0.5405 - val_accuracy: 0.8247\n",
      "Epoch 50/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4767 - accuracy: 0.8451 - val_loss: 0.5569 - val_accuracy: 0.8175\n",
      "Epoch 51/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4735 - accuracy: 0.8452 - val_loss: 0.5077 - val_accuracy: 0.8332\n",
      "Epoch 52/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4705 - accuracy: 0.8463 - val_loss: 0.4901 - val_accuracy: 0.8396\n",
      "Epoch 53/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4710 - accuracy: 0.8463 - val_loss: 0.4932 - val_accuracy: 0.8375\n",
      "Epoch 54/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4676 - accuracy: 0.8485 - val_loss: 0.4873 - val_accuracy: 0.8368\n",
      "Epoch 55/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4683 - accuracy: 0.8469 - val_loss: 0.5341 - val_accuracy: 0.8184\n",
      "Epoch 56/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4640 - accuracy: 0.8475 - val_loss: 0.5944 - val_accuracy: 0.8119\n",
      "Epoch 57/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4646 - accuracy: 0.8476 - val_loss: 0.5044 - val_accuracy: 0.8352\n",
      "Epoch 58/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4612 - accuracy: 0.8498 - val_loss: 0.4750 - val_accuracy: 0.8432\n",
      "Epoch 59/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4627 - accuracy: 0.8494 - val_loss: 0.5303 - val_accuracy: 0.8251\n",
      "Epoch 60/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4610 - accuracy: 0.8488 - val_loss: 0.4864 - val_accuracy: 0.8428\n",
      "Epoch 61/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4601 - accuracy: 0.8485 - val_loss: 0.4983 - val_accuracy: 0.8388\n",
      "Epoch 62/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4598 - accuracy: 0.8497 - val_loss: 0.5071 - val_accuracy: 0.8333\n",
      "Epoch 63/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4568 - accuracy: 0.8501 - val_loss: 0.4920 - val_accuracy: 0.8392\n",
      "Epoch 64/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4540 - accuracy: 0.8515 - val_loss: 0.5029 - val_accuracy: 0.8348\n",
      "Epoch 65/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4536 - accuracy: 0.8499 - val_loss: 0.4802 - val_accuracy: 0.8401\n",
      "Epoch 66/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4516 - accuracy: 0.8522 - val_loss: 0.4955 - val_accuracy: 0.8384\n",
      "Epoch 67/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.4556 - accuracy: 0.8509 - val_loss: 0.4721 - val_accuracy: 0.8487\n",
      "Epoch 68/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4513 - accuracy: 0.8516 - val_loss: 0.5147 - val_accuracy: 0.8291\n",
      "Epoch 69/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.4499 - accuracy: 0.8508 - val_loss: 0.4678 - val_accuracy: 0.8485\n",
      "Epoch 70/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4467 - accuracy: 0.8538 - val_loss: 0.5168 - val_accuracy: 0.8288\n",
      "Epoch 71/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4489 - accuracy: 0.8514 - val_loss: 0.4816 - val_accuracy: 0.8431\n",
      "Epoch 72/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4467 - accuracy: 0.8526 - val_loss: 0.4775 - val_accuracy: 0.8427\n",
      "Epoch 73/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4437 - accuracy: 0.8535 - val_loss: 0.4884 - val_accuracy: 0.8359\n",
      "Epoch 74/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4474 - accuracy: 0.8527 - val_loss: 0.4759 - val_accuracy: 0.8471\n",
      "Epoch 75/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4444 - accuracy: 0.8522 - val_loss: 0.4821 - val_accuracy: 0.8415\n",
      "Epoch 76/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4418 - accuracy: 0.8549 - val_loss: 0.4835 - val_accuracy: 0.8453\n",
      "Epoch 77/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4418 - accuracy: 0.8539 - val_loss: 0.5070 - val_accuracy: 0.8323\n",
      "Epoch 78/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4434 - accuracy: 0.8541 - val_loss: 0.4848 - val_accuracy: 0.8424\n",
      "Epoch 79/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4398 - accuracy: 0.8554 - val_loss: 0.4774 - val_accuracy: 0.8391\n",
      "Epoch 80/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4391 - accuracy: 0.8560 - val_loss: 0.4744 - val_accuracy: 0.8484\n",
      "Epoch 81/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4385 - accuracy: 0.8566 - val_loss: 0.4760 - val_accuracy: 0.8483\n",
      "Epoch 82/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4401 - accuracy: 0.8552 - val_loss: 0.4892 - val_accuracy: 0.8397\n",
      "Epoch 83/200\n",
      "405/713 [================>.............] - ETA: 9s - loss: 0.4401 - accuracy: 0.8557"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-20a1d8010408>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \"\"\"\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiTmL1G-cC0t"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/c.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkO1d5s_cC0t",
    "outputId": "3ef6fc4c-b0e4-45ca-fd38-ff2c5354dd11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 - 1s - loss: 0.4678 - accuracy: 0.8485 - 1s/epoch - 20ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCp4OyMRkt63"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/keras/c.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tIbo60E1m7r"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz6AI7Mp1m7s"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/d.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "156rc4Pd1m7t",
    "outputId": "796b37ec-897b-417a-bf69-e53041ee0e39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "713/713 [==============================] - 27s 33ms/step - loss: 1.5078 - accuracy: 0.5253 - val_loss: 1.0060 - val_accuracy: 0.6631\n",
      "Epoch 2/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 1.0458 - accuracy: 0.6589 - val_loss: 0.9262 - val_accuracy: 0.6965\n",
      "Epoch 3/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.9245 - accuracy: 0.6985 - val_loss: 0.8187 - val_accuracy: 0.7340\n",
      "Epoch 4/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.8458 - accuracy: 0.7264 - val_loss: 0.8541 - val_accuracy: 0.7272\n",
      "Epoch 5/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.7856 - accuracy: 0.7479 - val_loss: 0.7065 - val_accuracy: 0.7640\n",
      "Epoch 6/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.7512 - accuracy: 0.7593 - val_loss: 0.7037 - val_accuracy: 0.7783\n",
      "Epoch 7/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.7141 - accuracy: 0.7690 - val_loss: 0.6457 - val_accuracy: 0.7873\n",
      "Epoch 8/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6901 - accuracy: 0.7795 - val_loss: 0.6229 - val_accuracy: 0.7875\n",
      "Epoch 9/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6643 - accuracy: 0.7857 - val_loss: 0.6411 - val_accuracy: 0.7912\n",
      "Epoch 10/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6493 - accuracy: 0.7911 - val_loss: 0.7073 - val_accuracy: 0.7787\n",
      "Epoch 11/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.6326 - accuracy: 0.7963 - val_loss: 0.6054 - val_accuracy: 0.7965\n",
      "Epoch 12/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6189 - accuracy: 0.7994 - val_loss: 0.5890 - val_accuracy: 0.8077\n",
      "Epoch 13/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6013 - accuracy: 0.8059 - val_loss: 0.5755 - val_accuracy: 0.8175\n",
      "Epoch 14/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.5922 - accuracy: 0.8084 - val_loss: 0.6159 - val_accuracy: 0.8061\n",
      "Epoch 15/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.5825 - accuracy: 0.8126 - val_loss: 0.6749 - val_accuracy: 0.7889\n",
      "Epoch 16/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5733 - accuracy: 0.8137 - val_loss: 0.5359 - val_accuracy: 0.8315\n",
      "Epoch 17/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5620 - accuracy: 0.8186 - val_loss: 0.5807 - val_accuracy: 0.8184\n",
      "Epoch 18/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5562 - accuracy: 0.8199 - val_loss: 0.5395 - val_accuracy: 0.8265\n",
      "Epoch 19/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.5458 - accuracy: 0.8220 - val_loss: 0.8500 - val_accuracy: 0.7415\n",
      "Epoch 20/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5422 - accuracy: 0.8230 - val_loss: 0.5328 - val_accuracy: 0.8263\n",
      "Epoch 21/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5330 - accuracy: 0.8262 - val_loss: 0.6081 - val_accuracy: 0.8149\n",
      "Epoch 22/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5305 - accuracy: 0.8273 - val_loss: 0.5391 - val_accuracy: 0.8217\n",
      "Epoch 23/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5202 - accuracy: 0.8310 - val_loss: 0.5343 - val_accuracy: 0.8268\n",
      "Epoch 24/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.5134 - accuracy: 0.8315 - val_loss: 0.5781 - val_accuracy: 0.8160\n",
      "Epoch 25/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5120 - accuracy: 0.8329 - val_loss: 0.6029 - val_accuracy: 0.8096\n",
      "Epoch 26/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5075 - accuracy: 0.8343 - val_loss: 0.5245 - val_accuracy: 0.8279\n",
      "Epoch 27/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5019 - accuracy: 0.8358 - val_loss: 0.5068 - val_accuracy: 0.8392\n",
      "Epoch 28/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5019 - accuracy: 0.8368 - val_loss: 0.5043 - val_accuracy: 0.8367\n",
      "Epoch 29/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4921 - accuracy: 0.8380 - val_loss: 0.5076 - val_accuracy: 0.8417\n",
      "Epoch 30/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4896 - accuracy: 0.8404 - val_loss: 0.5175 - val_accuracy: 0.8367\n",
      "Epoch 31/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4859 - accuracy: 0.8416 - val_loss: 0.5231 - val_accuracy: 0.8369\n",
      "Epoch 32/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4821 - accuracy: 0.8424 - val_loss: 0.5095 - val_accuracy: 0.8360\n",
      "Epoch 33/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4784 - accuracy: 0.8431 - val_loss: 0.5266 - val_accuracy: 0.8320\n",
      "Epoch 34/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4749 - accuracy: 0.8444 - val_loss: 0.5344 - val_accuracy: 0.8239\n",
      "Epoch 35/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4710 - accuracy: 0.8454 - val_loss: 0.4930 - val_accuracy: 0.8409\n",
      "Epoch 36/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4691 - accuracy: 0.8456 - val_loss: 0.5233 - val_accuracy: 0.8303\n",
      "Epoch 37/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4658 - accuracy: 0.8471 - val_loss: 0.4895 - val_accuracy: 0.8384\n",
      "Epoch 38/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4635 - accuracy: 0.8478 - val_loss: 0.5106 - val_accuracy: 0.8395\n",
      "Epoch 39/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4591 - accuracy: 0.8489 - val_loss: 0.4924 - val_accuracy: 0.8439\n",
      "Epoch 40/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4576 - accuracy: 0.8501 - val_loss: 0.4856 - val_accuracy: 0.8421\n",
      "Epoch 41/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4531 - accuracy: 0.8493 - val_loss: 0.4897 - val_accuracy: 0.8444\n",
      "Epoch 42/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4540 - accuracy: 0.8511 - val_loss: 0.5177 - val_accuracy: 0.8341\n",
      "Epoch 43/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4500 - accuracy: 0.8520 - val_loss: 0.5265 - val_accuracy: 0.8305\n",
      "Epoch 44/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4469 - accuracy: 0.8525 - val_loss: 0.5334 - val_accuracy: 0.8291\n",
      "Epoch 45/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4467 - accuracy: 0.8521 - val_loss: 0.5022 - val_accuracy: 0.8396\n",
      "Epoch 46/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4426 - accuracy: 0.8538 - val_loss: 0.4907 - val_accuracy: 0.8395\n",
      "Epoch 47/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4404 - accuracy: 0.8538 - val_loss: 0.4827 - val_accuracy: 0.8488\n",
      "Epoch 48/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4360 - accuracy: 0.8549 - val_loss: 0.5077 - val_accuracy: 0.8416\n",
      "Epoch 49/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4359 - accuracy: 0.8560 - val_loss: 0.5129 - val_accuracy: 0.8337\n",
      "Epoch 50/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.4358 - accuracy: 0.8552 - val_loss: 0.4704 - val_accuracy: 0.8477\n",
      "Epoch 51/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4334 - accuracy: 0.8566 - val_loss: 0.5059 - val_accuracy: 0.8356\n",
      "Epoch 52/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4302 - accuracy: 0.8581 - val_loss: 0.4801 - val_accuracy: 0.8428\n",
      "Epoch 53/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4312 - accuracy: 0.8563 - val_loss: 0.4778 - val_accuracy: 0.8431\n",
      "Epoch 54/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4294 - accuracy: 0.8575 - val_loss: 0.4918 - val_accuracy: 0.8439\n",
      "Epoch 55/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4256 - accuracy: 0.8585 - val_loss: 0.4779 - val_accuracy: 0.8431\n",
      "Epoch 56/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4249 - accuracy: 0.8605 - val_loss: 0.5012 - val_accuracy: 0.8393\n",
      "Epoch 57/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4207 - accuracy: 0.8588 - val_loss: 0.4978 - val_accuracy: 0.8437\n",
      "Epoch 58/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4233 - accuracy: 0.8600 - val_loss: 0.4812 - val_accuracy: 0.8449\n",
      "Epoch 59/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4199 - accuracy: 0.8610 - val_loss: 0.5391 - val_accuracy: 0.8339\n",
      "Epoch 60/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4161 - accuracy: 0.8612 - val_loss: 0.4676 - val_accuracy: 0.8492\n",
      "Epoch 61/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4144 - accuracy: 0.8618 - val_loss: 0.4664 - val_accuracy: 0.8472\n",
      "Epoch 62/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4181 - accuracy: 0.8599 - val_loss: 0.4809 - val_accuracy: 0.8457\n",
      "Epoch 63/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4093 - accuracy: 0.8630 - val_loss: 0.4760 - val_accuracy: 0.8447\n",
      "Epoch 64/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4129 - accuracy: 0.8616 - val_loss: 0.4972 - val_accuracy: 0.8393\n",
      "Epoch 65/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4139 - accuracy: 0.8617 - val_loss: 0.4725 - val_accuracy: 0.8460\n",
      "Epoch 66/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4088 - accuracy: 0.8622 - val_loss: 0.5486 - val_accuracy: 0.8323\n",
      "Epoch 67/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4078 - accuracy: 0.8641 - val_loss: 0.4796 - val_accuracy: 0.8484\n",
      "Epoch 68/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4113 - accuracy: 0.8631 - val_loss: 0.4700 - val_accuracy: 0.8477\n",
      "Epoch 69/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4061 - accuracy: 0.8641 - val_loss: 0.4768 - val_accuracy: 0.8473\n",
      "Epoch 70/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4077 - accuracy: 0.8644 - val_loss: 0.4812 - val_accuracy: 0.8447\n",
      "Epoch 71/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4040 - accuracy: 0.8652 - val_loss: 0.4745 - val_accuracy: 0.8480\n",
      "Epoch 72/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4036 - accuracy: 0.8658 - val_loss: 0.4722 - val_accuracy: 0.8480\n",
      "Epoch 73/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4040 - accuracy: 0.8631 - val_loss: 0.5134 - val_accuracy: 0.8368\n",
      "Epoch 74/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4028 - accuracy: 0.8661 - val_loss: 0.4706 - val_accuracy: 0.8496\n",
      "Epoch 75/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3989 - accuracy: 0.8659 - val_loss: 0.4867 - val_accuracy: 0.8408\n",
      "Epoch 76/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.4017 - accuracy: 0.8664 - val_loss: 0.4658 - val_accuracy: 0.8481\n",
      "Epoch 77/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3948 - accuracy: 0.8683 - val_loss: 0.4970 - val_accuracy: 0.8436\n",
      "Epoch 78/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3950 - accuracy: 0.8673 - val_loss: 0.4965 - val_accuracy: 0.8453\n",
      "Epoch 79/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3987 - accuracy: 0.8654 - val_loss: 0.4884 - val_accuracy: 0.8465\n",
      "Epoch 80/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3962 - accuracy: 0.8666 - val_loss: 0.4760 - val_accuracy: 0.8433\n",
      "Epoch 81/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3909 - accuracy: 0.8680 - val_loss: 0.4768 - val_accuracy: 0.8545\n",
      "Epoch 82/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3930 - accuracy: 0.8688 - val_loss: 0.4684 - val_accuracy: 0.8463\n",
      "Epoch 83/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3919 - accuracy: 0.8674 - val_loss: 0.4667 - val_accuracy: 0.8501\n",
      "Epoch 84/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3889 - accuracy: 0.8690 - val_loss: 0.4743 - val_accuracy: 0.8483\n",
      "Epoch 85/200\n",
      "713/713 [==============================] - 23s 33ms/step - loss: 0.3906 - accuracy: 0.8693 - val_loss: 0.4629 - val_accuracy: 0.8504\n",
      "Epoch 86/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3887 - accuracy: 0.8684 - val_loss: 0.4852 - val_accuracy: 0.8477\n",
      "Epoch 87/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3898 - accuracy: 0.8698 - val_loss: 0.4618 - val_accuracy: 0.8519\n",
      "Epoch 88/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3885 - accuracy: 0.8699 - val_loss: 0.4830 - val_accuracy: 0.8443\n",
      "Epoch 89/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3847 - accuracy: 0.8707 - val_loss: 0.4761 - val_accuracy: 0.8496\n",
      "Epoch 90/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3869 - accuracy: 0.8708 - val_loss: 0.4775 - val_accuracy: 0.8484\n",
      "Epoch 91/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3865 - accuracy: 0.8704 - val_loss: 0.4642 - val_accuracy: 0.8499\n",
      "Epoch 92/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3866 - accuracy: 0.8690 - val_loss: 0.4791 - val_accuracy: 0.8493\n",
      "Epoch 93/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3831 - accuracy: 0.8713 - val_loss: 0.4669 - val_accuracy: 0.8540\n",
      "Epoch 94/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3837 - accuracy: 0.8700 - val_loss: 0.4662 - val_accuracy: 0.8505\n",
      "Epoch 95/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3827 - accuracy: 0.8717 - val_loss: 0.4607 - val_accuracy: 0.8501\n",
      "Epoch 96/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3820 - accuracy: 0.8724 - val_loss: 0.4788 - val_accuracy: 0.8489\n",
      "Epoch 97/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3828 - accuracy: 0.8714 - val_loss: 0.4688 - val_accuracy: 0.8507\n",
      "Epoch 98/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3808 - accuracy: 0.8726 - val_loss: 0.4797 - val_accuracy: 0.8496\n",
      "Epoch 99/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3793 - accuracy: 0.8712 - val_loss: 0.4800 - val_accuracy: 0.8487\n",
      "Epoch 100/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3788 - accuracy: 0.8719 - val_loss: 0.4773 - val_accuracy: 0.8452\n",
      "Epoch 101/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3793 - accuracy: 0.8717 - val_loss: 0.4964 - val_accuracy: 0.8463\n",
      "Epoch 102/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3793 - accuracy: 0.8716 - val_loss: 0.4644 - val_accuracy: 0.8529\n",
      "Epoch 103/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3754 - accuracy: 0.8726 - val_loss: 0.4694 - val_accuracy: 0.8533\n",
      "Epoch 104/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3753 - accuracy: 0.8740 - val_loss: 0.4745 - val_accuracy: 0.8545\n",
      "Epoch 105/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3718 - accuracy: 0.8741 - val_loss: 0.4754 - val_accuracy: 0.8516\n",
      "Epoch 106/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3755 - accuracy: 0.8726 - val_loss: 0.4677 - val_accuracy: 0.8555\n",
      "Epoch 107/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3764 - accuracy: 0.8718 - val_loss: 0.4764 - val_accuracy: 0.8540\n",
      "Epoch 108/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3722 - accuracy: 0.8743 - val_loss: 0.4601 - val_accuracy: 0.8535\n",
      "Epoch 109/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3747 - accuracy: 0.8718 - val_loss: 0.4647 - val_accuracy: 0.8552\n",
      "Epoch 110/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3753 - accuracy: 0.8727 - val_loss: 0.4902 - val_accuracy: 0.8472\n",
      "Epoch 111/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3740 - accuracy: 0.8735 - val_loss: 0.4528 - val_accuracy: 0.8541\n",
      "Epoch 112/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3702 - accuracy: 0.8752 - val_loss: 0.4727 - val_accuracy: 0.8525\n",
      "Epoch 113/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3701 - accuracy: 0.8738 - val_loss: 0.4627 - val_accuracy: 0.8521\n",
      "Epoch 114/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3697 - accuracy: 0.8745 - val_loss: 0.4978 - val_accuracy: 0.8491\n",
      "Epoch 115/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3697 - accuracy: 0.8751 - val_loss: 0.4667 - val_accuracy: 0.8563\n",
      "Epoch 116/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3687 - accuracy: 0.8745 - val_loss: 0.4716 - val_accuracy: 0.8559\n",
      "Epoch 117/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3711 - accuracy: 0.8747 - val_loss: 0.4810 - val_accuracy: 0.8500\n",
      "Epoch 118/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3696 - accuracy: 0.8735 - val_loss: 0.4835 - val_accuracy: 0.8487\n",
      "Epoch 119/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3699 - accuracy: 0.8747 - val_loss: 0.5125 - val_accuracy: 0.8413\n",
      "Epoch 120/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3664 - accuracy: 0.8756 - val_loss: 0.4745 - val_accuracy: 0.8567\n",
      "Epoch 121/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3643 - accuracy: 0.8764 - val_loss: 0.4495 - val_accuracy: 0.8537\n",
      "Epoch 122/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3680 - accuracy: 0.8740 - val_loss: 0.4850 - val_accuracy: 0.8516\n",
      "Epoch 123/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3637 - accuracy: 0.8759 - val_loss: 0.4717 - val_accuracy: 0.8549\n",
      "Epoch 124/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3654 - accuracy: 0.8763 - val_loss: 0.5200 - val_accuracy: 0.8405\n",
      "Epoch 125/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3669 - accuracy: 0.8754 - val_loss: 0.4634 - val_accuracy: 0.8531\n",
      "Epoch 126/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3672 - accuracy: 0.8748 - val_loss: 0.5037 - val_accuracy: 0.8452\n",
      "Epoch 127/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3612 - accuracy: 0.8767 - val_loss: 0.4712 - val_accuracy: 0.8524\n",
      "Epoch 128/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3608 - accuracy: 0.8778 - val_loss: 0.5069 - val_accuracy: 0.8424\n",
      "Epoch 129/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3645 - accuracy: 0.8758 - val_loss: 0.4676 - val_accuracy: 0.8540\n",
      "Epoch 130/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3597 - accuracy: 0.8775 - val_loss: 0.4845 - val_accuracy: 0.8555\n",
      "Epoch 131/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3618 - accuracy: 0.8770 - val_loss: 0.4879 - val_accuracy: 0.8527\n",
      "Epoch 132/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3638 - accuracy: 0.8761 - val_loss: 0.4623 - val_accuracy: 0.8549\n",
      "Epoch 133/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3581 - accuracy: 0.8778 - val_loss: 0.4772 - val_accuracy: 0.8560\n",
      "Epoch 134/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3606 - accuracy: 0.8777 - val_loss: 0.4693 - val_accuracy: 0.8537\n",
      "Epoch 135/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3598 - accuracy: 0.8773 - val_loss: 0.4900 - val_accuracy: 0.8497\n",
      "Epoch 136/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3589 - accuracy: 0.8769 - val_loss: 0.5017 - val_accuracy: 0.8488\n",
      "Epoch 137/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3624 - accuracy: 0.8763 - val_loss: 0.4790 - val_accuracy: 0.8491\n",
      "Epoch 138/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3599 - accuracy: 0.8775 - val_loss: 0.4716 - val_accuracy: 0.8519\n",
      "Epoch 139/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3599 - accuracy: 0.8770 - val_loss: 0.4765 - val_accuracy: 0.8523\n",
      "Epoch 140/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3609 - accuracy: 0.8768 - val_loss: 0.4590 - val_accuracy: 0.8547\n",
      "Epoch 141/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3578 - accuracy: 0.8785 - val_loss: 0.4814 - val_accuracy: 0.8515\n",
      "Epoch 142/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3580 - accuracy: 0.8775 - val_loss: 0.4665 - val_accuracy: 0.8572\n",
      "Epoch 143/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3582 - accuracy: 0.8774 - val_loss: 0.4663 - val_accuracy: 0.8528\n",
      "Epoch 144/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3532 - accuracy: 0.8795 - val_loss: 0.4732 - val_accuracy: 0.8491\n",
      "Epoch 145/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3551 - accuracy: 0.8787 - val_loss: 0.4800 - val_accuracy: 0.8520\n",
      "Epoch 146/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3554 - accuracy: 0.8791 - val_loss: 0.4981 - val_accuracy: 0.8484\n",
      "Epoch 147/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3550 - accuracy: 0.8784 - val_loss: 0.4706 - val_accuracy: 0.8556\n",
      "Epoch 148/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3551 - accuracy: 0.8781 - val_loss: 0.4783 - val_accuracy: 0.8513\n",
      "Epoch 149/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3540 - accuracy: 0.8795 - val_loss: 0.4813 - val_accuracy: 0.8547\n",
      "Epoch 150/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3495 - accuracy: 0.8809 - val_loss: 0.5014 - val_accuracy: 0.8515\n",
      "Epoch 151/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3513 - accuracy: 0.8794 - val_loss: 0.4704 - val_accuracy: 0.8548\n",
      "Epoch 152/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3519 - accuracy: 0.8796 - val_loss: 0.4835 - val_accuracy: 0.8491\n",
      "Epoch 153/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3526 - accuracy: 0.8794 - val_loss: 0.4806 - val_accuracy: 0.8527\n",
      "Epoch 154/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3528 - accuracy: 0.8789 - val_loss: 0.4823 - val_accuracy: 0.8519\n",
      "Epoch 155/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3527 - accuracy: 0.8796 - val_loss: 0.4837 - val_accuracy: 0.8471\n",
      "Epoch 156/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3533 - accuracy: 0.8801 - val_loss: 0.4787 - val_accuracy: 0.8500\n",
      "Epoch 157/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3494 - accuracy: 0.8812 - val_loss: 0.5036 - val_accuracy: 0.8473\n",
      "Epoch 158/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3495 - accuracy: 0.8810 - val_loss: 0.4836 - val_accuracy: 0.8531\n",
      "Epoch 159/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3541 - accuracy: 0.8789 - val_loss: 0.4804 - val_accuracy: 0.8513\n",
      "Epoch 160/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3508 - accuracy: 0.8797 - val_loss: 0.4809 - val_accuracy: 0.8533\n",
      "Epoch 161/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3475 - accuracy: 0.8809 - val_loss: 0.4873 - val_accuracy: 0.8544\n",
      "Epoch 162/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3526 - accuracy: 0.8792 - val_loss: 0.4765 - val_accuracy: 0.8517\n",
      "Epoch 163/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3495 - accuracy: 0.8802 - val_loss: 0.4799 - val_accuracy: 0.8559\n",
      "Epoch 164/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3472 - accuracy: 0.8802 - val_loss: 0.4763 - val_accuracy: 0.8553\n",
      "Epoch 165/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3479 - accuracy: 0.8813 - val_loss: 0.4863 - val_accuracy: 0.8512\n",
      "Epoch 166/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3481 - accuracy: 0.8809 - val_loss: 0.4725 - val_accuracy: 0.8512\n",
      "Epoch 167/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3497 - accuracy: 0.8791 - val_loss: 0.4620 - val_accuracy: 0.8540\n",
      "Epoch 168/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3480 - accuracy: 0.8800 - val_loss: 0.4955 - val_accuracy: 0.8515\n",
      "Epoch 169/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3463 - accuracy: 0.8815 - val_loss: 0.4835 - val_accuracy: 0.8553\n",
      "Epoch 170/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3496 - accuracy: 0.8812 - val_loss: 0.4768 - val_accuracy: 0.8549\n",
      "Epoch 171/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3476 - accuracy: 0.8809 - val_loss: 0.4638 - val_accuracy: 0.8523\n",
      "Epoch 172/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3478 - accuracy: 0.8810 - val_loss: 0.4781 - val_accuracy: 0.8537\n",
      "Epoch 173/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3478 - accuracy: 0.8809 - val_loss: 0.4868 - val_accuracy: 0.8440\n",
      "Epoch 174/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3467 - accuracy: 0.8806 - val_loss: 0.4813 - val_accuracy: 0.8535\n",
      "Epoch 175/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3466 - accuracy: 0.8804 - val_loss: 0.4747 - val_accuracy: 0.8549\n",
      "Epoch 176/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3458 - accuracy: 0.8804 - val_loss: 0.4931 - val_accuracy: 0.8519\n",
      "Epoch 177/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3447 - accuracy: 0.8812 - val_loss: 0.4797 - val_accuracy: 0.8567\n",
      "Epoch 178/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3434 - accuracy: 0.8806 - val_loss: 0.4869 - val_accuracy: 0.8557\n",
      "Epoch 179/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3448 - accuracy: 0.8823 - val_loss: 0.4982 - val_accuracy: 0.8499\n",
      "Epoch 180/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3446 - accuracy: 0.8810 - val_loss: 0.4850 - val_accuracy: 0.8505\n",
      "Epoch 181/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3428 - accuracy: 0.8824 - val_loss: 0.4707 - val_accuracy: 0.8567\n",
      "Epoch 182/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3436 - accuracy: 0.8820 - val_loss: 0.4719 - val_accuracy: 0.8487\n",
      "Epoch 183/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3425 - accuracy: 0.8826 - val_loss: 0.4830 - val_accuracy: 0.8531\n",
      "Epoch 184/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3418 - accuracy: 0.8820 - val_loss: 0.4649 - val_accuracy: 0.8549\n",
      "Epoch 185/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3445 - accuracy: 0.8824 - val_loss: 0.4798 - val_accuracy: 0.8545\n",
      "Epoch 186/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3378 - accuracy: 0.8844 - val_loss: 0.4726 - val_accuracy: 0.8507\n",
      "Epoch 187/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3415 - accuracy: 0.8828 - val_loss: 0.4657 - val_accuracy: 0.8544\n",
      "Epoch 188/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3416 - accuracy: 0.8828 - val_loss: 0.5069 - val_accuracy: 0.8512\n",
      "Epoch 189/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3409 - accuracy: 0.8830 - val_loss: 0.4755 - val_accuracy: 0.8532\n",
      "Epoch 190/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3406 - accuracy: 0.8831 - val_loss: 0.4820 - val_accuracy: 0.8537\n",
      "Epoch 191/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3393 - accuracy: 0.8824 - val_loss: 0.4920 - val_accuracy: 0.8472\n",
      "Epoch 192/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3388 - accuracy: 0.8835 - val_loss: 0.4865 - val_accuracy: 0.8475\n",
      "Epoch 193/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3418 - accuracy: 0.8821 - val_loss: 0.4687 - val_accuracy: 0.8516\n",
      "Epoch 194/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3402 - accuracy: 0.8822 - val_loss: 0.5044 - val_accuracy: 0.8500\n",
      "Epoch 195/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3393 - accuracy: 0.8831 - val_loss: 0.4875 - val_accuracy: 0.8539\n",
      "Epoch 196/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3376 - accuracy: 0.8826 - val_loss: 0.4851 - val_accuracy: 0.8553\n",
      "Epoch 197/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3398 - accuracy: 0.8817 - val_loss: 0.5227 - val_accuracy: 0.8432\n",
      "Epoch 198/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3400 - accuracy: 0.8827 - val_loss: 0.4833 - val_accuracy: 0.8521\n",
      "Epoch 199/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3382 - accuracy: 0.8826 - val_loss: 0.5074 - val_accuracy: 0.8529\n",
      "Epoch 200/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3382 - accuracy: 0.8833 - val_loss: 0.4889 - val_accuracy: 0.8469\n"
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMDibKTX2Jmh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddo2uF572Jx2"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/d.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgpkh_Sw2Jx3",
    "outputId": "ad5c2f83-14a1-44fa-f2e2-f4261836f905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 - 1s - loss: 0.4495 - accuracy: 0.8537 - 1s/epoch - 21ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxj3saf_2Jx3"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/d.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qoIIDy1H4IW"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJPX1Lc7H4Ia"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/e.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HVFqkDGkH4Ib",
    "outputId": "e5533254-3cfe-4735-f0d9-204c9f393d36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "713/713 [==============================] - 27s 32ms/step - loss: 1.4180 - accuracy: 0.5556 - val_loss: 1.0599 - val_accuracy: 0.6408\n",
      "Epoch 2/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.9703 - accuracy: 0.6857 - val_loss: 0.8768 - val_accuracy: 0.7091\n",
      "Epoch 3/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.8392 - accuracy: 0.7288 - val_loss: 0.8464 - val_accuracy: 0.7157\n",
      "Epoch 4/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.7617 - accuracy: 0.7551 - val_loss: 0.7240 - val_accuracy: 0.7600\n",
      "Epoch 5/200\n",
      "713/713 [==============================] - 24s 34ms/step - loss: 0.7143 - accuracy: 0.7709 - val_loss: 0.6285 - val_accuracy: 0.7919\n",
      "Epoch 6/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.6652 - accuracy: 0.7860 - val_loss: 0.6342 - val_accuracy: 0.7921\n",
      "Epoch 7/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.6317 - accuracy: 0.7965 - val_loss: 0.6845 - val_accuracy: 0.7683\n",
      "Epoch 8/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.6107 - accuracy: 0.8038 - val_loss: 0.5895 - val_accuracy: 0.7995\n",
      "Epoch 9/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5871 - accuracy: 0.8114 - val_loss: 0.5638 - val_accuracy: 0.8096\n",
      "Epoch 10/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.5641 - accuracy: 0.8168 - val_loss: 0.6901 - val_accuracy: 0.7717\n",
      "Epoch 11/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.5478 - accuracy: 0.8230 - val_loss: 0.5282 - val_accuracy: 0.8255\n",
      "Epoch 12/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.5338 - accuracy: 0.8271 - val_loss: 0.6058 - val_accuracy: 0.7920\n",
      "Epoch 13/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.5165 - accuracy: 0.8319 - val_loss: 0.5600 - val_accuracy: 0.8129\n",
      "Epoch 14/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.5064 - accuracy: 0.8362 - val_loss: 0.6356 - val_accuracy: 0.7816\n",
      "Epoch 15/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.4942 - accuracy: 0.8392 - val_loss: 0.6520 - val_accuracy: 0.7829\n",
      "Epoch 16/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4793 - accuracy: 0.8427 - val_loss: 0.5888 - val_accuracy: 0.8008\n",
      "Epoch 17/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4707 - accuracy: 0.8451 - val_loss: 0.5126 - val_accuracy: 0.8320\n",
      "Epoch 18/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4607 - accuracy: 0.8484 - val_loss: 0.6543 - val_accuracy: 0.7807\n",
      "Epoch 19/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4515 - accuracy: 0.8518 - val_loss: 0.5240 - val_accuracy: 0.8273\n",
      "Epoch 20/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.4449 - accuracy: 0.8530 - val_loss: 0.6129 - val_accuracy: 0.7924\n",
      "Epoch 21/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.4382 - accuracy: 0.8571 - val_loss: 0.5538 - val_accuracy: 0.8135\n",
      "Epoch 22/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4261 - accuracy: 0.8581 - val_loss: 0.5345 - val_accuracy: 0.8221\n",
      "Epoch 23/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4231 - accuracy: 0.8593 - val_loss: 0.5415 - val_accuracy: 0.8216\n",
      "Epoch 24/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4136 - accuracy: 0.8618 - val_loss: 0.6326 - val_accuracy: 0.7847\n",
      "Epoch 25/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.4095 - accuracy: 0.8620 - val_loss: 0.6180 - val_accuracy: 0.8024\n",
      "Epoch 26/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3999 - accuracy: 0.8670 - val_loss: 0.5629 - val_accuracy: 0.8219\n",
      "Epoch 27/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.3959 - accuracy: 0.8669 - val_loss: 0.5870 - val_accuracy: 0.8056\n",
      "Epoch 28/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3911 - accuracy: 0.8684 - val_loss: 0.5346 - val_accuracy: 0.8267\n",
      "Epoch 29/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3859 - accuracy: 0.8692 - val_loss: 0.6194 - val_accuracy: 0.7936\n",
      "Epoch 30/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3799 - accuracy: 0.8727 - val_loss: 0.5350 - val_accuracy: 0.8349\n",
      "Epoch 31/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3737 - accuracy: 0.8728 - val_loss: 0.5780 - val_accuracy: 0.8167\n",
      "Epoch 32/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.3697 - accuracy: 0.8741 - val_loss: 0.6391 - val_accuracy: 0.8039\n",
      "Epoch 33/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3686 - accuracy: 0.8736 - val_loss: 0.5515 - val_accuracy: 0.8265\n",
      "Epoch 34/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3658 - accuracy: 0.8764 - val_loss: 0.6025 - val_accuracy: 0.8100\n",
      "Epoch 35/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3604 - accuracy: 0.8774 - val_loss: 0.5407 - val_accuracy: 0.8297\n",
      "Epoch 36/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3544 - accuracy: 0.8791 - val_loss: 0.6702 - val_accuracy: 0.7724\n",
      "Epoch 37/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3512 - accuracy: 0.8800 - val_loss: 0.6243 - val_accuracy: 0.8037\n",
      "Epoch 38/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3483 - accuracy: 0.8805 - val_loss: 0.6662 - val_accuracy: 0.7879\n",
      "Epoch 39/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.3451 - accuracy: 0.8819 - val_loss: 0.5516 - val_accuracy: 0.8273\n",
      "Epoch 40/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3406 - accuracy: 0.8840 - val_loss: 0.5997 - val_accuracy: 0.8208\n",
      "Epoch 41/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.3371 - accuracy: 0.8843 - val_loss: 0.5510 - val_accuracy: 0.8239\n",
      "Epoch 42/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.3364 - accuracy: 0.8845 - val_loss: 0.5381 - val_accuracy: 0.8340\n",
      "Epoch 43/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3319 - accuracy: 0.8861 - val_loss: 0.5432 - val_accuracy: 0.8371\n",
      "Epoch 44/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3307 - accuracy: 0.8860 - val_loss: 0.5551 - val_accuracy: 0.8241\n",
      "Epoch 45/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3213 - accuracy: 0.8885 - val_loss: 0.5397 - val_accuracy: 0.8399\n",
      "Epoch 46/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3200 - accuracy: 0.8881 - val_loss: 0.5248 - val_accuracy: 0.8341\n",
      "Epoch 47/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3189 - accuracy: 0.8886 - val_loss: 0.5342 - val_accuracy: 0.8359\n",
      "Epoch 48/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.3175 - accuracy: 0.8904 - val_loss: 0.5550 - val_accuracy: 0.8273\n",
      "Epoch 49/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3116 - accuracy: 0.8910 - val_loss: 0.5357 - val_accuracy: 0.8260\n",
      "Epoch 50/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3156 - accuracy: 0.8902 - val_loss: 0.6029 - val_accuracy: 0.8301\n",
      "Epoch 51/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.3118 - accuracy: 0.8916 - val_loss: 0.5684 - val_accuracy: 0.8327\n",
      "Epoch 52/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3136 - accuracy: 0.8911 - val_loss: 0.5947 - val_accuracy: 0.8088\n",
      "Epoch 53/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.3021 - accuracy: 0.8944 - val_loss: 0.5482 - val_accuracy: 0.8365\n",
      "Epoch 54/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.3061 - accuracy: 0.8931 - val_loss: 0.5457 - val_accuracy: 0.8391\n",
      "Epoch 55/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2989 - accuracy: 0.8959 - val_loss: 0.5730 - val_accuracy: 0.8124\n",
      "Epoch 56/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2999 - accuracy: 0.8943 - val_loss: 0.5807 - val_accuracy: 0.8188\n",
      "Epoch 57/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2993 - accuracy: 0.8952 - val_loss: 0.5622 - val_accuracy: 0.8317\n",
      "Epoch 58/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2912 - accuracy: 0.8989 - val_loss: 0.5317 - val_accuracy: 0.8379\n",
      "Epoch 59/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.2935 - accuracy: 0.8967 - val_loss: 0.5456 - val_accuracy: 0.8415\n",
      "Epoch 60/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2913 - accuracy: 0.8986 - val_loss: 0.5425 - val_accuracy: 0.8372\n",
      "Epoch 61/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2883 - accuracy: 0.8986 - val_loss: 0.5374 - val_accuracy: 0.8393\n",
      "Epoch 62/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2921 - accuracy: 0.8975 - val_loss: 0.5532 - val_accuracy: 0.8395\n",
      "Epoch 63/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2862 - accuracy: 0.8994 - val_loss: 0.5710 - val_accuracy: 0.8299\n",
      "Epoch 64/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2823 - accuracy: 0.9003 - val_loss: 0.5617 - val_accuracy: 0.8327\n",
      "Epoch 65/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2841 - accuracy: 0.9004 - val_loss: 0.6887 - val_accuracy: 0.7901\n",
      "Epoch 66/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2819 - accuracy: 0.8992 - val_loss: 0.6188 - val_accuracy: 0.8191\n",
      "Epoch 67/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2770 - accuracy: 0.9026 - val_loss: 0.5586 - val_accuracy: 0.8393\n",
      "Epoch 68/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2751 - accuracy: 0.9026 - val_loss: 0.6229 - val_accuracy: 0.8275\n",
      "Epoch 69/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2773 - accuracy: 0.9014 - val_loss: 0.5702 - val_accuracy: 0.8307\n",
      "Epoch 70/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2761 - accuracy: 0.9028 - val_loss: 0.6153 - val_accuracy: 0.8236\n",
      "Epoch 71/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2736 - accuracy: 0.9029 - val_loss: 0.5575 - val_accuracy: 0.8429\n",
      "Epoch 72/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2719 - accuracy: 0.9046 - val_loss: 0.5692 - val_accuracy: 0.8309\n",
      "Epoch 73/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2715 - accuracy: 0.9039 - val_loss: 0.5645 - val_accuracy: 0.8373\n",
      "Epoch 74/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2721 - accuracy: 0.9033 - val_loss: 0.5476 - val_accuracy: 0.8359\n",
      "Epoch 75/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2669 - accuracy: 0.9055 - val_loss: 0.5871 - val_accuracy: 0.8375\n",
      "Epoch 76/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2662 - accuracy: 0.9058 - val_loss: 0.5559 - val_accuracy: 0.8403\n",
      "Epoch 77/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2681 - accuracy: 0.9058 - val_loss: 0.6256 - val_accuracy: 0.8272\n",
      "Epoch 78/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2622 - accuracy: 0.9054 - val_loss: 0.5942 - val_accuracy: 0.8348\n",
      "Epoch 79/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2630 - accuracy: 0.9071 - val_loss: 0.5834 - val_accuracy: 0.8389\n",
      "Epoch 80/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2606 - accuracy: 0.9069 - val_loss: 0.5809 - val_accuracy: 0.8380\n",
      "Epoch 81/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2599 - accuracy: 0.9065 - val_loss: 0.5815 - val_accuracy: 0.8368\n",
      "Epoch 82/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2623 - accuracy: 0.9069 - val_loss: 0.6019 - val_accuracy: 0.8361\n",
      "Epoch 83/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2593 - accuracy: 0.9075 - val_loss: 0.5565 - val_accuracy: 0.8380\n",
      "Epoch 84/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2563 - accuracy: 0.9077 - val_loss: 0.5738 - val_accuracy: 0.8417\n",
      "Epoch 85/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2566 - accuracy: 0.9095 - val_loss: 0.5907 - val_accuracy: 0.8361\n",
      "Epoch 86/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2530 - accuracy: 0.9092 - val_loss: 0.5976 - val_accuracy: 0.8432\n",
      "Epoch 87/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2546 - accuracy: 0.9093 - val_loss: 0.5809 - val_accuracy: 0.8315\n",
      "Epoch 88/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2483 - accuracy: 0.9104 - val_loss: 0.6000 - val_accuracy: 0.8259\n",
      "Epoch 89/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2511 - accuracy: 0.9095 - val_loss: 0.6148 - val_accuracy: 0.8256\n",
      "Epoch 90/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2509 - accuracy: 0.9099 - val_loss: 0.6369 - val_accuracy: 0.8380\n",
      "Epoch 91/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2520 - accuracy: 0.9104 - val_loss: 0.6283 - val_accuracy: 0.8375\n",
      "Epoch 92/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2488 - accuracy: 0.9105 - val_loss: 0.6118 - val_accuracy: 0.8329\n",
      "Epoch 93/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2464 - accuracy: 0.9131 - val_loss: 0.6164 - val_accuracy: 0.8281\n",
      "Epoch 94/200\n",
      "713/713 [==============================] - 22s 31ms/step - loss: 0.2477 - accuracy: 0.9097 - val_loss: 0.6378 - val_accuracy: 0.8387\n",
      "Epoch 95/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2437 - accuracy: 0.9122 - val_loss: 0.6126 - val_accuracy: 0.8317\n",
      "Epoch 96/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2447 - accuracy: 0.9119 - val_loss: 0.6133 - val_accuracy: 0.8368\n",
      "Epoch 97/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2449 - accuracy: 0.9117 - val_loss: 0.6515 - val_accuracy: 0.8017\n",
      "Epoch 98/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2427 - accuracy: 0.9119 - val_loss: 0.6083 - val_accuracy: 0.8380\n",
      "Epoch 99/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2435 - accuracy: 0.9124 - val_loss: 0.6588 - val_accuracy: 0.8311\n",
      "Epoch 100/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2427 - accuracy: 0.9116 - val_loss: 0.6195 - val_accuracy: 0.8369\n",
      "Epoch 101/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2406 - accuracy: 0.9124 - val_loss: 0.5928 - val_accuracy: 0.8387\n",
      "Epoch 102/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2366 - accuracy: 0.9149 - val_loss: 0.6438 - val_accuracy: 0.8195\n",
      "Epoch 103/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2413 - accuracy: 0.9128 - val_loss: 0.6308 - val_accuracy: 0.8316\n",
      "Epoch 104/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2390 - accuracy: 0.9141 - val_loss: 0.6518 - val_accuracy: 0.8323\n",
      "Epoch 105/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2362 - accuracy: 0.9142 - val_loss: 0.5957 - val_accuracy: 0.8368\n",
      "Epoch 106/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2378 - accuracy: 0.9134 - val_loss: 0.6763 - val_accuracy: 0.8055\n",
      "Epoch 107/200\n",
      "713/713 [==============================] - 23s 31ms/step - loss: 0.2348 - accuracy: 0.9145 - val_loss: 0.6319 - val_accuracy: 0.8296\n",
      "Epoch 108/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.2357 - accuracy: 0.9155 - val_loss: 0.6319 - val_accuracy: 0.8353\n",
      "Epoch 109/200\n",
      "713/713 [==============================] - 23s 32ms/step - loss: 0.2335 - accuracy: 0.9147 - val_loss: 0.6309 - val_accuracy: 0.8412\n",
      "Epoch 110/200\n",
      "597/713 [========================>.....] - ETA: 3s - loss: 0.2313 - accuracy: 0.9162"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7fc44b518050>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history1 = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history1 = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bduROdx8H4Ic"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "-9wFfrs0H4Ic",
    "outputId": "14b519dd-d856-4a5a-c2cf-76133099ffb1"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-76f86e14716d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the best weights for this model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/keras/final_w_paddingc.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/content/drive/MyDrive/keras/final_w_paddingc.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/e.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLQ2AtMqH4Id",
    "outputId": "7f1caa63-ea87-46cf-95aa-7008280bfa1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 - 1s - loss: 0.6120 - accuracy: 0.8291 - 1s/epoch - 20ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZZewPH0H4Id"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/e.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dameVmAtRd8i"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYqf_GqCRd8l"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/f.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-uJcRRroRd8l",
    "outputId": "3703c181-d4cc-4a95-9fec-b07fc07986a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "357/357 [==============================] - 41s 76ms/step - loss: 1.5529 - accuracy: 0.5178 - val_loss: 1.7670 - val_accuracy: 0.4092\n",
      "Epoch 2/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 1.0601 - accuracy: 0.6541 - val_loss: 0.8501 - val_accuracy: 0.7064\n",
      "Epoch 3/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.9061 - accuracy: 0.7039 - val_loss: 1.1810 - val_accuracy: 0.6255\n",
      "Epoch 4/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.8153 - accuracy: 0.7353 - val_loss: 0.7647 - val_accuracy: 0.7516\n",
      "Epoch 5/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.7454 - accuracy: 0.7589 - val_loss: 0.6587 - val_accuracy: 0.7811\n",
      "Epoch 6/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.6982 - accuracy: 0.7728 - val_loss: 0.6869 - val_accuracy: 0.7741\n",
      "Epoch 7/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.6610 - accuracy: 0.7856 - val_loss: 0.7294 - val_accuracy: 0.7555\n",
      "Epoch 8/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.6297 - accuracy: 0.7961 - val_loss: 0.6072 - val_accuracy: 0.7933\n",
      "Epoch 9/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5931 - accuracy: 0.8056 - val_loss: 0.5596 - val_accuracy: 0.8153\n",
      "Epoch 10/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.5684 - accuracy: 0.8124 - val_loss: 0.5583 - val_accuracy: 0.8093\n",
      "Epoch 11/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.5461 - accuracy: 0.8200 - val_loss: 0.5626 - val_accuracy: 0.8132\n",
      "Epoch 12/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.5293 - accuracy: 0.8252 - val_loss: 0.5968 - val_accuracy: 0.8089\n",
      "Epoch 13/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.5143 - accuracy: 0.8286 - val_loss: 0.5814 - val_accuracy: 0.8052\n",
      "Epoch 14/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4945 - accuracy: 0.8350 - val_loss: 0.5393 - val_accuracy: 0.8189\n",
      "Epoch 15/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4803 - accuracy: 0.8386 - val_loss: 0.5726 - val_accuracy: 0.8159\n",
      "Epoch 16/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4702 - accuracy: 0.8418 - val_loss: 0.5480 - val_accuracy: 0.8208\n",
      "Epoch 17/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4569 - accuracy: 0.8467 - val_loss: 0.5593 - val_accuracy: 0.8180\n",
      "Epoch 18/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4417 - accuracy: 0.8512 - val_loss: 0.5265 - val_accuracy: 0.8271\n",
      "Epoch 19/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4339 - accuracy: 0.8542 - val_loss: 0.5320 - val_accuracy: 0.8312\n",
      "Epoch 20/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4197 - accuracy: 0.8576 - val_loss: 0.5188 - val_accuracy: 0.8328\n",
      "Epoch 21/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4107 - accuracy: 0.8598 - val_loss: 0.5437 - val_accuracy: 0.8213\n",
      "Epoch 22/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4041 - accuracy: 0.8625 - val_loss: 0.5574 - val_accuracy: 0.8185\n",
      "Epoch 23/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3974 - accuracy: 0.8645 - val_loss: 0.5519 - val_accuracy: 0.8199\n",
      "Epoch 24/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3900 - accuracy: 0.8658 - val_loss: 0.5633 - val_accuracy: 0.8216\n",
      "Epoch 25/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3788 - accuracy: 0.8708 - val_loss: 0.5306 - val_accuracy: 0.8280\n",
      "Epoch 26/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3694 - accuracy: 0.8734 - val_loss: 0.5401 - val_accuracy: 0.8301\n",
      "Epoch 27/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.3647 - accuracy: 0.8738 - val_loss: 0.5644 - val_accuracy: 0.8219\n",
      "Epoch 28/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3601 - accuracy: 0.8747 - val_loss: 0.5657 - val_accuracy: 0.8316\n",
      "Epoch 29/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.3501 - accuracy: 0.8782 - val_loss: 0.5342 - val_accuracy: 0.8303\n",
      "Epoch 30/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3436 - accuracy: 0.8802 - val_loss: 0.5731 - val_accuracy: 0.8248\n",
      "Epoch 31/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.3394 - accuracy: 0.8807 - val_loss: 0.5930 - val_accuracy: 0.8180\n",
      "Epoch 32/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3357 - accuracy: 0.8834 - val_loss: 0.5407 - val_accuracy: 0.8257\n",
      "Epoch 33/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.3343 - accuracy: 0.8832 - val_loss: 0.6144 - val_accuracy: 0.8083\n",
      "Epoch 34/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3277 - accuracy: 0.8853 - val_loss: 0.5825 - val_accuracy: 0.8151\n",
      "Epoch 35/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.3237 - accuracy: 0.8869 - val_loss: 0.5700 - val_accuracy: 0.8325\n",
      "Epoch 36/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.3163 - accuracy: 0.8887 - val_loss: 0.5668 - val_accuracy: 0.8264\n",
      "Epoch 37/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3180 - accuracy: 0.8881 - val_loss: 0.5448 - val_accuracy: 0.8185\n",
      "Epoch 38/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3083 - accuracy: 0.8914 - val_loss: 0.5585 - val_accuracy: 0.8300\n",
      "Epoch 39/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.3054 - accuracy: 0.8915 - val_loss: 0.6080 - val_accuracy: 0.8095\n",
      "Epoch 40/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.2959 - accuracy: 0.8955 - val_loss: 0.5701 - val_accuracy: 0.8317\n",
      "Epoch 41/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3010 - accuracy: 0.8939 - val_loss: 0.5708 - val_accuracy: 0.8319\n",
      "Epoch 42/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.2938 - accuracy: 0.8954 - val_loss: 0.5833 - val_accuracy: 0.8167\n",
      "Epoch 43/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.2920 - accuracy: 0.8976 - val_loss: 0.6117 - val_accuracy: 0.8180\n",
      "Epoch 44/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.2872 - accuracy: 0.8974 - val_loss: 0.5641 - val_accuracy: 0.8337\n",
      "Epoch 45/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.2846 - accuracy: 0.8983 - val_loss: 0.5462 - val_accuracy: 0.8267\n",
      "Epoch 46/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.2812 - accuracy: 0.9002 - val_loss: 0.5639 - val_accuracy: 0.8331\n",
      "Epoch 47/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.2826 - accuracy: 0.8997 - val_loss: 0.5776 - val_accuracy: 0.8276\n",
      "Epoch 48/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.2757 - accuracy: 0.9009 - val_loss: 0.5786 - val_accuracy: 0.8309\n",
      "Epoch 49/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.2758 - accuracy: 0.9010 - val_loss: 0.5937 - val_accuracy: 0.8315\n",
      "Epoch 50/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.2725 - accuracy: 0.9019 - val_loss: 0.5822 - val_accuracy: 0.8319\n",
      "Epoch 51/200\n",
      "357/357 [==============================] - 28s 76ms/step - loss: 0.2717 - accuracy: 0.9025 - val_loss: 0.5711 - val_accuracy: 0.8341\n",
      "Epoch 52/200\n",
      "320/357 [=========================>....] - ETA: 2s - loss: 0.2660 - accuracy: 0.9046"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7fc44b518050>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history1 = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \"\"\"\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history2 = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "3cJ1inhBtRfq",
    "outputId": "9fcc0159-516c-4081-a610-ac74b234b3a9"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f010f69179f3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the best weights for this model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/keras/final_w_paddingb.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/legacy/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_layer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0;34m\"Layer count mismatch when loading weights from file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;34mf\"Model expected {len(filtered_layers)} layers, found \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 8 layers, found 10 saved layers."
     ]
    }
   ],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/e.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyEWs6L0tRft",
    "outputId": "70eaf3d0-6aa1-4602-a5df-7e515b36594c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 1s - loss: 0.6176 - accuracy: 0.8315 - 1s/epoch - 41ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpjUmItctRft"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/e.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcyQfbjxyQ7y"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01LmhTmhyQ70"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orp6wzGdyQ71"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/f.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "An7s1XfZyQ71",
    "outputId": "5e14b492-c9b9-41f4-f574-fe13702a3e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "357/357 [==============================] - 33s 84ms/step - loss: 1.7407 - accuracy: 0.4577 - val_loss: 2.6214 - val_accuracy: 0.3011\n",
      "Epoch 2/200\n",
      "357/357 [==============================] - 29s 81ms/step - loss: 1.2061 - accuracy: 0.6028 - val_loss: 1.0390 - val_accuracy: 0.6419\n",
      "Epoch 3/200\n",
      "357/357 [==============================] - 29s 81ms/step - loss: 1.0482 - accuracy: 0.6529 - val_loss: 1.0074 - val_accuracy: 0.6779\n",
      "Epoch 4/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.9469 - accuracy: 0.6888 - val_loss: 0.7688 - val_accuracy: 0.7459\n",
      "Epoch 5/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.8751 - accuracy: 0.7120 - val_loss: 0.7671 - val_accuracy: 0.7431\n",
      "Epoch 6/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.8187 - accuracy: 0.7331 - val_loss: 0.7085 - val_accuracy: 0.7653\n",
      "Epoch 7/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.7709 - accuracy: 0.7502 - val_loss: 0.7371 - val_accuracy: 0.7620\n",
      "Epoch 8/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.7316 - accuracy: 0.7624 - val_loss: 0.7277 - val_accuracy: 0.7748\n",
      "Epoch 9/200\n",
      "357/357 [==============================] - 29s 81ms/step - loss: 0.6991 - accuracy: 0.7712 - val_loss: 0.6648 - val_accuracy: 0.7859\n",
      "Epoch 10/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.6674 - accuracy: 0.7817 - val_loss: 0.7260 - val_accuracy: 0.7649\n",
      "Epoch 11/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.6377 - accuracy: 0.7924 - val_loss: 0.5982 - val_accuracy: 0.8087\n",
      "Epoch 12/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.6194 - accuracy: 0.7991 - val_loss: 0.6027 - val_accuracy: 0.8015\n",
      "Epoch 13/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.5991 - accuracy: 0.8031 - val_loss: 0.6931 - val_accuracy: 0.7739\n",
      "Epoch 14/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.5794 - accuracy: 0.8098 - val_loss: 0.6623 - val_accuracy: 0.7852\n",
      "Epoch 15/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.5645 - accuracy: 0.8145 - val_loss: 0.6155 - val_accuracy: 0.7957\n",
      "Epoch 16/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.5534 - accuracy: 0.8185 - val_loss: 0.6300 - val_accuracy: 0.8144\n",
      "Epoch 17/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5387 - accuracy: 0.8238 - val_loss: 0.5454 - val_accuracy: 0.8227\n",
      "Epoch 18/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.5253 - accuracy: 0.8270 - val_loss: 0.6195 - val_accuracy: 0.7972\n",
      "Epoch 19/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.5160 - accuracy: 0.8300 - val_loss: 0.5845 - val_accuracy: 0.8183\n",
      "Epoch 20/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.5059 - accuracy: 0.8323 - val_loss: 0.5746 - val_accuracy: 0.8147\n",
      "Epoch 21/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4950 - accuracy: 0.8357 - val_loss: 0.6608 - val_accuracy: 0.7949\n",
      "Epoch 22/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4907 - accuracy: 0.8368 - val_loss: 0.5481 - val_accuracy: 0.8227\n",
      "Epoch 23/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4805 - accuracy: 0.8407 - val_loss: 0.6190 - val_accuracy: 0.8044\n",
      "Epoch 24/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4684 - accuracy: 0.8435 - val_loss: 0.5579 - val_accuracy: 0.8239\n",
      "Epoch 25/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4674 - accuracy: 0.8438 - val_loss: 0.5462 - val_accuracy: 0.8235\n",
      "Epoch 26/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4562 - accuracy: 0.8480 - val_loss: 0.5152 - val_accuracy: 0.8361\n",
      "Epoch 27/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4516 - accuracy: 0.8489 - val_loss: 0.5699 - val_accuracy: 0.8141\n",
      "Epoch 28/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4480 - accuracy: 0.8487 - val_loss: 0.5694 - val_accuracy: 0.8145\n",
      "Epoch 29/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4397 - accuracy: 0.8516 - val_loss: 0.5344 - val_accuracy: 0.8331\n",
      "Epoch 30/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4346 - accuracy: 0.8529 - val_loss: 0.5265 - val_accuracy: 0.8337\n",
      "Epoch 31/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4299 - accuracy: 0.8554 - val_loss: 0.5707 - val_accuracy: 0.8168\n",
      "Epoch 32/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4253 - accuracy: 0.8564 - val_loss: 0.5061 - val_accuracy: 0.8417\n",
      "Epoch 33/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4197 - accuracy: 0.8589 - val_loss: 0.6153 - val_accuracy: 0.8113\n",
      "Epoch 34/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4147 - accuracy: 0.8600 - val_loss: 0.5603 - val_accuracy: 0.8253\n",
      "Epoch 35/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4078 - accuracy: 0.8618 - val_loss: 0.5707 - val_accuracy: 0.8260\n",
      "Epoch 36/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4068 - accuracy: 0.8614 - val_loss: 0.5684 - val_accuracy: 0.8119\n",
      "Epoch 37/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.4028 - accuracy: 0.8638 - val_loss: 0.5936 - val_accuracy: 0.8077\n",
      "Epoch 38/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.4005 - accuracy: 0.8642 - val_loss: 0.5087 - val_accuracy: 0.8455\n",
      "Epoch 39/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3972 - accuracy: 0.8657 - val_loss: 0.5222 - val_accuracy: 0.8427\n",
      "Epoch 40/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3925 - accuracy: 0.8677 - val_loss: 0.5521 - val_accuracy: 0.8309\n",
      "Epoch 41/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3893 - accuracy: 0.8676 - val_loss: 0.5500 - val_accuracy: 0.8252\n",
      "Epoch 42/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3874 - accuracy: 0.8687 - val_loss: 0.5518 - val_accuracy: 0.8300\n",
      "Epoch 43/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3820 - accuracy: 0.8687 - val_loss: 0.5100 - val_accuracy: 0.8412\n",
      "Epoch 44/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3793 - accuracy: 0.8698 - val_loss: 0.5291 - val_accuracy: 0.8299\n",
      "Epoch 45/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3806 - accuracy: 0.8696 - val_loss: 0.5512 - val_accuracy: 0.8351\n",
      "Epoch 46/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3723 - accuracy: 0.8727 - val_loss: 0.5359 - val_accuracy: 0.8360\n",
      "Epoch 47/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3680 - accuracy: 0.8740 - val_loss: 0.5844 - val_accuracy: 0.8068\n",
      "Epoch 48/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3651 - accuracy: 0.8735 - val_loss: 0.6004 - val_accuracy: 0.8217\n",
      "Epoch 49/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3652 - accuracy: 0.8747 - val_loss: 0.5373 - val_accuracy: 0.8345\n",
      "Epoch 50/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3596 - accuracy: 0.8762 - val_loss: 0.5329 - val_accuracy: 0.8392\n",
      "Epoch 51/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3611 - accuracy: 0.8763 - val_loss: 0.5182 - val_accuracy: 0.8372\n",
      "Epoch 52/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3567 - accuracy: 0.8775 - val_loss: 0.5512 - val_accuracy: 0.8291\n",
      "Epoch 53/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3572 - accuracy: 0.8768 - val_loss: 0.5366 - val_accuracy: 0.8313\n",
      "Epoch 54/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3501 - accuracy: 0.8781 - val_loss: 0.5371 - val_accuracy: 0.8312\n",
      "Epoch 55/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3468 - accuracy: 0.8804 - val_loss: 0.6102 - val_accuracy: 0.8241\n",
      "Epoch 56/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3476 - accuracy: 0.8805 - val_loss: 0.5528 - val_accuracy: 0.8307\n",
      "Epoch 57/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3486 - accuracy: 0.8793 - val_loss: 0.5533 - val_accuracy: 0.8253\n",
      "Epoch 58/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3439 - accuracy: 0.8810 - val_loss: 0.5339 - val_accuracy: 0.8399\n",
      "Epoch 59/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3402 - accuracy: 0.8804 - val_loss: 0.5387 - val_accuracy: 0.8331\n",
      "Epoch 60/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3426 - accuracy: 0.8808 - val_loss: 0.5517 - val_accuracy: 0.8209\n",
      "Epoch 61/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3364 - accuracy: 0.8826 - val_loss: 0.5114 - val_accuracy: 0.8391\n",
      "Epoch 62/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3362 - accuracy: 0.8824 - val_loss: 0.5160 - val_accuracy: 0.8327\n",
      "Epoch 63/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3319 - accuracy: 0.8845 - val_loss: 0.5353 - val_accuracy: 0.8397\n",
      "Epoch 64/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3335 - accuracy: 0.8839 - val_loss: 0.5471 - val_accuracy: 0.8351\n",
      "Epoch 65/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3335 - accuracy: 0.8839 - val_loss: 0.5213 - val_accuracy: 0.8428\n",
      "Epoch 66/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3298 - accuracy: 0.8842 - val_loss: 0.5221 - val_accuracy: 0.8356\n",
      "Epoch 67/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3288 - accuracy: 0.8854 - val_loss: 0.5250 - val_accuracy: 0.8417\n",
      "Epoch 68/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3272 - accuracy: 0.8862 - val_loss: 0.5326 - val_accuracy: 0.8373\n",
      "Epoch 69/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3257 - accuracy: 0.8864 - val_loss: 0.5283 - val_accuracy: 0.8343\n",
      "Epoch 70/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3194 - accuracy: 0.8883 - val_loss: 0.5356 - val_accuracy: 0.8441\n",
      "Epoch 71/200\n",
      "357/357 [==============================] - 28s 77ms/step - loss: 0.3243 - accuracy: 0.8876 - val_loss: 0.5205 - val_accuracy: 0.8308\n",
      "Epoch 72/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3191 - accuracy: 0.8888 - val_loss: 0.5392 - val_accuracy: 0.8375\n",
      "Epoch 73/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3206 - accuracy: 0.8873 - val_loss: 0.5561 - val_accuracy: 0.8300\n",
      "Epoch 74/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3170 - accuracy: 0.8888 - val_loss: 0.5297 - val_accuracy: 0.8432\n",
      "Epoch 75/200\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.3175 - accuracy: 0.8886 - val_loss: 0.5385 - val_accuracy: 0.8377\n",
      "Epoch 76/200\n",
      " 67/357 [====>.........................] - ETA: 21s - loss: 0.3178 - accuracy: 0.8881"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b733a679450a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history2 = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \"\"\"\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history2 = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CxSNDkhyQ73"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/f.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b67GssuwyQ73",
    "outputId": "a13d2b63-306c-42cb-a023-7b24f00311b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 1s - loss: 0.5061 - accuracy: 0.8417 - 1s/epoch - 42ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRKBFZzMyQ74"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/f.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4FhStLz64CU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhK5loDU6-RF"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88pI5eMl6-RI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRE67yvx6-RI"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/g.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vU1e2xtF6-RJ",
    "outputId": "78de539e-1532-4842-c201-d46b4678cefc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "357/357 [==============================] - 33s 84ms/step - loss: 1.7702 - accuracy: 0.4405 - val_loss: 2.4519 - val_accuracy: 0.2541\n",
      "Epoch 2/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 1.2548 - accuracy: 0.5841 - val_loss: 1.0950 - val_accuracy: 0.6231\n",
      "Epoch 3/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 1.1017 - accuracy: 0.6341 - val_loss: 0.9217 - val_accuracy: 0.6767\n",
      "Epoch 4/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.9941 - accuracy: 0.6735 - val_loss: 0.8162 - val_accuracy: 0.7272\n",
      "Epoch 5/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.9104 - accuracy: 0.7015 - val_loss: 0.8159 - val_accuracy: 0.7311\n",
      "Epoch 6/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.8549 - accuracy: 0.7184 - val_loss: 0.7550 - val_accuracy: 0.7595\n",
      "Epoch 7/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.8109 - accuracy: 0.7378 - val_loss: 0.6707 - val_accuracy: 0.7776\n",
      "Epoch 8/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.7665 - accuracy: 0.7500 - val_loss: 0.7207 - val_accuracy: 0.7655\n",
      "Epoch 9/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.7358 - accuracy: 0.7591 - val_loss: 0.6743 - val_accuracy: 0.7731\n",
      "Epoch 10/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.7089 - accuracy: 0.7705 - val_loss: 0.6442 - val_accuracy: 0.7776\n",
      "Epoch 11/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.6809 - accuracy: 0.7788 - val_loss: 0.6287 - val_accuracy: 0.7933\n",
      "Epoch 12/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.6600 - accuracy: 0.7853 - val_loss: 0.6166 - val_accuracy: 0.7913\n",
      "Epoch 13/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.6381 - accuracy: 0.7910 - val_loss: 0.6009 - val_accuracy: 0.8015\n",
      "Epoch 14/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.6198 - accuracy: 0.7984 - val_loss: 0.6199 - val_accuracy: 0.7919\n",
      "Epoch 15/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.6059 - accuracy: 0.8020 - val_loss: 0.5939 - val_accuracy: 0.8039\n",
      "Epoch 16/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.5956 - accuracy: 0.8053 - val_loss: 0.5799 - val_accuracy: 0.8091\n",
      "Epoch 17/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.5772 - accuracy: 0.8109 - val_loss: 0.5329 - val_accuracy: 0.8219\n",
      "Epoch 18/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5647 - accuracy: 0.8134 - val_loss: 0.5724 - val_accuracy: 0.8113\n",
      "Epoch 19/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.5549 - accuracy: 0.8167 - val_loss: 0.6741 - val_accuracy: 0.7715\n",
      "Epoch 20/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.5456 - accuracy: 0.8208 - val_loss: 0.5680 - val_accuracy: 0.8148\n",
      "Epoch 21/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5335 - accuracy: 0.8243 - val_loss: 0.6429 - val_accuracy: 0.7869\n",
      "Epoch 22/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5242 - accuracy: 0.8264 - val_loss: 0.5350 - val_accuracy: 0.8241\n",
      "Epoch 23/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5150 - accuracy: 0.8289 - val_loss: 0.5915 - val_accuracy: 0.8015\n",
      "Epoch 24/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.5090 - accuracy: 0.8307 - val_loss: 0.5205 - val_accuracy: 0.8288\n",
      "Epoch 25/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4987 - accuracy: 0.8344 - val_loss: 0.5469 - val_accuracy: 0.8220\n",
      "Epoch 26/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4948 - accuracy: 0.8356 - val_loss: 0.5557 - val_accuracy: 0.8231\n",
      "Epoch 27/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4879 - accuracy: 0.8370 - val_loss: 0.5218 - val_accuracy: 0.8300\n",
      "Epoch 28/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4824 - accuracy: 0.8394 - val_loss: 0.5632 - val_accuracy: 0.8155\n",
      "Epoch 29/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4731 - accuracy: 0.8423 - val_loss: 0.5397 - val_accuracy: 0.8245\n",
      "Epoch 30/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4728 - accuracy: 0.8420 - val_loss: 0.5356 - val_accuracy: 0.8344\n",
      "Epoch 31/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4690 - accuracy: 0.8438 - val_loss: 0.5351 - val_accuracy: 0.8284\n",
      "Epoch 32/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4604 - accuracy: 0.8465 - val_loss: 0.5265 - val_accuracy: 0.8349\n",
      "Epoch 33/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4546 - accuracy: 0.8491 - val_loss: 0.5609 - val_accuracy: 0.8157\n",
      "Epoch 34/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4536 - accuracy: 0.8491 - val_loss: 0.5916 - val_accuracy: 0.7999\n",
      "Epoch 35/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.4419 - accuracy: 0.8529 - val_loss: 0.5166 - val_accuracy: 0.8265\n",
      "Epoch 36/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.4417 - accuracy: 0.8526 - val_loss: 0.5144 - val_accuracy: 0.8328\n",
      "Epoch 37/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4391 - accuracy: 0.8521 - val_loss: 0.5165 - val_accuracy: 0.8349\n",
      "Epoch 38/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4269 - accuracy: 0.8550 - val_loss: 0.5233 - val_accuracy: 0.8369\n",
      "Epoch 39/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4258 - accuracy: 0.8565 - val_loss: 0.5296 - val_accuracy: 0.8323\n",
      "Epoch 40/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4233 - accuracy: 0.8577 - val_loss: 0.5207 - val_accuracy: 0.8331\n",
      "Epoch 41/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4239 - accuracy: 0.8579 - val_loss: 0.5247 - val_accuracy: 0.8285\n",
      "Epoch 42/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4187 - accuracy: 0.8571 - val_loss: 0.5350 - val_accuracy: 0.8249\n",
      "Epoch 43/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4136 - accuracy: 0.8596 - val_loss: 0.5196 - val_accuracy: 0.8311\n",
      "Epoch 44/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4172 - accuracy: 0.8584 - val_loss: 0.5685 - val_accuracy: 0.8232\n",
      "Epoch 45/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4134 - accuracy: 0.8595 - val_loss: 0.5215 - val_accuracy: 0.8357\n",
      "Epoch 46/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.4087 - accuracy: 0.8619 - val_loss: 0.5108 - val_accuracy: 0.8357\n",
      "Epoch 47/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3982 - accuracy: 0.8646 - val_loss: 0.6008 - val_accuracy: 0.8191\n",
      "Epoch 48/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4028 - accuracy: 0.8630 - val_loss: 0.5640 - val_accuracy: 0.8172\n",
      "Epoch 49/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3960 - accuracy: 0.8660 - val_loss: 0.5325 - val_accuracy: 0.8345\n",
      "Epoch 50/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3894 - accuracy: 0.8678 - val_loss: 0.5322 - val_accuracy: 0.8279\n",
      "Epoch 51/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3958 - accuracy: 0.8642 - val_loss: 0.5723 - val_accuracy: 0.8129\n",
      "Epoch 52/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3894 - accuracy: 0.8680 - val_loss: 0.5314 - val_accuracy: 0.8311\n",
      "Epoch 53/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3870 - accuracy: 0.8681 - val_loss: 0.5549 - val_accuracy: 0.8235\n",
      "Epoch 54/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3799 - accuracy: 0.8693 - val_loss: 0.5548 - val_accuracy: 0.8403\n",
      "Epoch 55/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3791 - accuracy: 0.8699 - val_loss: 0.5982 - val_accuracy: 0.8144\n",
      "Epoch 56/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3814 - accuracy: 0.8699 - val_loss: 0.5525 - val_accuracy: 0.8320\n",
      "Epoch 57/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3779 - accuracy: 0.8703 - val_loss: 0.5412 - val_accuracy: 0.8384\n",
      "Epoch 58/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3737 - accuracy: 0.8723 - val_loss: 0.5470 - val_accuracy: 0.8271\n",
      "Epoch 59/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3705 - accuracy: 0.8730 - val_loss: 0.5353 - val_accuracy: 0.8403\n",
      "Epoch 60/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3715 - accuracy: 0.8733 - val_loss: 0.5293 - val_accuracy: 0.8405\n",
      "Epoch 61/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3710 - accuracy: 0.8727 - val_loss: 0.5154 - val_accuracy: 0.8397\n",
      "Epoch 62/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3690 - accuracy: 0.8739 - val_loss: 0.6890 - val_accuracy: 0.7832\n",
      "Epoch 63/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3653 - accuracy: 0.8739 - val_loss: 0.5360 - val_accuracy: 0.8353\n",
      "Epoch 64/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3649 - accuracy: 0.8752 - val_loss: 0.5204 - val_accuracy: 0.8439\n",
      "Epoch 65/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3661 - accuracy: 0.8741 - val_loss: 0.5214 - val_accuracy: 0.8387\n",
      "Epoch 66/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3596 - accuracy: 0.8753 - val_loss: 0.5473 - val_accuracy: 0.8311\n",
      "Epoch 67/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3570 - accuracy: 0.8775 - val_loss: 0.5455 - val_accuracy: 0.8408\n",
      "Epoch 68/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3560 - accuracy: 0.8771 - val_loss: 0.5875 - val_accuracy: 0.8125\n",
      "Epoch 69/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3551 - accuracy: 0.8771 - val_loss: 0.5593 - val_accuracy: 0.8347\n",
      "Epoch 70/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3531 - accuracy: 0.8787 - val_loss: 0.5489 - val_accuracy: 0.8392\n",
      "Epoch 71/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3532 - accuracy: 0.8783 - val_loss: 0.5135 - val_accuracy: 0.8424\n",
      "Epoch 72/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3495 - accuracy: 0.8790 - val_loss: 0.5467 - val_accuracy: 0.8379\n",
      "Epoch 73/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3480 - accuracy: 0.8803 - val_loss: 0.5536 - val_accuracy: 0.8303\n",
      "Epoch 74/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3469 - accuracy: 0.8795 - val_loss: 0.5540 - val_accuracy: 0.8359\n",
      "Epoch 75/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3501 - accuracy: 0.8783 - val_loss: 0.5325 - val_accuracy: 0.8376\n",
      "Epoch 76/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3414 - accuracy: 0.8812 - val_loss: 0.5505 - val_accuracy: 0.8307\n",
      "Epoch 77/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3431 - accuracy: 0.8801 - val_loss: 0.5641 - val_accuracy: 0.8359\n",
      "Epoch 78/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3404 - accuracy: 0.8825 - val_loss: 0.5447 - val_accuracy: 0.8335\n",
      "Epoch 79/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3400 - accuracy: 0.8815 - val_loss: 0.5587 - val_accuracy: 0.8436\n",
      "Epoch 80/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3361 - accuracy: 0.8829 - val_loss: 0.5740 - val_accuracy: 0.8280\n",
      "Epoch 81/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3421 - accuracy: 0.8811 - val_loss: 0.5630 - val_accuracy: 0.8385\n",
      "Epoch 82/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3346 - accuracy: 0.8837 - val_loss: 0.5566 - val_accuracy: 0.8343\n",
      "Epoch 83/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.3359 - accuracy: 0.8837 - val_loss: 0.5095 - val_accuracy: 0.8408\n",
      "Epoch 84/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3371 - accuracy: 0.8835 - val_loss: 0.5453 - val_accuracy: 0.8385\n",
      "Epoch 85/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3321 - accuracy: 0.8844 - val_loss: 0.5758 - val_accuracy: 0.8392\n",
      "Epoch 86/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3340 - accuracy: 0.8839 - val_loss: 0.5527 - val_accuracy: 0.8445\n",
      "Epoch 87/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3286 - accuracy: 0.8861 - val_loss: 0.5758 - val_accuracy: 0.8345\n",
      "Epoch 88/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3287 - accuracy: 0.8851 - val_loss: 0.5477 - val_accuracy: 0.8433\n",
      "Epoch 89/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3276 - accuracy: 0.8857 - val_loss: 0.5466 - val_accuracy: 0.8392\n",
      "Epoch 90/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3260 - accuracy: 0.8873 - val_loss: 0.5799 - val_accuracy: 0.8297\n",
      "Epoch 91/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3274 - accuracy: 0.8863 - val_loss: 0.5736 - val_accuracy: 0.8437\n",
      "Epoch 92/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3246 - accuracy: 0.8874 - val_loss: 0.5299 - val_accuracy: 0.8449\n",
      "Epoch 93/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3233 - accuracy: 0.8878 - val_loss: 0.5540 - val_accuracy: 0.8356\n",
      "Epoch 94/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3224 - accuracy: 0.8871 - val_loss: 0.5407 - val_accuracy: 0.8469\n",
      "Epoch 95/200\n",
      "258/357 [====================>.........] - ETA: 7s - loss: 0.3223 - accuracy: 0.8880"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b733a679450a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history2 = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history2 = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypxuHJqZ6-RJ"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/g.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5KbgYXBK6-RK",
    "outputId": "0239943e-c3ae-40e8-8d4b-76767db56899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 1s - loss: 0.5095 - accuracy: 0.8408 - 1s/epoch - 41ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KETbsEZQ6-RK"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/g.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaJWrPmFFoIl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkDfrfAnFoVB"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CD8LnStQFoVD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CsATFDPFoVE"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/h.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wFTdwgVlFoVE",
    "outputId": "5e131d28-8505-4189-c018-2d85cd09401c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "357/357 [==============================] - 32s 81ms/step - loss: 1.7740 - accuracy: 0.4404 - val_loss: 2.5138 - val_accuracy: 0.2095\n",
      "Epoch 2/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 1.2492 - accuracy: 0.5895 - val_loss: 1.0677 - val_accuracy: 0.6220\n",
      "Epoch 3/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 1.0727 - accuracy: 0.6461 - val_loss: 0.9059 - val_accuracy: 0.7061\n",
      "Epoch 4/200\n",
      "357/357 [==============================] - 30s 84ms/step - loss: 0.9569 - accuracy: 0.6870 - val_loss: 0.8296 - val_accuracy: 0.7203\n",
      "Epoch 5/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.8696 - accuracy: 0.7167 - val_loss: 0.7304 - val_accuracy: 0.7491\n",
      "Epoch 6/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.8063 - accuracy: 0.7380 - val_loss: 0.8169 - val_accuracy: 0.7301\n",
      "Epoch 7/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.7519 - accuracy: 0.7566 - val_loss: 0.6573 - val_accuracy: 0.7864\n",
      "Epoch 8/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.7080 - accuracy: 0.7698 - val_loss: 0.6321 - val_accuracy: 0.7852\n",
      "Epoch 9/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.6738 - accuracy: 0.7823 - val_loss: 0.6788 - val_accuracy: 0.7757\n",
      "Epoch 10/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.6426 - accuracy: 0.7916 - val_loss: 0.6243 - val_accuracy: 0.7893\n",
      "Epoch 11/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.6151 - accuracy: 0.7987 - val_loss: 0.5899 - val_accuracy: 0.7973\n",
      "Epoch 12/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5952 - accuracy: 0.8066 - val_loss: 0.7271 - val_accuracy: 0.7549\n",
      "Epoch 13/200\n",
      "357/357 [==============================] - 32s 88ms/step - loss: 0.5729 - accuracy: 0.8117 - val_loss: 0.5756 - val_accuracy: 0.8035\n",
      "Epoch 14/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5570 - accuracy: 0.8158 - val_loss: 0.7930 - val_accuracy: 0.7367\n",
      "Epoch 15/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5437 - accuracy: 0.8222 - val_loss: 0.6137 - val_accuracy: 0.7945\n",
      "Epoch 16/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.5251 - accuracy: 0.8266 - val_loss: 0.5556 - val_accuracy: 0.8143\n",
      "Epoch 17/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.5134 - accuracy: 0.8302 - val_loss: 0.6162 - val_accuracy: 0.7991\n",
      "Epoch 18/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5001 - accuracy: 0.8353 - val_loss: 0.5614 - val_accuracy: 0.8152\n",
      "Epoch 19/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4888 - accuracy: 0.8369 - val_loss: 0.5692 - val_accuracy: 0.8091\n",
      "Epoch 20/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4764 - accuracy: 0.8418 - val_loss: 0.5905 - val_accuracy: 0.8065\n",
      "Epoch 21/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.4670 - accuracy: 0.8448 - val_loss: 0.5440 - val_accuracy: 0.8213\n",
      "Epoch 22/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.4584 - accuracy: 0.8446 - val_loss: 0.5354 - val_accuracy: 0.8264\n",
      "Epoch 23/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.4516 - accuracy: 0.8481 - val_loss: 0.5570 - val_accuracy: 0.8169\n",
      "Epoch 24/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4428 - accuracy: 0.8502 - val_loss: 0.5491 - val_accuracy: 0.8160\n",
      "Epoch 25/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4358 - accuracy: 0.8530 - val_loss: 0.7855 - val_accuracy: 0.7451\n",
      "Epoch 26/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.4294 - accuracy: 0.8556 - val_loss: 0.5294 - val_accuracy: 0.8265\n",
      "Epoch 27/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4222 - accuracy: 0.8567 - val_loss: 0.5602 - val_accuracy: 0.8180\n",
      "Epoch 28/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.4166 - accuracy: 0.8592 - val_loss: 0.5294 - val_accuracy: 0.8260\n",
      "Epoch 29/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4123 - accuracy: 0.8593 - val_loss: 0.5751 - val_accuracy: 0.8148\n",
      "Epoch 30/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3981 - accuracy: 0.8634 - val_loss: 0.5481 - val_accuracy: 0.8163\n",
      "Epoch 31/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3989 - accuracy: 0.8657 - val_loss: 0.5678 - val_accuracy: 0.8256\n",
      "Epoch 32/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3904 - accuracy: 0.8679 - val_loss: 0.5723 - val_accuracy: 0.8279\n",
      "Epoch 33/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3909 - accuracy: 0.8667 - val_loss: 0.5434 - val_accuracy: 0.8257\n",
      "Epoch 34/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3824 - accuracy: 0.8689 - val_loss: 0.5573 - val_accuracy: 0.8329\n",
      "Epoch 35/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3801 - accuracy: 0.8699 - val_loss: 0.5615 - val_accuracy: 0.8141\n",
      "Epoch 36/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3737 - accuracy: 0.8724 - val_loss: 0.5549 - val_accuracy: 0.8303\n",
      "Epoch 37/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3713 - accuracy: 0.8717 - val_loss: 0.5637 - val_accuracy: 0.8285\n",
      "Epoch 38/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.3644 - accuracy: 0.8753 - val_loss: 0.5284 - val_accuracy: 0.8360\n",
      "Epoch 39/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3643 - accuracy: 0.8745 - val_loss: 0.5524 - val_accuracy: 0.8360\n",
      "Epoch 40/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3581 - accuracy: 0.8763 - val_loss: 0.5626 - val_accuracy: 0.8289\n",
      "Epoch 41/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3610 - accuracy: 0.8765 - val_loss: 0.5486 - val_accuracy: 0.8292\n",
      "Epoch 42/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3538 - accuracy: 0.8778 - val_loss: 0.5412 - val_accuracy: 0.8275\n",
      "Epoch 43/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3484 - accuracy: 0.8804 - val_loss: 0.5467 - val_accuracy: 0.8321\n",
      "Epoch 44/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3448 - accuracy: 0.8807 - val_loss: 0.6090 - val_accuracy: 0.8211\n",
      "Epoch 45/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3460 - accuracy: 0.8810 - val_loss: 0.5627 - val_accuracy: 0.8313\n",
      "Epoch 46/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3394 - accuracy: 0.8822 - val_loss: 0.5606 - val_accuracy: 0.8253\n",
      "Epoch 47/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3404 - accuracy: 0.8823 - val_loss: 0.5763 - val_accuracy: 0.8125\n",
      "Epoch 48/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3338 - accuracy: 0.8835 - val_loss: 0.5572 - val_accuracy: 0.8364\n",
      "Epoch 49/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3295 - accuracy: 0.8840 - val_loss: 0.6789 - val_accuracy: 0.7928\n",
      "Epoch 50/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3329 - accuracy: 0.8838 - val_loss: 0.5391 - val_accuracy: 0.8383\n",
      "Epoch 51/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.3258 - accuracy: 0.8869 - val_loss: 0.5268 - val_accuracy: 0.8376\n",
      "Epoch 52/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3217 - accuracy: 0.8878 - val_loss: 0.5483 - val_accuracy: 0.8401\n",
      "Epoch 53/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3228 - accuracy: 0.8874 - val_loss: 0.5650 - val_accuracy: 0.8331\n",
      "Epoch 54/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3216 - accuracy: 0.8875 - val_loss: 0.5853 - val_accuracy: 0.8264\n",
      "Epoch 55/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3166 - accuracy: 0.8892 - val_loss: 0.5772 - val_accuracy: 0.8291\n",
      "Epoch 56/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3142 - accuracy: 0.8900 - val_loss: 0.5552 - val_accuracy: 0.8369\n",
      "Epoch 57/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3128 - accuracy: 0.8899 - val_loss: 0.5667 - val_accuracy: 0.8428\n",
      "Epoch 58/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3072 - accuracy: 0.8916 - val_loss: 0.5653 - val_accuracy: 0.8373\n",
      "Epoch 59/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3068 - accuracy: 0.8932 - val_loss: 0.5543 - val_accuracy: 0.8381\n",
      "Epoch 60/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3077 - accuracy: 0.8914 - val_loss: 0.5577 - val_accuracy: 0.8348\n",
      "Epoch 61/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3036 - accuracy: 0.8924 - val_loss: 0.5872 - val_accuracy: 0.8344\n",
      "Epoch 62/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3027 - accuracy: 0.8936 - val_loss: 0.5873 - val_accuracy: 0.8380\n",
      "Epoch 63/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3032 - accuracy: 0.8932 - val_loss: 0.6447 - val_accuracy: 0.8216\n",
      "Epoch 64/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2988 - accuracy: 0.8955 - val_loss: 0.6970 - val_accuracy: 0.8195\n",
      "Epoch 65/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2960 - accuracy: 0.8955 - val_loss: 0.5852 - val_accuracy: 0.8331\n",
      "Epoch 66/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2977 - accuracy: 0.8958 - val_loss: 0.5666 - val_accuracy: 0.8384\n",
      "Epoch 67/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2919 - accuracy: 0.8973 - val_loss: 0.5786 - val_accuracy: 0.8405\n",
      "Epoch 68/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2902 - accuracy: 0.8976 - val_loss: 0.5729 - val_accuracy: 0.8332\n",
      "Epoch 69/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2927 - accuracy: 0.8960 - val_loss: 0.5695 - val_accuracy: 0.8328\n",
      "Epoch 70/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2936 - accuracy: 0.8977 - val_loss: 0.5820 - val_accuracy: 0.8377\n",
      "Epoch 71/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2837 - accuracy: 0.8989 - val_loss: 0.5788 - val_accuracy: 0.8400\n",
      "Epoch 72/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2856 - accuracy: 0.8983 - val_loss: 0.5737 - val_accuracy: 0.8423\n",
      "Epoch 73/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2848 - accuracy: 0.8988 - val_loss: 0.5752 - val_accuracy: 0.8359\n",
      "Epoch 74/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2842 - accuracy: 0.8987 - val_loss: 0.5835 - val_accuracy: 0.8276\n",
      "Epoch 75/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2855 - accuracy: 0.8992 - val_loss: 0.6169 - val_accuracy: 0.8217\n",
      "Epoch 76/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2774 - accuracy: 0.9013 - val_loss: 0.6170 - val_accuracy: 0.8328\n",
      "Epoch 77/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2766 - accuracy: 0.9014 - val_loss: 0.5658 - val_accuracy: 0.8415\n",
      "Epoch 78/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2776 - accuracy: 0.9007 - val_loss: 0.5699 - val_accuracy: 0.8377\n",
      "Epoch 79/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2797 - accuracy: 0.9003 - val_loss: 0.7149 - val_accuracy: 0.7928\n",
      "Epoch 80/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2747 - accuracy: 0.9018 - val_loss: 0.5711 - val_accuracy: 0.8359\n",
      "Epoch 81/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2742 - accuracy: 0.9018 - val_loss: 0.6061 - val_accuracy: 0.8363\n",
      "Epoch 82/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2726 - accuracy: 0.9028 - val_loss: 0.7299 - val_accuracy: 0.8148\n",
      "Epoch 83/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2742 - accuracy: 0.9028 - val_loss: 0.5938 - val_accuracy: 0.8381\n",
      "Epoch 84/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2717 - accuracy: 0.9026 - val_loss: 0.5866 - val_accuracy: 0.8252\n",
      "Epoch 85/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2701 - accuracy: 0.9034 - val_loss: 0.5780 - val_accuracy: 0.8373\n",
      "Epoch 86/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2659 - accuracy: 0.9041 - val_loss: 0.7677 - val_accuracy: 0.8104\n",
      "Epoch 87/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2691 - accuracy: 0.9040 - val_loss: 0.5877 - val_accuracy: 0.8401\n",
      "Epoch 88/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2605 - accuracy: 0.9060 - val_loss: 0.5825 - val_accuracy: 0.8373\n",
      "Epoch 89/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2673 - accuracy: 0.9045 - val_loss: 0.5894 - val_accuracy: 0.8391\n",
      "Epoch 90/200\n",
      " 78/357 [=====>........................] - ETA: 21s - loss: 0.2736 - accuracy: 0.9040"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b733a679450a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history2 = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history2 = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv8b1VrlFoVF"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/h.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdHi96HtFoVF",
    "outputId": "e07e50d9-5796-4f41-d18e-3fce0b20331a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 1s - loss: 0.5268 - accuracy: 0.8376 - 1s/epoch - 41ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWDZH2sxFoVG"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/h.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T61G_KrM69e"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3T76SyEM69g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHAPPvX8M69g"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/i.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QriMxPVQM69g",
    "outputId": "d69bb2f0-aceb-4b50-f685-7f61e5b5ed76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "357/357 [==============================] - 32s 82ms/step - loss: 1.6915 - accuracy: 0.4796 - val_loss: 2.8780 - val_accuracy: 0.1881\n",
      "Epoch 2/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 1.1600 - accuracy: 0.6218 - val_loss: 0.9487 - val_accuracy: 0.6875\n",
      "Epoch 3/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.9928 - accuracy: 0.6776 - val_loss: 0.8242 - val_accuracy: 0.7247\n",
      "Epoch 4/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.8892 - accuracy: 0.7076 - val_loss: 0.7893 - val_accuracy: 0.7300\n",
      "Epoch 5/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.8219 - accuracy: 0.7327 - val_loss: 0.7688 - val_accuracy: 0.7447\n",
      "Epoch 6/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.7698 - accuracy: 0.7503 - val_loss: 0.7510 - val_accuracy: 0.7492\n",
      "Epoch 7/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.7281 - accuracy: 0.7643 - val_loss: 0.7475 - val_accuracy: 0.7579\n",
      "Epoch 8/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.6896 - accuracy: 0.7754 - val_loss: 0.6240 - val_accuracy: 0.7915\n",
      "Epoch 9/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.6624 - accuracy: 0.7845 - val_loss: 0.6384 - val_accuracy: 0.7885\n",
      "Epoch 10/200\n",
      "357/357 [==============================] - 30s 83ms/step - loss: 0.6313 - accuracy: 0.7956 - val_loss: 0.6056 - val_accuracy: 0.7968\n",
      "Epoch 11/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.6026 - accuracy: 0.8013 - val_loss: 0.7647 - val_accuracy: 0.7420\n",
      "Epoch 12/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.5850 - accuracy: 0.8067 - val_loss: 0.5925 - val_accuracy: 0.7988\n",
      "Epoch 13/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5637 - accuracy: 0.8165 - val_loss: 0.6808 - val_accuracy: 0.7669\n",
      "Epoch 14/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5423 - accuracy: 0.8213 - val_loss: 0.6455 - val_accuracy: 0.7799\n",
      "Epoch 15/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.5292 - accuracy: 0.8251 - val_loss: 0.5317 - val_accuracy: 0.8189\n",
      "Epoch 16/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5205 - accuracy: 0.8289 - val_loss: 0.6222 - val_accuracy: 0.7828\n",
      "Epoch 17/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.5062 - accuracy: 0.8332 - val_loss: 0.5322 - val_accuracy: 0.8197\n",
      "Epoch 18/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4865 - accuracy: 0.8394 - val_loss: 0.6136 - val_accuracy: 0.7872\n",
      "Epoch 19/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4744 - accuracy: 0.8440 - val_loss: 0.5540 - val_accuracy: 0.8164\n",
      "Epoch 20/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4663 - accuracy: 0.8464 - val_loss: 0.5823 - val_accuracy: 0.8117\n",
      "Epoch 21/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4577 - accuracy: 0.8482 - val_loss: 0.5552 - val_accuracy: 0.8197\n",
      "Epoch 22/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4479 - accuracy: 0.8514 - val_loss: 0.5669 - val_accuracy: 0.8159\n",
      "Epoch 23/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4399 - accuracy: 0.8536 - val_loss: 0.5353 - val_accuracy: 0.8277\n",
      "Epoch 24/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4307 - accuracy: 0.8556 - val_loss: 0.5392 - val_accuracy: 0.8251\n",
      "Epoch 25/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4273 - accuracy: 0.8562 - val_loss: 0.5959 - val_accuracy: 0.8161\n",
      "Epoch 26/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4185 - accuracy: 0.8591 - val_loss: 0.5718 - val_accuracy: 0.8200\n",
      "Epoch 27/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4109 - accuracy: 0.8622 - val_loss: 0.5363 - val_accuracy: 0.8304\n",
      "Epoch 28/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4058 - accuracy: 0.8627 - val_loss: 0.7466 - val_accuracy: 0.7680\n",
      "Epoch 29/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.4011 - accuracy: 0.8636 - val_loss: 0.6006 - val_accuracy: 0.8188\n",
      "Epoch 30/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.3931 - accuracy: 0.8678 - val_loss: 0.5171 - val_accuracy: 0.8285\n",
      "Epoch 31/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3892 - accuracy: 0.8684 - val_loss: 0.5360 - val_accuracy: 0.8333\n",
      "Epoch 32/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3814 - accuracy: 0.8700 - val_loss: 0.5999 - val_accuracy: 0.8148\n",
      "Epoch 33/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3843 - accuracy: 0.8692 - val_loss: 0.5994 - val_accuracy: 0.8128\n",
      "Epoch 34/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3773 - accuracy: 0.8715 - val_loss: 0.5981 - val_accuracy: 0.8219\n",
      "Epoch 35/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3707 - accuracy: 0.8733 - val_loss: 0.6621 - val_accuracy: 0.7847\n",
      "Epoch 36/200\n",
      "357/357 [==============================] - 30s 82ms/step - loss: 0.3633 - accuracy: 0.8762 - val_loss: 0.5168 - val_accuracy: 0.8405\n",
      "Epoch 37/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3632 - accuracy: 0.8763 - val_loss: 0.5176 - val_accuracy: 0.8441\n",
      "Epoch 38/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3585 - accuracy: 0.8769 - val_loss: 0.5827 - val_accuracy: 0.8252\n",
      "Epoch 39/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3542 - accuracy: 0.8787 - val_loss: 0.5722 - val_accuracy: 0.8148\n",
      "Epoch 40/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3502 - accuracy: 0.8794 - val_loss: 0.5894 - val_accuracy: 0.8232\n",
      "Epoch 41/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3457 - accuracy: 0.8812 - val_loss: 0.5655 - val_accuracy: 0.8292\n",
      "Epoch 42/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3405 - accuracy: 0.8826 - val_loss: 0.5308 - val_accuracy: 0.8348\n",
      "Epoch 43/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3373 - accuracy: 0.8839 - val_loss: 0.5583 - val_accuracy: 0.8299\n",
      "Epoch 44/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3335 - accuracy: 0.8851 - val_loss: 0.5928 - val_accuracy: 0.8173\n",
      "Epoch 45/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3304 - accuracy: 0.8851 - val_loss: 0.6388 - val_accuracy: 0.7997\n",
      "Epoch 46/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3254 - accuracy: 0.8860 - val_loss: 0.7959 - val_accuracy: 0.7480\n",
      "Epoch 47/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3311 - accuracy: 0.8864 - val_loss: 0.5843 - val_accuracy: 0.8185\n",
      "Epoch 48/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3250 - accuracy: 0.8887 - val_loss: 0.5326 - val_accuracy: 0.8360\n",
      "Epoch 49/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3244 - accuracy: 0.8881 - val_loss: 0.5941 - val_accuracy: 0.8253\n",
      "Epoch 50/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3179 - accuracy: 0.8897 - val_loss: 0.5470 - val_accuracy: 0.8280\n",
      "Epoch 51/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3129 - accuracy: 0.8909 - val_loss: 0.5611 - val_accuracy: 0.8351\n",
      "Epoch 52/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3171 - accuracy: 0.8908 - val_loss: 0.5529 - val_accuracy: 0.8380\n",
      "Epoch 53/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3091 - accuracy: 0.8926 - val_loss: 0.5504 - val_accuracy: 0.8387\n",
      "Epoch 54/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3075 - accuracy: 0.8936 - val_loss: 0.5396 - val_accuracy: 0.8437\n",
      "Epoch 55/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.3072 - accuracy: 0.8916 - val_loss: 0.6021 - val_accuracy: 0.8257\n",
      "Epoch 56/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3071 - accuracy: 0.8929 - val_loss: 0.5650 - val_accuracy: 0.8384\n",
      "Epoch 57/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3065 - accuracy: 0.8925 - val_loss: 0.5479 - val_accuracy: 0.8325\n",
      "Epoch 58/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3010 - accuracy: 0.8954 - val_loss: 0.6931 - val_accuracy: 0.8071\n",
      "Epoch 59/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.3013 - accuracy: 0.8955 - val_loss: 0.7166 - val_accuracy: 0.7976\n",
      "Epoch 60/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2965 - accuracy: 0.8965 - val_loss: 0.5766 - val_accuracy: 0.8392\n",
      "Epoch 61/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2955 - accuracy: 0.8967 - val_loss: 0.5484 - val_accuracy: 0.8384\n",
      "Epoch 62/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2966 - accuracy: 0.8963 - val_loss: 0.6218 - val_accuracy: 0.8217\n",
      "Epoch 63/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2922 - accuracy: 0.8971 - val_loss: 0.5489 - val_accuracy: 0.8352\n",
      "Epoch 64/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2877 - accuracy: 0.8996 - val_loss: 0.5658 - val_accuracy: 0.8279\n",
      "Epoch 65/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2875 - accuracy: 0.8996 - val_loss: 0.5647 - val_accuracy: 0.8443\n",
      "Epoch 66/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2853 - accuracy: 0.9003 - val_loss: 0.6009 - val_accuracy: 0.8207\n",
      "Epoch 67/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2852 - accuracy: 0.8994 - val_loss: 0.5505 - val_accuracy: 0.8441\n",
      "Epoch 68/200\n",
      "357/357 [==============================] - 29s 79ms/step - loss: 0.2838 - accuracy: 0.9011 - val_loss: 0.5329 - val_accuracy: 0.8399\n",
      "Epoch 69/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2800 - accuracy: 0.9019 - val_loss: 0.6438 - val_accuracy: 0.8060\n",
      "Epoch 70/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2805 - accuracy: 0.9015 - val_loss: 0.5560 - val_accuracy: 0.8380\n",
      "Epoch 71/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2813 - accuracy: 0.9017 - val_loss: 0.5959 - val_accuracy: 0.8340\n",
      "Epoch 72/200\n",
      "357/357 [==============================] - 29s 80ms/step - loss: 0.2792 - accuracy: 0.9024 - val_loss: 0.6029 - val_accuracy: 0.8353\n",
      "Epoch 73/200\n",
      "106/357 [=======>......................] - ETA: 19s - loss: 0.2819 - accuracy: 0.9023"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-b733a679450a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history2 = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_ds_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \"\"\"\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history2 = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZuYCGMSM69h"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/h.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBd9tKFSM69i",
    "outputId": "e5c9fab9-24b7-4f68-a9d8-3b5a300744e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 1s - loss: 0.5168 - accuracy: 0.8405 - 1s/epoch - 40ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK97Dmy4M69i"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/keras/h.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DALYbJZwWLEC"
   },
   "outputs": [],
   "source": [
    "# Setting up model\n",
    "img_width, img_height = 70, 70\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "n_classes = 15\n",
    "input_shape= (img_width, img_height, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1n5EpKqWLEE"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPKjIQLFWLEE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOxHHmDLWLEE"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/i.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujQEkPrmWLEF",
    "outputId": "94133ec9-9116-4030-9497-c32214b913a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "357/357 [==============================] - 28s 65ms/step - loss: 1.5629 - accuracy: 0.5103 - val_loss: 2.1090 - val_accuracy: 0.3393\n",
      "Epoch 2/200\n",
      "357/357 [==============================] - 24s 65ms/step - loss: 1.0675 - accuracy: 0.6529 - val_loss: 0.9357 - val_accuracy: 0.6648\n",
      "Epoch 3/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.9315 - accuracy: 0.6958 - val_loss: 0.8541 - val_accuracy: 0.7143\n",
      "Epoch 4/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.8480 - accuracy: 0.7267 - val_loss: 0.7463 - val_accuracy: 0.7504\n",
      "Epoch 5/200\n",
      "357/357 [==============================] - 24s 65ms/step - loss: 0.7876 - accuracy: 0.7453 - val_loss: 0.6954 - val_accuracy: 0.7659\n",
      "Epoch 6/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.7454 - accuracy: 0.7619 - val_loss: 0.7308 - val_accuracy: 0.7531\n",
      "Epoch 7/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.7089 - accuracy: 0.7716 - val_loss: 0.7110 - val_accuracy: 0.7557\n",
      "Epoch 8/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.6798 - accuracy: 0.7823 - val_loss: 0.5957 - val_accuracy: 0.8007\n",
      "Epoch 9/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.6563 - accuracy: 0.7893 - val_loss: 0.7259 - val_accuracy: 0.7777\n",
      "Epoch 10/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.6355 - accuracy: 0.7950 - val_loss: 0.6235 - val_accuracy: 0.7956\n",
      "Epoch 11/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.6143 - accuracy: 0.8012 - val_loss: 0.6881 - val_accuracy: 0.7760\n",
      "Epoch 12/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.6076 - accuracy: 0.8047 - val_loss: 0.6087 - val_accuracy: 0.7944\n",
      "Epoch 13/200\n",
      "357/357 [==============================] - 23s 65ms/step - loss: 0.5850 - accuracy: 0.8111 - val_loss: 0.5395 - val_accuracy: 0.8196\n",
      "Epoch 14/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.5790 - accuracy: 0.8115 - val_loss: 0.5873 - val_accuracy: 0.8052\n",
      "Epoch 15/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.5667 - accuracy: 0.8162 - val_loss: 0.5496 - val_accuracy: 0.8191\n",
      "Epoch 16/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.5556 - accuracy: 0.8199 - val_loss: 0.5513 - val_accuracy: 0.8165\n",
      "Epoch 17/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.5441 - accuracy: 0.8230 - val_loss: 0.5565 - val_accuracy: 0.8131\n",
      "Epoch 18/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.5391 - accuracy: 0.8242 - val_loss: 0.5782 - val_accuracy: 0.8113\n",
      "Epoch 19/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5313 - accuracy: 0.8279 - val_loss: 0.5326 - val_accuracy: 0.8219\n",
      "Epoch 20/200\n",
      "357/357 [==============================] - 24s 65ms/step - loss: 0.5187 - accuracy: 0.8303 - val_loss: 0.5160 - val_accuracy: 0.8307\n",
      "Epoch 21/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.5143 - accuracy: 0.8320 - val_loss: 0.6724 - val_accuracy: 0.7820\n",
      "Epoch 22/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.5032 - accuracy: 0.8352 - val_loss: 0.5094 - val_accuracy: 0.8252\n",
      "Epoch 23/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.5027 - accuracy: 0.8355 - val_loss: 0.5257 - val_accuracy: 0.8249\n",
      "Epoch 24/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4955 - accuracy: 0.8374 - val_loss: 0.5243 - val_accuracy: 0.8252\n",
      "Epoch 25/200\n",
      "357/357 [==============================] - 24s 65ms/step - loss: 0.4903 - accuracy: 0.8404 - val_loss: 0.4896 - val_accuracy: 0.8443\n",
      "Epoch 26/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4858 - accuracy: 0.8411 - val_loss: 0.5871 - val_accuracy: 0.7997\n",
      "Epoch 27/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4817 - accuracy: 0.8416 - val_loss: 0.5324 - val_accuracy: 0.8256\n",
      "Epoch 28/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4716 - accuracy: 0.8445 - val_loss: 0.5311 - val_accuracy: 0.8300\n",
      "Epoch 29/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4690 - accuracy: 0.8451 - val_loss: 0.5382 - val_accuracy: 0.8248\n",
      "Epoch 30/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4693 - accuracy: 0.8457 - val_loss: 0.4886 - val_accuracy: 0.8420\n",
      "Epoch 31/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4609 - accuracy: 0.8488 - val_loss: 0.5356 - val_accuracy: 0.8259\n",
      "Epoch 32/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4583 - accuracy: 0.8482 - val_loss: 0.5266 - val_accuracy: 0.8333\n",
      "Epoch 33/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4534 - accuracy: 0.8505 - val_loss: 0.5401 - val_accuracy: 0.8225\n",
      "Epoch 34/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4581 - accuracy: 0.8494 - val_loss: 0.5042 - val_accuracy: 0.8324\n",
      "Epoch 35/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4471 - accuracy: 0.8502 - val_loss: 0.4994 - val_accuracy: 0.8351\n",
      "Epoch 36/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4420 - accuracy: 0.8537 - val_loss: 0.5069 - val_accuracy: 0.8401\n",
      "Epoch 37/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4432 - accuracy: 0.8539 - val_loss: 0.5072 - val_accuracy: 0.8404\n",
      "Epoch 38/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4368 - accuracy: 0.8543 - val_loss: 0.5090 - val_accuracy: 0.8371\n",
      "Epoch 39/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4371 - accuracy: 0.8550 - val_loss: 0.5533 - val_accuracy: 0.8268\n",
      "Epoch 40/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.4316 - accuracy: 0.8565 - val_loss: 0.4873 - val_accuracy: 0.8408\n",
      "Epoch 41/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4331 - accuracy: 0.8562 - val_loss: 0.5163 - val_accuracy: 0.8303\n",
      "Epoch 42/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4348 - accuracy: 0.8554 - val_loss: 0.6368 - val_accuracy: 0.8109\n",
      "Epoch 43/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4262 - accuracy: 0.8565 - val_loss: 0.5196 - val_accuracy: 0.8389\n",
      "Epoch 44/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4241 - accuracy: 0.8595 - val_loss: 0.4884 - val_accuracy: 0.8448\n",
      "Epoch 45/200\n",
      "357/357 [==============================] - 24s 67ms/step - loss: 0.4208 - accuracy: 0.8596 - val_loss: 0.4845 - val_accuracy: 0.8415\n",
      "Epoch 46/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4186 - accuracy: 0.8595 - val_loss: 0.5548 - val_accuracy: 0.8267\n",
      "Epoch 47/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4206 - accuracy: 0.8600 - val_loss: 0.4938 - val_accuracy: 0.8416\n",
      "Epoch 48/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4156 - accuracy: 0.8618 - val_loss: 0.5282 - val_accuracy: 0.8325\n",
      "Epoch 49/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4110 - accuracy: 0.8623 - val_loss: 0.5038 - val_accuracy: 0.8445\n",
      "Epoch 50/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4087 - accuracy: 0.8632 - val_loss: 0.4856 - val_accuracy: 0.8448\n",
      "Epoch 51/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4077 - accuracy: 0.8629 - val_loss: 0.5005 - val_accuracy: 0.8428\n",
      "Epoch 52/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4032 - accuracy: 0.8655 - val_loss: 0.5398 - val_accuracy: 0.8303\n",
      "Epoch 53/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4041 - accuracy: 0.8635 - val_loss: 0.5304 - val_accuracy: 0.8320\n",
      "Epoch 54/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.4003 - accuracy: 0.8660 - val_loss: 0.4898 - val_accuracy: 0.8453\n",
      "Epoch 55/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.4059 - accuracy: 0.8646 - val_loss: 0.4878 - val_accuracy: 0.8460\n",
      "Epoch 56/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3987 - accuracy: 0.8662 - val_loss: 0.5448 - val_accuracy: 0.8363\n",
      "Epoch 57/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3961 - accuracy: 0.8671 - val_loss: 0.5146 - val_accuracy: 0.8357\n",
      "Epoch 58/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3958 - accuracy: 0.8651 - val_loss: 0.5358 - val_accuracy: 0.8367\n",
      "Epoch 59/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3903 - accuracy: 0.8679 - val_loss: 0.5083 - val_accuracy: 0.8411\n",
      "Epoch 60/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3951 - accuracy: 0.8664 - val_loss: 0.4844 - val_accuracy: 0.8484\n",
      "Epoch 61/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3884 - accuracy: 0.8688 - val_loss: 0.4975 - val_accuracy: 0.8371\n",
      "Epoch 62/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3878 - accuracy: 0.8689 - val_loss: 0.4923 - val_accuracy: 0.8445\n",
      "Epoch 63/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3870 - accuracy: 0.8697 - val_loss: 0.5084 - val_accuracy: 0.8415\n",
      "Epoch 64/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3859 - accuracy: 0.8709 - val_loss: 0.5251 - val_accuracy: 0.8408\n",
      "Epoch 65/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3852 - accuracy: 0.8695 - val_loss: 0.4952 - val_accuracy: 0.8449\n",
      "Epoch 66/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3839 - accuracy: 0.8700 - val_loss: 0.5075 - val_accuracy: 0.8412\n",
      "Epoch 67/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3816 - accuracy: 0.8719 - val_loss: 0.4993 - val_accuracy: 0.8451\n",
      "Epoch 68/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3834 - accuracy: 0.8697 - val_loss: 0.4979 - val_accuracy: 0.8431\n",
      "Epoch 69/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3776 - accuracy: 0.8721 - val_loss: 0.5353 - val_accuracy: 0.8405\n",
      "Epoch 70/200\n",
      "357/357 [==============================] - 23s 64ms/step - loss: 0.3746 - accuracy: 0.8728 - val_loss: 0.4810 - val_accuracy: 0.8476\n",
      "Epoch 71/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3780 - accuracy: 0.8730 - val_loss: 0.4880 - val_accuracy: 0.8501\n",
      "Epoch 72/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3715 - accuracy: 0.8731 - val_loss: 0.4868 - val_accuracy: 0.8427\n",
      "Epoch 73/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3714 - accuracy: 0.8743 - val_loss: 0.5558 - val_accuracy: 0.8305\n",
      "Epoch 74/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3709 - accuracy: 0.8750 - val_loss: 0.4925 - val_accuracy: 0.8456\n",
      "Epoch 75/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3703 - accuracy: 0.8740 - val_loss: 0.5619 - val_accuracy: 0.8244\n",
      "Epoch 76/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3728 - accuracy: 0.8735 - val_loss: 0.4924 - val_accuracy: 0.8499\n",
      "Epoch 77/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3698 - accuracy: 0.8747 - val_loss: 0.4939 - val_accuracy: 0.8465\n",
      "Epoch 78/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3684 - accuracy: 0.8744 - val_loss: 0.5066 - val_accuracy: 0.8496\n",
      "Epoch 79/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3658 - accuracy: 0.8758 - val_loss: 0.5199 - val_accuracy: 0.8365\n",
      "Epoch 80/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3631 - accuracy: 0.8781 - val_loss: 0.4955 - val_accuracy: 0.8460\n",
      "Epoch 81/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3648 - accuracy: 0.8758 - val_loss: 0.4863 - val_accuracy: 0.8481\n",
      "Epoch 82/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3636 - accuracy: 0.8769 - val_loss: 0.5036 - val_accuracy: 0.8467\n",
      "Epoch 83/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3617 - accuracy: 0.8764 - val_loss: 0.5786 - val_accuracy: 0.8368\n",
      "Epoch 84/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3628 - accuracy: 0.8758 - val_loss: 0.5385 - val_accuracy: 0.8388\n",
      "Epoch 85/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3616 - accuracy: 0.8769 - val_loss: 0.5131 - val_accuracy: 0.8468\n",
      "Epoch 86/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3595 - accuracy: 0.8782 - val_loss: 0.5126 - val_accuracy: 0.8429\n",
      "Epoch 87/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3592 - accuracy: 0.8781 - val_loss: 0.4926 - val_accuracy: 0.8469\n",
      "Epoch 88/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3578 - accuracy: 0.8777 - val_loss: 0.5613 - val_accuracy: 0.8335\n",
      "Epoch 89/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3548 - accuracy: 0.8787 - val_loss: 0.5558 - val_accuracy: 0.8327\n",
      "Epoch 90/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3549 - accuracy: 0.8790 - val_loss: 0.5031 - val_accuracy: 0.8433\n",
      "Epoch 91/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3556 - accuracy: 0.8784 - val_loss: 0.6108 - val_accuracy: 0.8249\n",
      "Epoch 92/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3514 - accuracy: 0.8804 - val_loss: 0.5071 - val_accuracy: 0.8452\n",
      "Epoch 93/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3550 - accuracy: 0.8785 - val_loss: 0.5053 - val_accuracy: 0.8419\n",
      "Epoch 94/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3528 - accuracy: 0.8793 - val_loss: 0.5253 - val_accuracy: 0.8363\n",
      "Epoch 95/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3522 - accuracy: 0.8793 - val_loss: 0.5491 - val_accuracy: 0.8351\n",
      "Epoch 96/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3487 - accuracy: 0.8804 - val_loss: 0.6277 - val_accuracy: 0.8208\n",
      "Epoch 97/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3474 - accuracy: 0.8811 - val_loss: 0.4949 - val_accuracy: 0.8479\n",
      "Epoch 98/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3514 - accuracy: 0.8793 - val_loss: 0.6309 - val_accuracy: 0.8201\n",
      "Epoch 99/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.3543 - accuracy: 0.8779 - val_loss: 0.5497 - val_accuracy: 0.8355\n",
      "Epoch 100/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3493 - accuracy: 0.8793 - val_loss: 0.4958 - val_accuracy: 0.8468\n",
      "Epoch 101/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3504 - accuracy: 0.8789 - val_loss: 0.6191 - val_accuracy: 0.8248\n",
      "Epoch 102/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3499 - accuracy: 0.8803 - val_loss: 0.5045 - val_accuracy: 0.8531\n",
      "Epoch 103/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3463 - accuracy: 0.8808 - val_loss: 0.5106 - val_accuracy: 0.8519\n",
      "Epoch 104/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3452 - accuracy: 0.8825 - val_loss: 0.5033 - val_accuracy: 0.8487\n",
      "Epoch 105/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3439 - accuracy: 0.8814 - val_loss: 0.5109 - val_accuracy: 0.8441\n",
      "Epoch 106/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3427 - accuracy: 0.8821 - val_loss: 0.4817 - val_accuracy: 0.8531\n",
      "Epoch 107/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3445 - accuracy: 0.8823 - val_loss: 0.4961 - val_accuracy: 0.8431\n",
      "Epoch 108/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3415 - accuracy: 0.8835 - val_loss: 0.5205 - val_accuracy: 0.8457\n",
      "Epoch 109/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3409 - accuracy: 0.8827 - val_loss: 0.5053 - val_accuracy: 0.8449\n",
      "Epoch 110/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3401 - accuracy: 0.8829 - val_loss: 0.4986 - val_accuracy: 0.8541\n",
      "Epoch 111/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3434 - accuracy: 0.8820 - val_loss: 0.4953 - val_accuracy: 0.8520\n",
      "Epoch 112/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3418 - accuracy: 0.8831 - val_loss: 0.4875 - val_accuracy: 0.8516\n",
      "Epoch 113/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3389 - accuracy: 0.8836 - val_loss: 0.4983 - val_accuracy: 0.8475\n",
      "Epoch 114/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3354 - accuracy: 0.8845 - val_loss: 0.5069 - val_accuracy: 0.8501\n",
      "Epoch 115/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3399 - accuracy: 0.8846 - val_loss: 0.5104 - val_accuracy: 0.8447\n",
      "Epoch 116/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3383 - accuracy: 0.8837 - val_loss: 0.5011 - val_accuracy: 0.8487\n",
      "Epoch 117/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3335 - accuracy: 0.8844 - val_loss: 0.5160 - val_accuracy: 0.8456\n",
      "Epoch 118/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3387 - accuracy: 0.8832 - val_loss: 0.6637 - val_accuracy: 0.8181\n",
      "Epoch 119/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3301 - accuracy: 0.8852 - val_loss: 0.5116 - val_accuracy: 0.8483\n",
      "Epoch 120/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3340 - accuracy: 0.8847 - val_loss: 0.5066 - val_accuracy: 0.8449\n",
      "Epoch 121/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3337 - accuracy: 0.8842 - val_loss: 0.5112 - val_accuracy: 0.8456\n",
      "Epoch 122/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3319 - accuracy: 0.8856 - val_loss: 0.6950 - val_accuracy: 0.8221\n",
      "Epoch 123/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3331 - accuracy: 0.8851 - val_loss: 0.5031 - val_accuracy: 0.8471\n",
      "Epoch 124/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3314 - accuracy: 0.8856 - val_loss: 0.5215 - val_accuracy: 0.8428\n",
      "Epoch 125/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3321 - accuracy: 0.8861 - val_loss: 0.5131 - val_accuracy: 0.8505\n",
      "Epoch 126/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3300 - accuracy: 0.8864 - val_loss: 0.5455 - val_accuracy: 0.8431\n",
      "Epoch 127/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3319 - accuracy: 0.8856 - val_loss: 0.5045 - val_accuracy: 0.8495\n",
      "Epoch 128/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3279 - accuracy: 0.8865 - val_loss: 0.4937 - val_accuracy: 0.8485\n",
      "Epoch 129/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3325 - accuracy: 0.8864 - val_loss: 0.5412 - val_accuracy: 0.8413\n",
      "Epoch 130/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3287 - accuracy: 0.8868 - val_loss: 0.5020 - val_accuracy: 0.8488\n",
      "Epoch 131/200\n",
      "357/357 [==============================] - 23s 63ms/step - loss: 0.3285 - accuracy: 0.8859 - val_loss: 0.5126 - val_accuracy: 0.8477\n",
      "Epoch 132/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3268 - accuracy: 0.8879 - val_loss: 0.5852 - val_accuracy: 0.8337\n",
      "Epoch 133/200\n",
      "357/357 [==============================] - 22s 61ms/step - loss: 0.3279 - accuracy: 0.8872 - val_loss: 0.4992 - val_accuracy: 0.8484\n",
      "Epoch 134/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3278 - accuracy: 0.8866 - val_loss: 0.5767 - val_accuracy: 0.8283\n",
      "Epoch 135/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3246 - accuracy: 0.8883 - val_loss: 0.5348 - val_accuracy: 0.8396\n",
      "Epoch 136/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3233 - accuracy: 0.8883 - val_loss: 0.5120 - val_accuracy: 0.8495\n",
      "Epoch 137/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3241 - accuracy: 0.8877 - val_loss: 0.5158 - val_accuracy: 0.8444\n",
      "Epoch 138/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3243 - accuracy: 0.8893 - val_loss: 0.5071 - val_accuracy: 0.8485\n",
      "Epoch 139/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3263 - accuracy: 0.8882 - val_loss: 0.5421 - val_accuracy: 0.8293\n",
      "Epoch 140/200\n",
      "357/357 [==============================] - 22s 61ms/step - loss: 0.3214 - accuracy: 0.8898 - val_loss: 0.7444 - val_accuracy: 0.8124\n",
      "Epoch 141/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3226 - accuracy: 0.8889 - val_loss: 0.5151 - val_accuracy: 0.8477\n",
      "Epoch 142/200\n",
      "357/357 [==============================] - 22s 61ms/step - loss: 0.3227 - accuracy: 0.8881 - val_loss: 0.6458 - val_accuracy: 0.8244\n",
      "Epoch 143/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3205 - accuracy: 0.8891 - val_loss: 0.5274 - val_accuracy: 0.8428\n",
      "Epoch 144/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3207 - accuracy: 0.8891 - val_loss: 0.5395 - val_accuracy: 0.8340\n",
      "Epoch 145/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3235 - accuracy: 0.8876 - val_loss: 0.5040 - val_accuracy: 0.8519\n",
      "Epoch 146/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3202 - accuracy: 0.8891 - val_loss: 0.5717 - val_accuracy: 0.8393\n",
      "Epoch 147/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3217 - accuracy: 0.8874 - val_loss: 0.5147 - val_accuracy: 0.8521\n",
      "Epoch 148/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3199 - accuracy: 0.8895 - val_loss: 0.5464 - val_accuracy: 0.8405\n",
      "Epoch 149/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3228 - accuracy: 0.8881 - val_loss: 0.5131 - val_accuracy: 0.8485\n",
      "Epoch 150/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3184 - accuracy: 0.8892 - val_loss: 0.5131 - val_accuracy: 0.8461\n",
      "Epoch 151/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3178 - accuracy: 0.8898 - val_loss: 0.4908 - val_accuracy: 0.8529\n",
      "Epoch 152/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3167 - accuracy: 0.8908 - val_loss: 0.4891 - val_accuracy: 0.8496\n",
      "Epoch 153/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3188 - accuracy: 0.8883 - val_loss: 0.4903 - val_accuracy: 0.8519\n",
      "Epoch 154/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3190 - accuracy: 0.8892 - val_loss: 0.4899 - val_accuracy: 0.8547\n",
      "Epoch 155/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3169 - accuracy: 0.8901 - val_loss: 0.5203 - val_accuracy: 0.8469\n",
      "Epoch 156/200\n",
      "357/357 [==============================] - 22s 61ms/step - loss: 0.3171 - accuracy: 0.8914 - val_loss: 0.5022 - val_accuracy: 0.8499\n",
      "Epoch 157/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3150 - accuracy: 0.8904 - val_loss: 0.5746 - val_accuracy: 0.8395\n",
      "Epoch 158/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3178 - accuracy: 0.8907 - val_loss: 0.7370 - val_accuracy: 0.7975\n",
      "Epoch 159/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3145 - accuracy: 0.8909 - val_loss: 0.5705 - val_accuracy: 0.8364\n",
      "Epoch 160/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3143 - accuracy: 0.8906 - val_loss: 0.4926 - val_accuracy: 0.8524\n",
      "Epoch 161/200\n",
      "357/357 [==============================] - 22s 61ms/step - loss: 0.3167 - accuracy: 0.8896 - val_loss: 0.4898 - val_accuracy: 0.8501\n",
      "Epoch 162/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3156 - accuracy: 0.8911 - val_loss: 0.5211 - val_accuracy: 0.8513\n",
      "Epoch 163/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3148 - accuracy: 0.8903 - val_loss: 0.5446 - val_accuracy: 0.8431\n",
      "Epoch 164/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3150 - accuracy: 0.8903 - val_loss: 0.5059 - val_accuracy: 0.8535\n",
      "Epoch 165/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3141 - accuracy: 0.8913 - val_loss: 0.5827 - val_accuracy: 0.8317\n",
      "Epoch 166/200\n",
      "357/357 [==============================] - 22s 61ms/step - loss: 0.3141 - accuracy: 0.8899 - val_loss: 0.5068 - val_accuracy: 0.8511\n",
      "Epoch 167/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3119 - accuracy: 0.8927 - val_loss: 0.5234 - val_accuracy: 0.8468\n",
      "Epoch 168/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3124 - accuracy: 0.8909 - val_loss: 0.4978 - val_accuracy: 0.8525\n",
      "Epoch 169/200\n",
      "357/357 [==============================] - 22s 61ms/step - loss: 0.3097 - accuracy: 0.8918 - val_loss: 0.5361 - val_accuracy: 0.8496\n",
      "Epoch 170/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3110 - accuracy: 0.8925 - val_loss: 0.5041 - val_accuracy: 0.8516\n",
      "Epoch 171/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3119 - accuracy: 0.8915 - val_loss: 0.5919 - val_accuracy: 0.8351\n",
      "Epoch 172/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3102 - accuracy: 0.8928 - val_loss: 0.5163 - val_accuracy: 0.8475\n",
      "Epoch 173/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3128 - accuracy: 0.8923 - val_loss: 0.5151 - val_accuracy: 0.8480\n",
      "Epoch 174/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3102 - accuracy: 0.8915 - val_loss: 0.4881 - val_accuracy: 0.8567\n",
      "Epoch 175/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3104 - accuracy: 0.8917 - val_loss: 0.4892 - val_accuracy: 0.8553\n",
      "Epoch 176/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3101 - accuracy: 0.8920 - val_loss: 0.5084 - val_accuracy: 0.8504\n",
      "Epoch 177/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3092 - accuracy: 0.8920 - val_loss: 0.5296 - val_accuracy: 0.8471\n",
      "Epoch 178/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3087 - accuracy: 0.8925 - val_loss: 0.5012 - val_accuracy: 0.8513\n",
      "Epoch 179/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3111 - accuracy: 0.8910 - val_loss: 0.5157 - val_accuracy: 0.8505\n",
      "Epoch 180/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3089 - accuracy: 0.8928 - val_loss: 0.4994 - val_accuracy: 0.8555\n",
      "Epoch 181/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3078 - accuracy: 0.8923 - val_loss: 0.5362 - val_accuracy: 0.8425\n",
      "Epoch 182/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3114 - accuracy: 0.8922 - val_loss: 0.5146 - val_accuracy: 0.8555\n",
      "Epoch 183/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3077 - accuracy: 0.8931 - val_loss: 0.4953 - val_accuracy: 0.8539\n",
      "Epoch 184/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3071 - accuracy: 0.8936 - val_loss: 0.5252 - val_accuracy: 0.8516\n",
      "Epoch 185/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3090 - accuracy: 0.8925 - val_loss: 0.5268 - val_accuracy: 0.8448\n",
      "Epoch 186/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3054 - accuracy: 0.8946 - val_loss: 0.5101 - val_accuracy: 0.8528\n",
      "Epoch 187/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3060 - accuracy: 0.8938 - val_loss: 0.4843 - val_accuracy: 0.8515\n",
      "Epoch 188/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3070 - accuracy: 0.8926 - val_loss: 0.5052 - val_accuracy: 0.8507\n",
      "Epoch 189/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3086 - accuracy: 0.8931 - val_loss: 0.5487 - val_accuracy: 0.8379\n",
      "Epoch 190/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3055 - accuracy: 0.8929 - val_loss: 0.5057 - val_accuracy: 0.8503\n",
      "Epoch 191/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3044 - accuracy: 0.8941 - val_loss: 0.5017 - val_accuracy: 0.8541\n",
      "Epoch 192/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3027 - accuracy: 0.8940 - val_loss: 0.5061 - val_accuracy: 0.8521\n",
      "Epoch 193/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3018 - accuracy: 0.8934 - val_loss: 0.5165 - val_accuracy: 0.8559\n",
      "Epoch 194/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3037 - accuracy: 0.8934 - val_loss: 0.5102 - val_accuracy: 0.8515\n",
      "Epoch 195/200\n",
      "357/357 [==============================] - 22s 61ms/step - loss: 0.3034 - accuracy: 0.8941 - val_loss: 0.5443 - val_accuracy: 0.8433\n",
      "Epoch 196/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3036 - accuracy: 0.8944 - val_loss: 0.5184 - val_accuracy: 0.8507\n",
      "Epoch 197/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3034 - accuracy: 0.8952 - val_loss: 0.5201 - val_accuracy: 0.8483\n",
      "Epoch 198/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3031 - accuracy: 0.8932 - val_loss: 0.5117 - val_accuracy: 0.8496\n",
      "Epoch 199/200\n",
      "357/357 [==============================] - 23s 62ms/step - loss: 0.3001 - accuracy: 0.8958 - val_loss: 0.5103 - val_accuracy: 0.8529\n",
      "Epoch 200/200\n",
      "357/357 [==============================] - 22s 62ms/step - loss: 0.3021 - accuracy: 0.8941 - val_loss: 0.5290 - val_accuracy: 0.8493\n"
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV3lGbJiWLEF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_aFmskeWLEG"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/i.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggm-SB-kWLEG",
    "outputId": "349f2ebd-8fe4-4a01-c327-1ef0be00b38a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 1s - loss: 0.4810 - accuracy: 0.8476 - 1s/epoch - 44ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eNkPzePY2qK"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/i.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4awY5zskA-8"
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(140, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4CRlz02kA--"
   },
   "outputs": [],
   "source": [
    "# Saving the weights for the lowest loss value\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/keras/j.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_EnFr2OkA-_",
    "outputId": "e5a09c20-5f49-4fc7-c619-89f781b817f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "357/357 [==============================] - 25s 56ms/step - loss: 1.9115 - accuracy: 0.4128 - val_loss: 2.0859 - val_accuracy: 0.3457\n",
      "Epoch 2/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 1.2892 - accuracy: 0.5893 - val_loss: 1.1201 - val_accuracy: 0.6269\n",
      "Epoch 3/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 1.1189 - accuracy: 0.6393 - val_loss: 0.9717 - val_accuracy: 0.6651\n",
      "Epoch 4/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 1.0262 - accuracy: 0.6695 - val_loss: 0.9340 - val_accuracy: 0.6793\n",
      "Epoch 5/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.9587 - accuracy: 0.6905 - val_loss: 0.8819 - val_accuracy: 0.7071\n",
      "Epoch 6/200\n",
      "357/357 [==============================] - 21s 58ms/step - loss: 0.9162 - accuracy: 0.7050 - val_loss: 0.8453 - val_accuracy: 0.7153\n",
      "Epoch 7/200\n",
      "357/357 [==============================] - 21s 58ms/step - loss: 0.8785 - accuracy: 0.7174 - val_loss: 0.7536 - val_accuracy: 0.7380\n",
      "Epoch 8/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.8587 - accuracy: 0.7248 - val_loss: 0.7608 - val_accuracy: 0.7372\n",
      "Epoch 9/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.8290 - accuracy: 0.7326 - val_loss: 0.7389 - val_accuracy: 0.7476\n",
      "Epoch 10/200\n",
      "357/357 [==============================] - 21s 56ms/step - loss: 0.8136 - accuracy: 0.7389 - val_loss: 0.7339 - val_accuracy: 0.7473\n",
      "Epoch 11/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.7937 - accuracy: 0.7456 - val_loss: 0.8456 - val_accuracy: 0.7156\n",
      "Epoch 12/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.7779 - accuracy: 0.7509 - val_loss: 1.0157 - val_accuracy: 0.6887\n",
      "Epoch 13/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.7666 - accuracy: 0.7537 - val_loss: 0.9406 - val_accuracy: 0.6999\n",
      "Epoch 14/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.7552 - accuracy: 0.7590 - val_loss: 0.7622 - val_accuracy: 0.7420\n",
      "Epoch 15/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.7437 - accuracy: 0.7622 - val_loss: 0.6908 - val_accuracy: 0.7679\n",
      "Epoch 16/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.7337 - accuracy: 0.7646 - val_loss: 0.6738 - val_accuracy: 0.7779\n",
      "Epoch 17/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.7270 - accuracy: 0.7655 - val_loss: 0.7152 - val_accuracy: 0.7636\n",
      "Epoch 18/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.7169 - accuracy: 0.7720 - val_loss: 0.8688 - val_accuracy: 0.7268\n",
      "Epoch 19/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.7144 - accuracy: 0.7714 - val_loss: 0.6803 - val_accuracy: 0.7615\n",
      "Epoch 20/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.7051 - accuracy: 0.7742 - val_loss: 0.6503 - val_accuracy: 0.7791\n",
      "Epoch 21/200\n",
      "357/357 [==============================] - 21s 58ms/step - loss: 0.6893 - accuracy: 0.7788 - val_loss: 0.6140 - val_accuracy: 0.7944\n",
      "Epoch 22/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6910 - accuracy: 0.7765 - val_loss: 0.6929 - val_accuracy: 0.7731\n",
      "Epoch 23/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6831 - accuracy: 0.7811 - val_loss: 0.7543 - val_accuracy: 0.7544\n",
      "Epoch 24/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.6835 - accuracy: 0.7814 - val_loss: 0.6106 - val_accuracy: 0.7937\n",
      "Epoch 25/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6776 - accuracy: 0.7825 - val_loss: 0.7282 - val_accuracy: 0.7592\n",
      "Epoch 26/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6739 - accuracy: 0.7842 - val_loss: 0.7189 - val_accuracy: 0.7672\n",
      "Epoch 27/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6684 - accuracy: 0.7846 - val_loss: 0.6745 - val_accuracy: 0.7743\n",
      "Epoch 28/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.6648 - accuracy: 0.7859 - val_loss: 0.5895 - val_accuracy: 0.7980\n",
      "Epoch 29/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6611 - accuracy: 0.7898 - val_loss: 0.6238 - val_accuracy: 0.7880\n",
      "Epoch 30/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6625 - accuracy: 0.7892 - val_loss: 0.6113 - val_accuracy: 0.8024\n",
      "Epoch 31/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6555 - accuracy: 0.7904 - val_loss: 0.6077 - val_accuracy: 0.7963\n",
      "Epoch 32/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6562 - accuracy: 0.7898 - val_loss: 0.6621 - val_accuracy: 0.7805\n",
      "Epoch 33/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6483 - accuracy: 0.7926 - val_loss: 0.5927 - val_accuracy: 0.7995\n",
      "Epoch 34/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6468 - accuracy: 0.7935 - val_loss: 0.5969 - val_accuracy: 0.8000\n",
      "Epoch 35/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6423 - accuracy: 0.7931 - val_loss: 0.6017 - val_accuracy: 0.7975\n",
      "Epoch 36/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.6400 - accuracy: 0.7948 - val_loss: 0.5841 - val_accuracy: 0.8025\n",
      "Epoch 37/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6392 - accuracy: 0.7964 - val_loss: 0.6061 - val_accuracy: 0.7991\n",
      "Epoch 38/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.6338 - accuracy: 0.7979 - val_loss: 0.6248 - val_accuracy: 0.7916\n",
      "Epoch 39/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6341 - accuracy: 0.7958 - val_loss: 0.7233 - val_accuracy: 0.7667\n",
      "Epoch 40/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6311 - accuracy: 0.7987 - val_loss: 0.5944 - val_accuracy: 0.8035\n",
      "Epoch 41/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.6278 - accuracy: 0.7990 - val_loss: 0.5813 - val_accuracy: 0.8053\n",
      "Epoch 42/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6224 - accuracy: 0.8012 - val_loss: 0.6039 - val_accuracy: 0.8019\n",
      "Epoch 43/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.6268 - accuracy: 0.7984 - val_loss: 0.5667 - val_accuracy: 0.8127\n",
      "Epoch 44/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6235 - accuracy: 0.7993 - val_loss: 0.6227 - val_accuracy: 0.7969\n",
      "Epoch 45/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6203 - accuracy: 0.8030 - val_loss: 0.5950 - val_accuracy: 0.8035\n",
      "Epoch 46/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6200 - accuracy: 0.8005 - val_loss: 0.5998 - val_accuracy: 0.7965\n",
      "Epoch 47/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6218 - accuracy: 0.8022 - val_loss: 0.6065 - val_accuracy: 0.7981\n",
      "Epoch 48/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6148 - accuracy: 0.8029 - val_loss: 0.5908 - val_accuracy: 0.8020\n",
      "Epoch 49/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.6167 - accuracy: 0.8018 - val_loss: 0.5536 - val_accuracy: 0.8148\n",
      "Epoch 50/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6137 - accuracy: 0.8041 - val_loss: 0.5681 - val_accuracy: 0.8097\n",
      "Epoch 51/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6099 - accuracy: 0.8044 - val_loss: 0.5904 - val_accuracy: 0.8043\n",
      "Epoch 52/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6106 - accuracy: 0.8056 - val_loss: 0.6416 - val_accuracy: 0.7860\n",
      "Epoch 53/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6088 - accuracy: 0.8057 - val_loss: 0.6221 - val_accuracy: 0.7879\n",
      "Epoch 54/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.6042 - accuracy: 0.8062 - val_loss: 0.5496 - val_accuracy: 0.8153\n",
      "Epoch 55/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.6056 - accuracy: 0.8068 - val_loss: 0.5467 - val_accuracy: 0.8179\n",
      "Epoch 56/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6009 - accuracy: 0.8077 - val_loss: 0.5605 - val_accuracy: 0.8113\n",
      "Epoch 57/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6017 - accuracy: 0.8072 - val_loss: 0.6114 - val_accuracy: 0.7932\n",
      "Epoch 58/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6002 - accuracy: 0.8072 - val_loss: 0.5498 - val_accuracy: 0.8139\n",
      "Epoch 59/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.6046 - accuracy: 0.8062 - val_loss: 0.5773 - val_accuracy: 0.8021\n",
      "Epoch 60/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5982 - accuracy: 0.8085 - val_loss: 0.6337 - val_accuracy: 0.7900\n",
      "Epoch 61/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5988 - accuracy: 0.8070 - val_loss: 0.5446 - val_accuracy: 0.8201\n",
      "Epoch 62/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.6002 - accuracy: 0.8066 - val_loss: 0.6666 - val_accuracy: 0.7808\n",
      "Epoch 63/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5938 - accuracy: 0.8085 - val_loss: 0.5850 - val_accuracy: 0.8051\n",
      "Epoch 64/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5916 - accuracy: 0.8102 - val_loss: 0.6162 - val_accuracy: 0.7937\n",
      "Epoch 65/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5980 - accuracy: 0.8087 - val_loss: 0.6516 - val_accuracy: 0.7895\n",
      "Epoch 66/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5899 - accuracy: 0.8106 - val_loss: 0.5496 - val_accuracy: 0.8189\n",
      "Epoch 67/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5905 - accuracy: 0.8107 - val_loss: 0.5780 - val_accuracy: 0.8083\n",
      "Epoch 68/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5856 - accuracy: 0.8128 - val_loss: 0.5569 - val_accuracy: 0.8105\n",
      "Epoch 69/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5884 - accuracy: 0.8127 - val_loss: 0.5839 - val_accuracy: 0.8080\n",
      "Epoch 70/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.5869 - accuracy: 0.8114 - val_loss: 0.5355 - val_accuracy: 0.8249\n",
      "Epoch 71/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5855 - accuracy: 0.8120 - val_loss: 0.5643 - val_accuracy: 0.8111\n",
      "Epoch 72/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5854 - accuracy: 0.8113 - val_loss: 0.6261 - val_accuracy: 0.7945\n",
      "Epoch 73/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5835 - accuracy: 0.8135 - val_loss: 0.5866 - val_accuracy: 0.8044\n",
      "Epoch 74/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5825 - accuracy: 0.8127 - val_loss: 0.5460 - val_accuracy: 0.8173\n",
      "Epoch 75/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5837 - accuracy: 0.8116 - val_loss: 0.5496 - val_accuracy: 0.8215\n",
      "Epoch 76/200\n",
      "357/357 [==============================] - 21s 56ms/step - loss: 0.5836 - accuracy: 0.8132 - val_loss: 0.5345 - val_accuracy: 0.8225\n",
      "Epoch 77/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5812 - accuracy: 0.8150 - val_loss: 0.5361 - val_accuracy: 0.8251\n",
      "Epoch 78/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5809 - accuracy: 0.8134 - val_loss: 0.6562 - val_accuracy: 0.7912\n",
      "Epoch 79/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5784 - accuracy: 0.8146 - val_loss: 0.6125 - val_accuracy: 0.7968\n",
      "Epoch 80/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5803 - accuracy: 0.8135 - val_loss: 0.5501 - val_accuracy: 0.8143\n",
      "Epoch 81/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5762 - accuracy: 0.8155 - val_loss: 0.5690 - val_accuracy: 0.8117\n",
      "Epoch 82/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5780 - accuracy: 0.8150 - val_loss: 0.5394 - val_accuracy: 0.8195\n",
      "Epoch 83/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5753 - accuracy: 0.8146 - val_loss: 0.5439 - val_accuracy: 0.8151\n",
      "Epoch 84/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5766 - accuracy: 0.8154 - val_loss: 0.5479 - val_accuracy: 0.8195\n",
      "Epoch 85/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5747 - accuracy: 0.8150 - val_loss: 0.6211 - val_accuracy: 0.7991\n",
      "Epoch 86/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5747 - accuracy: 0.8160 - val_loss: 0.5616 - val_accuracy: 0.8085\n",
      "Epoch 87/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5747 - accuracy: 0.8157 - val_loss: 0.6067 - val_accuracy: 0.7992\n",
      "Epoch 88/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5752 - accuracy: 0.8142 - val_loss: 0.5938 - val_accuracy: 0.7997\n",
      "Epoch 89/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5725 - accuracy: 0.8165 - val_loss: 0.6114 - val_accuracy: 0.7996\n",
      "Epoch 90/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5699 - accuracy: 0.8160 - val_loss: 0.5995 - val_accuracy: 0.8008\n",
      "Epoch 91/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5709 - accuracy: 0.8174 - val_loss: 0.5644 - val_accuracy: 0.8116\n",
      "Epoch 92/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.5719 - accuracy: 0.8155 - val_loss: 0.5273 - val_accuracy: 0.8263\n",
      "Epoch 93/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5689 - accuracy: 0.8169 - val_loss: 0.5484 - val_accuracy: 0.8191\n",
      "Epoch 94/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5722 - accuracy: 0.8163 - val_loss: 0.5326 - val_accuracy: 0.8231\n",
      "Epoch 95/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5692 - accuracy: 0.8171 - val_loss: 0.5358 - val_accuracy: 0.8255\n",
      "Epoch 96/200\n",
      "357/357 [==============================] - 21s 57ms/step - loss: 0.5643 - accuracy: 0.8188 - val_loss: 0.5190 - val_accuracy: 0.8264\n",
      "Epoch 97/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5642 - accuracy: 0.8190 - val_loss: 0.5387 - val_accuracy: 0.8211\n",
      "Epoch 98/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5664 - accuracy: 0.8188 - val_loss: 0.5799 - val_accuracy: 0.8017\n",
      "Epoch 99/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5664 - accuracy: 0.8180 - val_loss: 0.5451 - val_accuracy: 0.8224\n",
      "Epoch 100/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5651 - accuracy: 0.8185 - val_loss: 0.5844 - val_accuracy: 0.8071\n",
      "Epoch 101/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5642 - accuracy: 0.8192 - val_loss: 0.5590 - val_accuracy: 0.8107\n",
      "Epoch 102/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5601 - accuracy: 0.8178 - val_loss: 0.5484 - val_accuracy: 0.8153\n",
      "Epoch 103/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5665 - accuracy: 0.8170 - val_loss: 0.5192 - val_accuracy: 0.8256\n",
      "Epoch 104/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5583 - accuracy: 0.8208 - val_loss: 0.5191 - val_accuracy: 0.8264\n",
      "Epoch 105/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5607 - accuracy: 0.8179 - val_loss: 0.5440 - val_accuracy: 0.8203\n",
      "Epoch 106/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5610 - accuracy: 0.8214 - val_loss: 0.6212 - val_accuracy: 0.8017\n",
      "Epoch 107/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5614 - accuracy: 0.8188 - val_loss: 0.6075 - val_accuracy: 0.7963\n",
      "Epoch 108/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5593 - accuracy: 0.8201 - val_loss: 0.5789 - val_accuracy: 0.8053\n",
      "Epoch 109/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5597 - accuracy: 0.8193 - val_loss: 0.5788 - val_accuracy: 0.8033\n",
      "Epoch 110/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5585 - accuracy: 0.8207 - val_loss: 0.5727 - val_accuracy: 0.8068\n",
      "Epoch 111/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5616 - accuracy: 0.8197 - val_loss: 0.5654 - val_accuracy: 0.8092\n",
      "Epoch 112/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5589 - accuracy: 0.8198 - val_loss: 0.5497 - val_accuracy: 0.8168\n",
      "Epoch 113/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5618 - accuracy: 0.8205 - val_loss: 0.5365 - val_accuracy: 0.8181\n",
      "Epoch 114/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5620 - accuracy: 0.8183 - val_loss: 0.5616 - val_accuracy: 0.8156\n",
      "Epoch 115/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5581 - accuracy: 0.8215 - val_loss: 0.5756 - val_accuracy: 0.8079\n",
      "Epoch 116/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5584 - accuracy: 0.8203 - val_loss: 0.5269 - val_accuracy: 0.8207\n",
      "Epoch 117/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5559 - accuracy: 0.8222 - val_loss: 0.5820 - val_accuracy: 0.8063\n",
      "Epoch 118/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5567 - accuracy: 0.8205 - val_loss: 0.6429 - val_accuracy: 0.7872\n",
      "Epoch 119/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5555 - accuracy: 0.8208 - val_loss: 0.5637 - val_accuracy: 0.8136\n",
      "Epoch 120/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5540 - accuracy: 0.8221 - val_loss: 0.6132 - val_accuracy: 0.7956\n",
      "Epoch 121/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5559 - accuracy: 0.8219 - val_loss: 0.5188 - val_accuracy: 0.8273\n",
      "Epoch 122/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5516 - accuracy: 0.8227 - val_loss: 0.5372 - val_accuracy: 0.8197\n",
      "Epoch 123/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5533 - accuracy: 0.8229 - val_loss: 0.5368 - val_accuracy: 0.8176\n",
      "Epoch 124/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5531 - accuracy: 0.8219 - val_loss: 0.5209 - val_accuracy: 0.8264\n",
      "Epoch 125/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5543 - accuracy: 0.8209 - val_loss: 0.5208 - val_accuracy: 0.8269\n",
      "Epoch 126/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5503 - accuracy: 0.8223 - val_loss: 0.5245 - val_accuracy: 0.8255\n",
      "Epoch 127/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5523 - accuracy: 0.8220 - val_loss: 0.5385 - val_accuracy: 0.8191\n",
      "Epoch 128/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5518 - accuracy: 0.8230 - val_loss: 0.5349 - val_accuracy: 0.8228\n",
      "Epoch 129/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5502 - accuracy: 0.8231 - val_loss: 0.5762 - val_accuracy: 0.8072\n",
      "Epoch 130/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5515 - accuracy: 0.8229 - val_loss: 0.5220 - val_accuracy: 0.8284\n",
      "Epoch 131/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5479 - accuracy: 0.8234 - val_loss: 0.5527 - val_accuracy: 0.8119\n",
      "Epoch 132/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5504 - accuracy: 0.8235 - val_loss: 0.5378 - val_accuracy: 0.8201\n",
      "Epoch 133/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5502 - accuracy: 0.8233 - val_loss: 0.5275 - val_accuracy: 0.8225\n",
      "Epoch 134/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5518 - accuracy: 0.8219 - val_loss: 0.5469 - val_accuracy: 0.8183\n",
      "Epoch 135/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5488 - accuracy: 0.8250 - val_loss: 0.5752 - val_accuracy: 0.8085\n",
      "Epoch 136/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5470 - accuracy: 0.8222 - val_loss: 0.5432 - val_accuracy: 0.8145\n",
      "Epoch 137/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5481 - accuracy: 0.8248 - val_loss: 0.5934 - val_accuracy: 0.8047\n",
      "Epoch 138/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5490 - accuracy: 0.8241 - val_loss: 0.5347 - val_accuracy: 0.8213\n",
      "Epoch 139/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5482 - accuracy: 0.8246 - val_loss: 0.5563 - val_accuracy: 0.8144\n",
      "Epoch 140/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5465 - accuracy: 0.8248 - val_loss: 0.5308 - val_accuracy: 0.8237\n",
      "Epoch 141/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5471 - accuracy: 0.8240 - val_loss: 0.5541 - val_accuracy: 0.8115\n",
      "Epoch 142/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5454 - accuracy: 0.8250 - val_loss: 0.5501 - val_accuracy: 0.8152\n",
      "Epoch 143/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5474 - accuracy: 0.8237 - val_loss: 0.5362 - val_accuracy: 0.8203\n",
      "Epoch 144/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5469 - accuracy: 0.8244 - val_loss: 0.5629 - val_accuracy: 0.8128\n",
      "Epoch 145/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5466 - accuracy: 0.8231 - val_loss: 0.5830 - val_accuracy: 0.8020\n",
      "Epoch 146/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5473 - accuracy: 0.8228 - val_loss: 0.6217 - val_accuracy: 0.7977\n",
      "Epoch 147/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5469 - accuracy: 0.8244 - val_loss: 0.5317 - val_accuracy: 0.8199\n",
      "Epoch 148/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5473 - accuracy: 0.8243 - val_loss: 0.5543 - val_accuracy: 0.8119\n",
      "Epoch 149/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5468 - accuracy: 0.8239 - val_loss: 0.5149 - val_accuracy: 0.8272\n",
      "Epoch 150/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5471 - accuracy: 0.8245 - val_loss: 0.5197 - val_accuracy: 0.8217\n",
      "Epoch 151/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5456 - accuracy: 0.8244 - val_loss: 0.5985 - val_accuracy: 0.7988\n",
      "Epoch 152/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5461 - accuracy: 0.8238 - val_loss: 0.5753 - val_accuracy: 0.8072\n",
      "Epoch 153/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5460 - accuracy: 0.8241 - val_loss: 0.5080 - val_accuracy: 0.8297\n",
      "Epoch 154/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5417 - accuracy: 0.8263 - val_loss: 0.5546 - val_accuracy: 0.8155\n",
      "Epoch 155/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5446 - accuracy: 0.8238 - val_loss: 0.5152 - val_accuracy: 0.8255\n",
      "Epoch 156/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5430 - accuracy: 0.8259 - val_loss: 0.5263 - val_accuracy: 0.8228\n",
      "Epoch 157/200\n",
      "357/357 [==============================] - 20s 56ms/step - loss: 0.5426 - accuracy: 0.8261 - val_loss: 0.5017 - val_accuracy: 0.8352\n",
      "Epoch 158/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5396 - accuracy: 0.8249 - val_loss: 0.5525 - val_accuracy: 0.8157\n",
      "Epoch 159/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5472 - accuracy: 0.8237 - val_loss: 0.5440 - val_accuracy: 0.8177\n",
      "Epoch 160/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5411 - accuracy: 0.8260 - val_loss: 0.5105 - val_accuracy: 0.8299\n",
      "Epoch 161/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5386 - accuracy: 0.8256 - val_loss: 0.5203 - val_accuracy: 0.8251\n",
      "Epoch 162/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5371 - accuracy: 0.8263 - val_loss: 0.5269 - val_accuracy: 0.8221\n",
      "Epoch 163/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5404 - accuracy: 0.8271 - val_loss: 0.5241 - val_accuracy: 0.8239\n",
      "Epoch 164/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5401 - accuracy: 0.8255 - val_loss: 0.5369 - val_accuracy: 0.8179\n",
      "Epoch 165/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5413 - accuracy: 0.8258 - val_loss: 0.5305 - val_accuracy: 0.8235\n",
      "Epoch 166/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5408 - accuracy: 0.8266 - val_loss: 0.5285 - val_accuracy: 0.8247\n",
      "Epoch 167/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5400 - accuracy: 0.8262 - val_loss: 0.5281 - val_accuracy: 0.8213\n",
      "Epoch 168/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5376 - accuracy: 0.8260 - val_loss: 0.5384 - val_accuracy: 0.8213\n",
      "Epoch 169/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5385 - accuracy: 0.8271 - val_loss: 0.5432 - val_accuracy: 0.8201\n",
      "Epoch 170/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5394 - accuracy: 0.8265 - val_loss: 0.5937 - val_accuracy: 0.8017\n",
      "Epoch 171/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5370 - accuracy: 0.8266 - val_loss: 0.5925 - val_accuracy: 0.8039\n",
      "Epoch 172/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5381 - accuracy: 0.8267 - val_loss: 0.5201 - val_accuracy: 0.8285\n",
      "Epoch 173/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5422 - accuracy: 0.8249 - val_loss: 0.5429 - val_accuracy: 0.8147\n",
      "Epoch 174/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5409 - accuracy: 0.8265 - val_loss: 0.5020 - val_accuracy: 0.8316\n",
      "Epoch 175/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5352 - accuracy: 0.8286 - val_loss: 0.5105 - val_accuracy: 0.8301\n",
      "Epoch 176/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5370 - accuracy: 0.8277 - val_loss: 0.5460 - val_accuracy: 0.8193\n",
      "Epoch 177/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5368 - accuracy: 0.8265 - val_loss: 0.5363 - val_accuracy: 0.8233\n",
      "Epoch 178/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5362 - accuracy: 0.8267 - val_loss: 0.5345 - val_accuracy: 0.8236\n",
      "Epoch 179/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5382 - accuracy: 0.8267 - val_loss: 0.5276 - val_accuracy: 0.8277\n",
      "Epoch 180/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5413 - accuracy: 0.8252 - val_loss: 0.5240 - val_accuracy: 0.8264\n",
      "Epoch 181/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5400 - accuracy: 0.8272 - val_loss: 0.5333 - val_accuracy: 0.8213\n",
      "Epoch 182/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5361 - accuracy: 0.8260 - val_loss: 0.5188 - val_accuracy: 0.8271\n",
      "Epoch 183/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5337 - accuracy: 0.8277 - val_loss: 0.5450 - val_accuracy: 0.8213\n",
      "Epoch 184/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5371 - accuracy: 0.8277 - val_loss: 0.5278 - val_accuracy: 0.8233\n",
      "Epoch 185/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5341 - accuracy: 0.8281 - val_loss: 0.5174 - val_accuracy: 0.8281\n",
      "Epoch 186/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5354 - accuracy: 0.8286 - val_loss: 0.5111 - val_accuracy: 0.8288\n",
      "Epoch 187/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5308 - accuracy: 0.8279 - val_loss: 0.5179 - val_accuracy: 0.8267\n",
      "Epoch 188/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5332 - accuracy: 0.8277 - val_loss: 0.5121 - val_accuracy: 0.8295\n",
      "Epoch 189/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5364 - accuracy: 0.8278 - val_loss: 0.6060 - val_accuracy: 0.7991\n",
      "Epoch 190/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5375 - accuracy: 0.8272 - val_loss: 0.5126 - val_accuracy: 0.8259\n",
      "Epoch 191/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5349 - accuracy: 0.8276 - val_loss: 0.5089 - val_accuracy: 0.8325\n",
      "Epoch 192/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5315 - accuracy: 0.8286 - val_loss: 0.5081 - val_accuracy: 0.8311\n",
      "Epoch 193/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5374 - accuracy: 0.8283 - val_loss: 0.5563 - val_accuracy: 0.8175\n",
      "Epoch 194/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5358 - accuracy: 0.8276 - val_loss: 0.5157 - val_accuracy: 0.8288\n",
      "Epoch 195/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5378 - accuracy: 0.8278 - val_loss: 0.5277 - val_accuracy: 0.8240\n",
      "Epoch 196/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5335 - accuracy: 0.8273 - val_loss: 0.5190 - val_accuracy: 0.8316\n",
      "Epoch 197/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5350 - accuracy: 0.8279 - val_loss: 0.5258 - val_accuracy: 0.8267\n",
      "Epoch 198/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5335 - accuracy: 0.8270 - val_loss: 0.5199 - val_accuracy: 0.8293\n",
      "Epoch 199/200\n",
      "357/357 [==============================] - 20s 54ms/step - loss: 0.5315 - accuracy: 0.8282 - val_loss: 0.5356 - val_accuracy: 0.8216\n",
      "Epoch 200/200\n",
      "357/357 [==============================] - 20s 55ms/step - loss: 0.5326 - accuracy: 0.8291 - val_loss: 0.5232 - val_accuracy: 0.8264\n"
     ]
    }
   ],
   "source": [
    "# Running model\n",
    "history2 = model.fit(\n",
    "    x=train_ds_norm,\n",
    "    y=None,\n",
    "    validation_data= val_ds_norm,\n",
    "    epochs=epochs,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.0,\n",
    "    shuffle=True,\n",
    "    validation_freq=1,\n",
    "    batch_size = batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IhwPBnfkA_B"
   },
   "outputs": [],
   "source": [
    "# Loading the best weights for this model\n",
    "model.load_weights('models/keras/j.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hFgMkEFkA_C",
    "outputId": "d87851a5-e256-439c-f3a9-8098db04db30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 - 1s - loss: 0.4662 - accuracy: 0.8521 - 1s/epoch - 20ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds_norm, y=None, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0llfsPwPkA_C"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('models/j.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "0-pgO6Ackkrc",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "6328bf68-7ff7-45fa-8d75-6139dc176fee",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy and loss values for both the training and validation sets\n",
    "fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "accuracy = history2.history['accuracy']\n",
    "val_accuracy = history2.history['val_accuracy']\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "ax1.plot(epochs, accuracy, 'b--', label='Training accuracy')\n",
    "ax1.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "ax2.plot(epochs, loss, 'b--', label='Training loss')\n",
    "ax2.plot(epochs, val_loss, 'b',label='Validation loss')\n",
    "ax1.set_title('Accuracy')\n",
    "ax2.set_title('Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "\n",
    "ax2.set_xlabel('Epochs')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
